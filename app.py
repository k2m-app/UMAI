import time
import json
import re
import math
import requests
import streamlit as st
import streamlit.components.v1 as components
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

BASE_URL = "https://s.keibabook.co.jp"

PLACE_NAMES = {
    "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰", "04": "æ±äº¬",
    "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ", "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
}

# ç«¶é¦¬ãƒ–ãƒƒã‚¯ PLACEã‚³ãƒ¼ãƒ‰ â†’ netkeiba ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
KEIBABOOK_TO_NETKEIBA_PLACE = {
    "08": "01", "09": "02", "06": "03", "07": "04", "04": "05",
    "05": "06", "02": "07", "00": "08", "01": "09", "03": "10",
}

# ==================================================
# é¦¬å ´ãƒã‚¤ã‚¢ã‚¹è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Œå…¨ç‰ˆï¼‰
# ==================================================
BABA_BIAS_DATA = {
    "ä¸­å±±ãƒ€ãƒ¼ãƒˆ1200": {5: [6, 7, 8], 2: [5]},
    "ä¸­äº¬ãƒ€ãƒ¼ãƒˆ1400": {5: [6, 7, 8], 2: [3, 5]},
    "äº¬éƒ½ãƒ€ãƒ¼ãƒˆ1200": {5: [6, 7, 8]},
    "ä¸­å±±èŠ1200": {5: [1, 2, 3]},
    "é˜ªç¥èŠ1600": {5: [1, 2, 3]},
    "é˜ªç¥èŠ1400": {5: [1, 2, 3]},
    "é˜ªç¥èŠ1200": {5: [1, 2, 3], 2: [4]},
    "å‡½é¤¨èŠ1800": {5: [1, 2, 3]},
    "æ±äº¬èŠ2000": {5: [5], 2: [1]},
    "æ–°æ½ŸèŠ1000": {5: [7, 8], 3: [6]},
    "æ±äº¬ãƒ€ãƒ¼ãƒˆ1600": {5: [6, 8], 3: [7], 2: [5]},
    "æ±äº¬èŠ1600": {5: [6, 8]},
    "æœ­å¹Œãƒ€ãƒ¼ãƒˆ1000": {5: [7, 8]},
    "é˜ªç¥ãƒ€ãƒ¼ãƒˆ1400": {5: [8], 3: [4, 6], 2: [4, 6]},
    "æ±äº¬èŠ1400": {5: [8]},
    "äº¬éƒ½èŠ1600å†…": {5: [6]},
    "ä¸­å±±ãƒ€ãƒ¼ãƒˆ1800": {5: [7, 8], 2: [4, 5]},
    "ä¸­å±±èŠ2500": {5: [5], 3: [6, 8]},
    "ä¸­äº¬èŠ1200": {5: [2, 3], 3: [1], 2: [4, 5]},
    "äº¬éƒ½ãƒ€ãƒ¼ãƒˆ1800": {5: [6]},
    "äº¬éƒ½ãƒ€ãƒ¼ãƒˆ1900": {5: [3]},
    "äº¬éƒ½èŠ1200": {5: [7]},
    "äº¬éƒ½èŠ2400": {5: [2, 4]},
    "å°å€‰èŠ1200": {5: [7], 3: [8], 2: [6]},
    "æ–°æ½Ÿãƒ€ãƒ¼ãƒˆ1200": {5: [6, 7], 2: [4, 8]},
    "æ–°æ½ŸèŠ1600": {5: [5, 7]},
    "æ±äº¬ãƒ€ãƒ¼ãƒˆ1400": {5: [6, 7], 3: [4, 8]},
    "é˜ªç¥ãƒ€ãƒ¼ãƒˆ1800": {5: [6, 7]},
    "é˜ªç¥ãƒ€ãƒ¼ãƒˆ1200": {5: [8], 3: [5, 6, 7], 2: [4]},
    "ä¸­äº¬ãƒ€ãƒ¼ãƒˆ1200": {3: [1, 6]},
    "ä¸­å±±èŠ1600": {5: [1], 3: [2, 3, 4]},
    "ä¸­äº¬èŠ1400": {5: [3], 3: [1, 4]},
    "æ±äº¬èŠ2400": {3: [1, 3]},
    "é˜ªç¥èŠ1800": {5: [1, 3], 3: [2, 4]},
    "å‡½é¤¨èŠ2000": {5: [2], 3: [1, 5], 2: [4, 6]},
    "æœ­å¹ŒèŠ2000": {5: [1, 5], 3: [2, 3]},
    "æœ­å¹ŒèŠ1200": {3: [1, 8], 2: [6, 7]},
}


# ==================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ==================================================
def _clean_text_ja(s: str) -> str:
    """å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹é™¤å»ãƒ»ç©ºç™½æ­£è¦åŒ–"""
    if not s:
        return ""
    s = s.replace("\u3000", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _is_missing_marker(s: str) -> bool:
    """æƒ…å ±ãªã—ãƒãƒ¼ã‚«ãƒ¼åˆ¤å®š"""
    t = _clean_text_ja(s)
    return t in {"ï¼", "-", "â€”", "â€•", "â€", ""}


def render_copy_button(text: str, label: str, dom_id: str):
    """ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³è¡¨ç¤º"""
    safe_text = json.dumps(text)
    html = f"""
    <div style="margin:5px 0;">
    <button onclick="copyToClipboard_{dom_id}()" 
            style="padding:6px 12px; background:#4CAF50; color:white; border:none; 
                   border-radius:4px; cursor:pointer; font-size:12px;">
        {label}
    </button>
    </div>
    <script>
    function copyToClipboard_{dom_id}() {{
        const text = {safe_text};
        navigator.clipboard.writeText(text).then(() => {{
        }}).catch(err => {{
        }});
    }}
    </script>
    """
    components.html(html, height=40)


def _safe_int(s, default=0) -> int:
    """'-' ç­‰ã‚’å®‰å…¨ã« int åŒ–"""
    try:
        if s is None:
            return default
        if isinstance(s, (int, float)):
            return int(s)
        ss = str(s).strip()
        if ss in {"", "-", "ï¼"}:
            return default
        return int(ss)
    except:
        return default


# ==================================================
# ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°ï¼ˆåŸºç¤å€¤â†’åå·®å€¤â†’30ç‚¹æº€ç‚¹å¤‰æ›ï¼‰
# ==================================================
def compute_speed_metrics(cpu_data: dict, w_max: float = 2.0, w_last: float = 1.8, w_avg: float = 1.2) -> dict:
    raw_scores = {}
    for umaban, d in cpu_data.items():
        last = _safe_int(d.get("sp_last"), 0)
        two = _safe_int(d.get("sp_2"), 0)
        thr = _safe_int(d.get("sp_3"), 0)
        vals = [v for v in [last, two, thr] if v > 0]
        if not vals:
            continue
        avg = sum(vals) / len(vals)
        max_v = max(vals)
        denom = (w_max + w_last + w_avg)
        raw = (max_v * w_max + last * w_last + avg * w_avg) / denom
        raw_scores[umaban] = raw

    if not raw_scores:
        return {}

    values = list(raw_scores.values())
    mean = sum(values) / len(values)
    std = math.sqrt(sum((v - mean) ** 2 for v in values) / len(values)) if len(values) > 1 else 0

    # åå·®å€¤ç®—å‡º
    hensachi_scores = {}
    for umaban, raw in raw_scores.items():
        if std == 0:
            hensachi = 50.0
        else:
            hensachi = 50.0 + 10.0 * (raw - mean) / std
        hensachi_scores[umaban] = hensachi

    # æœ€é«˜åå·®å€¤ã‚’30ç‚¹ã«æ­£è¦åŒ–
    if not hensachi_scores:
        return {}
    
    max_hensachi = max(hensachi_scores.values())
    
    out = {}
    for umaban, raw in raw_scores.items():
        hensachi = hensachi_scores[umaban]
        
        if max_hensachi == 0:
             score = 0.0
        elif max_hensachi == min(hensachi_scores.values()):
            score = 30.0
        else:
            score = 30.0 * (hensachi / max_hensachi)
        
        out[umaban] = {
            "raw": round(raw, 2),
            "hensachi": round(hensachi, 2),
            "score": round(score, 2)
        }

    return out


# ==================================================
# é¦¬å ´ãƒã‚¤ã‚¢ã‚¹è©•ä¾¡é–¢æ•°ï¼ˆå®Œå…¨ç‰ˆï¼‰
# ==================================================
def extract_race_info(race_title: str) -> dict:
    result = {
        "place": None,
        "distance": None,
        "track_type": None,
        "day": None,
        "course_variant": ""
    }
    
    # ç«¶é¦¬å ´åã¨é–‹å‚¬æ—¥ã®æŠ½å‡ºï¼ˆ1å›ä¸­å±±4æ—¥ç›® ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    place_day_pattern = r'(\d+)å›([^0-9]+?)(\d+)æ—¥ç›®'
    place_day_match = re.search(place_day_pattern, race_title)
    if place_day_match:
        result["place"] = place_day_match.group(2).strip()
        result["day"] = int(place_day_match.group(3))
    
    # è·é›¢ã®æŠ½å‡ºï¼ˆ1200m ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    distance_pattern = r'(\d{3,4})m'
    distance_match = re.search(distance_pattern, race_title)
    if distance_match:
        result["distance"] = distance_match.group(1)
    
    # èŠ/ãƒ€ãƒ¼ãƒˆã®åˆ¤å®š
    if 'ãƒ€ãƒ¼ãƒˆ' in race_title:
        result["track_type"] = "dirt"
    elif 'èŠ' in race_title:
        result["track_type"] = "turf"
    
    # ã‚³ãƒ¼ã‚¹ç¨®åˆ¥ï¼ˆå†…å›ã‚Šãƒ»å¤–å›ã‚Šï¼‰ã®åˆ¤å®š
    if 'å†…' in race_title:
        result["course_variant"] = "å†…"
    elif 'å¤–' in race_title:
        result["course_variant"] = "å¤–"
    
    return result


def calculate_baba_bias(waku: int, race_title: str) -> dict:
    kaisai_bias = 0
    course_bias = 0
    debug_info = []
    
    # ãƒ¬ãƒ¼ã‚¹æƒ…å ±æŠ½å‡º
    race_info = extract_race_info(race_title)
    
    place_name = race_info["place"]
    distance = race_info["distance"]
    track_type = race_info["track_type"]
    race_day = race_info["day"]
    course_variant = race_info["course_variant"]
    
    # é–‹å‚¬é€±ãƒã‚¤ã‚¢ã‚¹(èŠã®ãƒ¬ãƒ¼ã‚¹ã§é–‹å‚¬1-2æ—¥ç›®ã®ã¿)
    if track_type == "turf" and race_day in [1, 2]:
        if waku == 1:
            kaisai_bias = 5
        elif waku == 2:
            kaisai_bias = 3
        elif waku == 3:
            kaisai_bias = 2
    
    # ã‚³ãƒ¼ã‚¹ãƒã‚¤ã‚¢ã‚¹è©•ä¾¡
    if place_name and distance and track_type:
        track_str = "èŠ" if track_type == "turf" else "ãƒ€ãƒ¼ãƒˆ"
        course_key = f"{place_name}{track_str}{distance}{course_variant}"
        
        if course_key in BABA_BIAS_DATA:
            bias_data = BABA_BIAS_DATA[course_key]
            
            # ç‚¹æ•°ã®é«˜ã„é †ã«ãƒã‚§ãƒƒã‚¯ï¼ˆ5â†’3â†’2ï¼‰
            for points in [5, 3, 2]:
                if points in bias_data and waku in bias_data[points]:
                    course_bias = points
                    break
    
    return {
        "kaisai_bias": kaisai_bias,
        "course_bias": course_bias,
        "total": kaisai_bias + course_bias,
        "debug": " | ".join(debug_info)
    }


# ==================================================
# Selenium Setup
# ==================================================
def build_driver() -> webdriver.Chrome:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,2200")
    options.add_argument("--lang=ja-JP")
    options.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(60)
    return driver


def login_keibabook(driver: webdriver.Chrome) -> None:
    if not KEIBA_ID or not KEIBA_PASS:
        return
    driver.get(f"{BASE_URL}/login/login")
    try:
        WebDriverWait(driver, 5).until(
            EC.visibility_of_element_located((By.NAME, "login_id"))
        ).send_keys(KEIBA_ID)
        WebDriverWait(driver, 5).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
        ).send_keys(KEIBA_PASS)
        WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))
        ).click()
        time.sleep(1.0)
    except:
        pass


# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯:å©èˆã®è©± (Danwa) - æ ç•ªè¿½åŠ ç‰ˆ
# ==================================================
def parse_race_info_from_danwa(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"header_text": ""}

    racemei = racetitle.find("div", class_="racemei")
    header_parts = []
    if racemei:
        for p in racemei.find_all("p"):
            header_parts.append(p.get_text(strip=True))

    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    if racetitle_sub:
        for p in racetitle_sub.find_all("p"):
            header_parts.append(p.get_text(strip=True))

    return {"header_text": "\n".join(header_parts)}


def parse_danwa_horses(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_=lambda c: c and "danwa" in str(c))
    if not table or not table.tbody:
        return {}

    horses = {}
    current_umaban = None
    current_waku = None

    rows = table.tbody.find_all("tr", recursive=False)
    for tr in rows:
        classes = tr.get("class", [])
        if "spacer" in classes:
            continue

        # æ ç•ªå–å¾—
        waku_td = tr.find("td", class_="waku")
        umaban_td = tr.find("td", class_="umaban")
        bamei_td = tr.find("td", class_="left")

        if waku_td and umaban_td and bamei_td:
            # æ ç•ªæŠ½å‡º
            waku_p = waku_td.find("p")
            if waku_p:
                waku_class = waku_p.get("class", [])
                for cls in waku_class:
                    if cls.startswith("waku"):
                        current_waku = re.sub(r"\D", "", cls)
                        break
            
            # é¦¬ç•ªæŠ½å‡º
            raw_umaban = umaban_td.get_text(strip=True)
            current_umaban = re.sub(r"\D", "", raw_umaban)

            # é¦¬åæŠ½å‡º
            anchor = bamei_td.find("a")
            if anchor:
                raw_name = anchor.get_text(strip=True)
            else:
                raw_name = bamei_td.get_text(strip=True)
            clean_name = _clean_text_ja(raw_name)

            if current_umaban:
                horses[current_umaban] = {
                    "name": clean_name,
                    "waku": current_waku if current_waku else "?",
                    "danwa": ""
                }
            continue

        danwa_td = tr.find("td", class_="danwa")
        if danwa_td and current_umaban:
            comment_text = danwa_td.get_text("\n", strip=True)
            comment_text = _clean_text_ja(comment_text)
            if horses[current_umaban]["danwa"]:
                horses[current_umaban]["danwa"] += (" " + comment_text)
            else:
                horses[current_umaban]["danwa"] = comment_text

    return horses


def fetch_keibabook_danwa(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/danwa/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.default.danwa"))
        )
    except:
        pass
    html = driver.page_source
    return parse_race_info_from_danwa(html), parse_danwa_horses(html)


# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯:èª¿æ•™ (Chokyo)
# ==================================================
def parse_keibabook_chokyo(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    data = {}
    tables = soup.find_all("table", class_="cyokyo")

    for tbl in tables:
        umaban_td = tbl.find("td", class_="umaban")
        if not umaban_td:
            continue
        umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))

        tanpyo_td = tbl.find("td", class_="tanpyo")
        tanpyo = _clean_text_ja(tanpyo_td.get_text(strip=True)) if tanpyo_td else "ãªã—"

        details_text_parts = []
        detail_cell = tbl.find("td", colspan="5")
        if detail_cell:
            for child in detail_cell.children:
                if child.name == "dl" and "dl-table" in child.get("class", []):
                    dt_texts = [c.get_text(strip=True) for c in child.find_all(["dt", "dd"])]
                    line = " ".join(dt_texts)
                    details_text_parts.append(line)
                elif child.name == "table" and "cyokyodata" in child.get("class", []):
                    time_tr = child.find("tr", class_="time")
                    if time_tr:
                        times = [td.get_text(strip=True) for td in time_tr.find_all("td")]
                        details_text_parts.append(" ".join(times))
                    awase_tr = child.find("tr", class_="awase")
                    if awase_tr:
                        awase_txt = _clean_text_ja(awase_tr.get_text(strip=True))
                        details_text_parts.append(awase_txt)

            semekaisetu_div = detail_cell.find("div", class_="semekaisetu")
            if semekaisetu_div:
                kaisetu_p = semekaisetu_div.find("p")
                if kaisetu_p:
                    k_text = _clean_text_ja(kaisetu_p.get_text(strip=True))
                    details_text_parts.append(f"[æ”»ã‚è§£èª¬] {k_text}")

        full_detail = " ".join(details_text_parts)
        full_detail = re.sub(r"\s+", " ", full_detail).strip()
        data[umaban] = {"tanpyo": tanpyo, "details": full_detail}

    return data


def fetch_keibabook_chokyo(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/cyokyo/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "cyokyo"))
        )
    except:
        pass
    html = driver.page_source
    return parse_keibabook_chokyo(html)


# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯:å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ (Syoin)
# ==================================================
def parse_zenkoso_interview(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_=lambda c: c and "syoin" in str(c))
    if not table or not table.tbody:
        return {}

    interview_data = {}
    current_umaban = None

    rows = table.tbody.find_all("tr", recursive=False)
    for tr in rows:
        classes = tr.get("class", [])
        if "spacer" in classes:
            continue

        umaban_td = tr.find("td", class_="umaban")
        if umaban_td:
            raw_u = umaban_td.get_text(strip=True)
            current_umaban = re.sub(r"\D", "", raw_u)
            continue

        syoin_td = tr.find("td", class_="syoin")
        if syoin_td and current_umaban:
            meta_div = syoin_td.find("div", class_="syoindata")
            if meta_div:
                meta_div.decompose()
            raw_text = syoin_td.get_text(" ", strip=True)
            clean_text = _clean_text_ja(raw_text)
            if not _is_missing_marker(clean_text) and len(clean_text) > 1:
                interview_data[current_umaban] = clean_text
            current_umaban = None

    return interview_data


def fetch_zenkoso_interview(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syoin/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.default.syoin"))
        )
    except:
        pass
    return parse_zenkoso_interview(driver.page_source)


# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯:CPUäºˆæƒ³ (æ–°é¦¬å¯¾å¿œç‰ˆ)
# ==================================================
def parse_keibabook_cpu(html: str, is_shinba: bool = False) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    data = {}

    # --- ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ« ---
    speed_tbl = soup.find("table", id="cpu_speed_sort_table")
    if speed_tbl and speed_tbl.tbody:
        for tr in speed_tbl.tbody.find_all("tr"):
            umaban_td = tr.find("td", class_="umaban")
            if not umaban_td:
                continue
            umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))
            if not umaban:
                continue

            tds = tr.find_all("td")
            if len(tds) < 8:
                continue

            def get_v(idx):
                p = tds[idx].find("p")
                txt = re.sub(r"\D", "", p.get_text(strip=True)) if p else ""
                val = int(txt) if txt else 0
                return val if val < 900 else 0

            last = get_v(-1)
            two = get_v(-2)
            thr = get_v(-3)

            vals = [x for x in [last, two, thr] if x > 0]
            avg = round(sum(vals) / len(vals)) if vals else 0

            data[umaban] = {
                "sp_last": str(last) if last else "-",
                "sp_2": str(two) if two else "-",
                "sp_3": str(thr) if thr else "-",
                "sp_avg": str(avg) if avg else "-",
            }

    # --- ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ« ---
    factor_tbl = None
    for t in soup.find_all("table"):
        c = t.find("caption")
        if c and "ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼" in c.get_text():
            factor_tbl = t
            break

    if factor_tbl and factor_tbl.tbody:
        for tr in factor_tbl.tbody.find_all("tr"):
            umaban_td = tr.find("td", class_="umaban")
            if not umaban_td:
                continue
            umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))
            if not umaban:
                continue

            tds = tr.find_all("td")
            if len(tds) < 6:
                continue

            def get_m(idx):
                if idx >= len(tds):
                    return "-"
                p = tds[idx].find("p")
                t = p.get_text(strip=True) if p else ""
                return t if t else "-"

            if umaban not in data:
                data[umaban] = {}

            if is_shinba:
                data[umaban].update(
                    {
                        "fac_deashi": get_m(5),
                        "fac_kettou": get_m(6),
                        "fac_ugoki": get_m(8),
                    }
                )
            else:
                data[umaban].update(
                    {
                        "fac_crs": get_m(5),
                        "fac_dis": get_m(6),
                        "fac_zen": get_m(7),
                    }
                )

    return data


def fetch_keibabook_cpu_data(driver, race_id: str, is_shinba: bool = False):
    url = f"{BASE_URL}/cyuou/cpu/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, "main")))
    except:
        pass
    return parse_keibabook_cpu(driver.page_source, is_shinba)


# ==================================================
# Netkeiba (é¨æ‰‹ãƒ»æˆ¦ç¸¾è©³ç´°å–å¾—)
# ==================================================
def _parse_netkeiba_past_td(td) -> str:
    """netkeibaã®éå»èµ°ã‚»ãƒ«(td.Past)ã‚’è§£æã—ã¦æ–‡å­—åˆ—åŒ–"""
    if not td:
        return "-"

    data01 = td.find("div", class_="Data01")
    date_place = _clean_text_ja(data01.get_text(strip=True)) if data01 else ""

    data02 = td.find("div", class_="Data02")
    race_name = _clean_text_ja(data02.get_text(strip=True)) if data02 else ""

    data03 = td.find("div", class_="Data03")
    jockey_weight = _clean_text_ja(data03.get_text(" ", strip=True)) if data03 else ""

    rank = "?"
    rank_tag = td.find("span", class_="Num")
    if not rank_tag:
        rank_tag = td.find("div", class_="Rank") or td.find("span", class_="Rank") or td.find("span", class_="Order")
    if rank_tag:
        rank = _clean_text_ja(rank_tag.get_text(strip=True))

    data05 = td.find("div", class_="Data05")
    time_dist = _clean_text_ja(data05.get_text(" ", strip=True)) if data05 else ""

    passing = ""
    data06 = td.find("div", class_="Data06")
    if data06:
        raw_d6 = _clean_text_ja(data06.get_text(strip=True))
        match = re.search(r"(\d{1,2}(?:-\d{1,2})+)", raw_d6)
        if match:
            passing = match.group(1)
        if not passing:
            match_single = re.match(r"^(\d{1,2})\s", raw_d6)
            if match_single:
                passing = match_single.group(1)

    if len(date_place) < 2:
        return "-"

    rank_display = f"{passing}â†’{rank}ç€" if passing else f"{rank}ç€"
    return f"[{date_place} {race_name} {jockey_weight} {time_dist} ({rank_display})]"


def fetch_netkeiba_data(driver, year, kai, place, day, race_num):
    nk_place = KEIBABOOK_TO_NETKEIBA_PLACE.get(place, "")
    if not nk_place:
        return {}

    nk_race_id = f"{year}{nk_place}{kai.zfill(2)}{day.zfill(2)}{race_num.zfill(2)}"
    url = f"https://race.netkeiba.com/race/shutuba_past.html?race_id={nk_race_id}&rf=shutuba_submenu"

    driver.get(url)
    try:
        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, "Shutuba_Past5_Table")))
    except:
        return {}

    soup = BeautifulSoup(driver.page_source, "html.parser")
    data = {}

    rows = soup.find_all("tr", class_="HorseList")
    for tr in rows:
        waku_tds = tr.find_all("td", class_="Waku")
        umaban = ""
        for td in waku_tds:
            txt = re.sub(r"\D", "", td.get_text(strip=True))
            if txt:
                umaban = txt
                break

        if not umaban:
            continue

        jockey_td = tr.find("td", class_="Jockey")
        jockey = "ä¸æ˜"
        if jockey_td:
            a_tag = jockey_td.find("a")
            if a_tag:
                jockey = a_tag.get_text(strip=True)
            else:
                barei = jockey_td.find("span", class_="Barei")
                if barei:
                    barei.decompose()
                jockey = jockey_td.get_text(strip=True)
        jockey = _clean_text_ja(jockey)

        past_tds = tr.find_all("td", class_="Past")
        past_list = []
        for td in past_tds[:3]:
            if "Rest" in td.get("class", []):
                past_list.append("(æ”¾ç‰§/ä¼‘é¤Š)")
            else:
                past_list.append(_parse_netkeiba_past_td(td))

        data[umaban] = {"jockey": jockey, "past": past_list}

    return data


# ==================================================
# Dify Streaming
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ DIFY_API_KEY æœªè¨­å®š"
        return

    payload = {
        "inputs": {"text": full_text},
        "response_mode": "streaming",
        "user": "keiba-bot",
    }

    headers = {"Authorization": f"Bearer {DIFY_API_KEY}", "Content-Type": "application/json"}

    try:
        res = requests.post(
            "https://api.dify.ai/v1/workflows/run",
            headers=headers,
            json=payload,
            stream=True,
            timeout=90
        )
        for line in res.iter_lines():
            if not line:
                continue
            decoded = line.decode("utf-8").replace("data: ", "")
            try:
                data = json.loads(decoded)
                event = data.get("event")
                if event == "workflow_finished":
                    outputs = data.get("data", {}).get("outputs", {})
                    for val in outputs.values():
                        if isinstance(val, str):
                            yield val
                if event == "message" or "answer" in data:
                    yield data.get("answer", "")
            except:
                pass
    except Exception as e:
        yield f"Error: {e}"


# ==================================================
# Main Execution (Batch)
# ==================================================
def run_batch_prediction(jobs_config):
    """
    è¤‡æ•°å ´ã®è¨­å®šã‚’å—ã‘å–ã‚Šã€ãƒ–ãƒ©ã‚¦ã‚¶ã‚’ä¸€åº¦ç«‹ã¡ä¸Šã’ãŸã‚‰é€£ç¶šã§å‡¦ç†ã™ã‚‹
    jobs_config = [
        {"year": "2026", "kai": "01", "place": "05", "day": "04", "races": [1,2...], "place_name": "ä¸­å±±"},
        ...
    ]
    """
    driver = build_driver()
    full_output_log = ""
    
    try:
        st.info(f"ãƒ­ã‚°ã‚¤ãƒ³ä¸­... (ID: {KEIBA_ID[:2]}**)")
        login_keibabook(driver)
        st.success("ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸã€‚é€£ç¶šå®Ÿè¡Œã‚’é–‹å§‹ã—ã¾ã™ã€‚")

        # ã‚¸ãƒ§ãƒ–ï¼ˆé–‹å‚¬å ´ï¼‰ãƒ«ãƒ¼ãƒ—
        for job_idx, job in enumerate(jobs_config):
            year = job["year"]
            kai = str(job["kai"]).zfill(2)
            place = str(job["place"]).zfill(2)
            day = str(job["day"]).zfill(2)
            target_races = sorted(job["races"])
            place_name = job["place_name"]
            
            base_id = f"{year}{kai}{place}{day}"
            
            st.markdown(f"## ğŸ {place_name}é–‹å‚¬ ({day}æ—¥ç›®)")
            full_output_log += f"\n\n{'='*30}\nã€{place_name}ã€‘ {year}å¹´{kai}å›{day}æ—¥ç›®\n{'='*30}\n"

            # ãƒ¬ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ—
            for r in target_races:
                race_num_str = f"{r:02}"
                race_id = base_id + race_num_str
                
                st.markdown(f"### {place_name} {r}R")
                status = st.empty()
                status.text("ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
                
                # --- ãƒ‡ãƒ¼ã‚¿å–å¾— ---
                header_info, danwa_data = fetch_keibabook_danwa(driver, race_id)
                if not danwa_data:
                    st.error(f"{place_name} {r}R ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•— (ID:{race_id})")
                    continue
                
                race_title = header_info.get("header_text", "")
                is_shinba = ("æ–°é¦¬" in race_title) or ("ãƒ¡ã‚¤ã‚¯ãƒ‡ãƒ“ãƒ¥ãƒ¼" in race_title)
                
                cpu_data = fetch_keibabook_cpu_data(driver, race_id, is_shinba=is_shinba)
                speed_metrics = compute_speed_metrics(cpu_data)
                interview_data = fetch_zenkoso_interview(driver, race_id)
                chokyo_data = fetch_keibabook_chokyo(driver, race_id)
                nk_data = fetch_netkeiba_data(driver, year, kai, place, day, race_num_str)
                
                # --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ ---
                lines = []
                for umaban in sorted(danwa_data.keys(), key=int):
                    d_info = danwa_data[umaban]
                    c_info = cpu_data.get(umaban, {})
                    i_text = interview_data.get(umaban, "ãªã—")
                    k_info = chokyo_data.get(umaban, {"tanpyo": "-", "details": "-"})
                    n_info = nk_data.get(umaban, {})
                    waku = d_info.get("waku", "?")
                    
                    sm = speed_metrics.get(umaban, {})
                    baba_bias = calculate_baba_bias(int(waku) if waku.isdigit() else 0, race_title)
                    
                    past_str = " / ".join(n_info.get("past", [])) or "æƒ…å ±ãªã—"
                    sp_str = f"æŒ‡æ•°:{sm.get('score','-')}/30 (åå·®å€¤:{sm.get('hensachi','-')})"
                    bias_str = f"ãƒã‚¤ã‚¢ã‚¹:{baba_bias['total']}/10"
                    
                    if is_shinba:
                        fac_str = f"F:{c_info.get('fac_deashi','-')}/{c_info.get('fac_kettou','-')}"
                    else:
                        fac_str = f"F:{c_info.get('fac_crs','-')}/{c_info.get('fac_dis','-')}"
                        
                    line = (
                        f"â–¼{waku}æ {umaban}ç•ª {d_info['name']} (é¨æ‰‹:{n_info.get('jockey','-')})\n"
                        f"ã€ãƒ‡ãƒ¼ã‚¿ã€‘{sp_str} {bias_str} {fac_str}\n"
                        f"ã€å©èˆã€‘{d_info['danwa']}\n"
                        f"ã€å‰èµ°ã€‘{i_text}\n"
                        f"ã€èª¿æ•™ã€‘{k_info['tanpyo']} {k_info['details']}\n"
                        f"ã€è¿‘èµ°ã€‘{past_str}\n"
                    )
                    lines.append(line)

                full_prompt = f"â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n{race_title}\n\nâ– å„é¦¬è©³ç´°\n" + "\n".join(lines)
                
                # AIç”Ÿæˆ
                status.text("AIåˆ†æä¸­...")
                result_area = st.empty()
                ai_output = ""
                for chunk in stream_dify_workflow(full_prompt):
                    ai_output += chunk
                    result_area.markdown(ai_output + "â–Œ")
                
                result_area.markdown(ai_output)
                
                # ãƒ­ã‚°è“„ç©
                race_log = f"\n--- {place_name} {r}R ---\n{ai_output}"
                full_output_log += race_log
                
                render_copy_button(ai_output, f"{place_name}{r}R ã‚³ãƒ”ãƒ¼", f"copy_{job_idx}_{r}")
                status.success("å®Œäº†")
            
            # 1é–‹å‚¬çµ‚äº†ã”ã¨ã®åŒºåˆ‡ã‚Š
            st.success(f"âœ… {place_name}é–‹å‚¬åˆ†ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
            
    except Exception as e:
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    finally:
        driver.quit()
        
    return full_output_logJRAimport streamlit as st
import keiba_bot

# ==================================================
# App config
# ==================================================
st.set_page_config(
    page_title="UMAI",
    layout="wide",
    initial_sidebar_state="expanded",
)

# è»½é‡CSS
st.markdown(
    """
    <style>
      .block-container { padding-top: 1rem; padding-bottom: 2rem; }
      .stButton > button { width: 100%; padding: 0.6rem; border-radius: 8px; }
      label { font-size: 0.9rem !important; }
      /* ã‚¿ãƒ–ã®è¦‹ãŸç›®èª¿æ•´ */
      .stTabs [data-baseweb="tab-list"] { gap: 8px; }
      .stTabs [data-baseweb="tab"] { height: 50px; white-space: pre-wrap; background-color: #f0f2f6; border-radius: 4px 4px 0 0; gap: 1px; padding-top: 10px; padding-bottom: 10px; }
      .stTabs [aria-selected="true"] { background-color: #ffffff; border-top: 2px solid #ff4b4b; }
    </style>
    """,
    unsafe_allow_html=True
)

PLACE_NAMES = {
    "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰", "04": "æ±äº¬",
    "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ", "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
}

# å¹´ã®é¸æŠè‚¢ï¼ˆ2026ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ã™ã‚‹ãŸã‚å…ˆé ­ã«é…ç½®ï¼‰
YEAR_OPTIONS = ["2026", "2025"]

# ==================================================
# Session state init
# ==================================================
# 3å ´åˆ†ã®è¨­å®šã‚’ä¿æŒã™ã‚‹ãŸã‚ã®åˆæœŸåŒ–
MAX_VENUES = 3

if "combined_output" not in st.session_state:
    st.session_state.combined_output = ""

# å„ä¼šå ´ã®è¨­å®šä¿å­˜ç”¨StateåˆæœŸåŒ–
for v_idx in range(MAX_VENUES):
    prefix = f"v{v_idx}"
    
    if f"{prefix}_active" not in st.session_state:
        st.session_state[f"{prefix}_active"] = (v_idx == 0) # æœ€åˆã ã‘Active
    
    # å¹´ã®åˆæœŸå€¤ã¯ "2026"
    if f"{prefix}_year" not in st.session_state:
        st.session_state[f"{prefix}_year"] = "2026"
        
    if f"{prefix}_kai" not in st.session_state:
        st.session_state[f"{prefix}_kai"] = "01"
    if f"{prefix}_place" not in st.session_state:
        st.session_state[f"{prefix}_place"] = "05" # Default ä¸­å±±
    if f"{prefix}_day" not in st.session_state:
        st.session_state[f"{prefix}_day"] = "01"
    
    # ãƒ¬ãƒ¼ã‚¹é¸æŠåˆæœŸåŒ–
    for r in range(1, 13):
        rk = f"{prefix}_r{r}"
        if rk not in st.session_state:
            st.session_state[rk] = False

# ==================================================
# Helper Functions
# ==================================================
def set_preset(v_idx, mode):
    prefix = f"v{v_idx}"
    for r in range(1, 13):
        key = f"{prefix}_r{r}"
        if mode == "all":
            st.session_state[key] = True
        elif mode == "clear":
            st.session_state[key] = False
        elif mode == "1-6":
            st.session_state[key] = (r <= 6)
        elif mode == "7-12":
            st.session_state[key] = (r >= 7)

# ==================================================
# Main UI
# ==================================================
st.title("UMAI")
st.caption("æœ€å¤§3ã¤ã®é–‹å‚¬å ´ã‚’ä¸€æ‹¬è¨­å®šã—ã€é€£ç¶šã§äºˆæƒ³ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")

# ã‚¿ãƒ–ã§ä¼šå ´åˆ‡ã‚Šæ›¿ãˆ
tabs = st.tabs([f"é–‹å‚¬è¨­å®š {i+1}" for i in range(MAX_VENUES)])

jobs_config = [] # å®Ÿè¡Œæ™‚ã«æ¸¡ã™è¨­å®šãƒªã‚¹ãƒˆ

for v_idx, tab in enumerate(tabs):
    prefix = f"v{v_idx}"
    with tab:
        is_active = st.toggle(f"ã“ã®é–‹å‚¬ï¼ˆé–‹å‚¬è¨­å®š{v_idx+1}ï¼‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹", key=f"{prefix}_active")
        
        if is_active:
            col_p1, col_p2, col_p3, col_p4 = st.columns([1, 1, 1, 1])
            with col_p1:
                # ã€ä¿®æ­£ç®‡æ‰€ã€‘ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‹ã‚‰ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹ã«å¤‰æ›´
                st.selectbox("å¹´", YEAR_OPTIONS, key=f"{prefix}_year")
            with col_p2:
                st.selectbox("å›", [f"{i:02}" for i in range(1, 7)], key=f"{prefix}_kai")
            with col_p3:
                # keyã‹ã‚‰indexé€†ç®—
                opts = list(PLACE_NAMES.keys())
                curr = st.session_state[f"{prefix}_place"]
                idx = opts.index(curr) if curr in opts else 0
                st.selectbox("å ´æ‰€", opts, format_func=lambda x: f"{x}:{PLACE_NAMES[x]}", index=idx, key=f"{prefix}_place")
            with col_p4:
                st.selectbox("æ—¥ç›®", [f"{i:02}" for i in range(1, 15)], key=f"{prefix}_day")
            
            st.markdown("---")
            st.caption("â–¼ å¯¾è±¡ãƒ¬ãƒ¼ã‚¹é¸æŠ")
            
            # ãƒ—ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
            bc1, bc2, bc3, bc4 = st.columns(4)
            if bc1.button("å…¨é¸æŠ", key=f"btn_all_{v_idx}"): set_preset(v_idx, "all")
            if bc2.button("ã‚¯ãƒªã‚¢", key=f"btn_clr_{v_idx}"): set_preset(v_idx, "clear")
            if bc3.button("1-6R", key=f"btn_frst_{v_idx}"): set_preset(v_idx, "1-6")
            if bc4.button("7-12R", key=f"btn_last_{v_idx}"): set_preset(v_idx, "7-12")

            # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚°ãƒªãƒƒãƒ‰
            chk_cols = st.columns(6)
            selected_races = []
            for r in range(1, 13):
                is_checked = st.checkbox(f"{r}R", key=f"{prefix}_r{r}")
                if is_checked:
                    selected_races.append(r)
            
            # è¨­å®šæƒ…å ±ã¾ã¨ã‚
            place_name = PLACE_NAMES.get(st.session_state[f"{prefix}_place"], "ä¸æ˜")
            summary_text = f"è¨­å®šä¸­ï¼š{st.session_state[f'{prefix}_year']}å¹´ {st.session_state[f'{prefix}_kai']}å› {place_name} {st.session_state[f'{prefix}_day']}æ—¥ç›® ({len(selected_races)}ãƒ¬ãƒ¼ã‚¹é¸æŠ)"
            st.info(summary_text)

            # ã‚¸ãƒ§ãƒ–ãƒªã‚¹ãƒˆã«è¿½åŠ 
            if selected_races:
                jobs_config.append({
                    "year": st.session_state[f"{prefix}_year"],
                    "kai": st.session_state[f"{prefix}_kai"],
                    "place": st.session_state[f"{prefix}_place"],
                    "day": st.session_state[f"{prefix}_day"],
                    "races": selected_races,
                    "place_name": place_name
                })
        else:
            st.warning("ã“ã®é–‹å‚¬è¨­å®šã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚")

st.divider()

# ==================================================
# Execution Block
# ==================================================
st.subheader("ğŸš€ å®Ÿè¡Œ")

if not jobs_config:
    st.error("å®Ÿè¡Œå¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å°‘ãªãã¨ã‚‚1ã¤ã®é–‹å‚¬ã‚’æœ‰åŠ¹ã«ã—ã€ãƒ¬ãƒ¼ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    btn_disabled = True
else:
    btn_disabled = False
    msg = "ä»¥ä¸‹ã®å†…å®¹ã§é€£ç¶šå®Ÿè¡Œã—ã¾ã™ï¼š\n"
    for job in jobs_config:
        msg += f"- ã€{job['place_name']}ã€‘ {job['day']}æ—¥ç›® : {job['races']}\n"
    st.text(msg)

if st.button("AIäºˆæƒ³ã‚’é–‹å§‹ã™ã‚‹", type="primary", disabled=btn_disabled):
    st.session_state["combined_output"] = ""
    
    # ãƒãƒƒãƒå®Ÿè¡Œå‘¼ã³å‡ºã—
    result_text = keiba_bot.run_batch_prediction(jobs_config)
    
    st.session_state["combined_output"] = result_text

# ==================================================
# Output Area
# ==================================================
if st.session_state["combined_output"]:
    st.divider()
    st.subheader("ğŸ“Œ çµ±åˆå‡ºåŠ›çµæœ")
    st.text_area(
        "å…¨é–‹å‚¬ãƒ»å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚",
        value=st.session_state["combined_output"],
        height=600
    )
