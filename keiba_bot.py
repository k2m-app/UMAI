import time
import json
import re
import requests
import streamlit as st
import streamlit.components.v1 as components
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================
# .streamlit/secrets.toml ã«ä»¥ä¸‹ã®è¨­å®šãŒå¿…è¦ã§ã™
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
NETKEIBA_ID = st.secrets.get("NETKEIBA_ID", "")
NETKEIBA_PASS = st.secrets.get("NETKEIBA_PASS", "")

DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆUIæ“ä½œã§ä¸Šæ›¸ãã•ã‚Œã¾ã™ï¼‰
YEAR = "2026"
KAI = "01"
PLACE = "02"
DAY = "01"

BASE_URL = "https://s.keibabook.co.jp"

PLACE_NAMES = {
    "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰", "04": "æ±äº¬",
    "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ", "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
}

# ç«¶é¦¬ãƒ–ãƒƒã‚¯ PLACEã‚³ãƒ¼ãƒ‰ â†’ netkeiba ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰å¤‰æ›
KEIBABOOK_TO_NETKEIBA_PLACE = {
    "08": "01",  # æœ­å¹Œ
    "09": "02",  # å‡½é¤¨
    "06": "03",  # ç¦å³¶
    "07": "04",  # æ–°æ½Ÿ
    "04": "05",  # æ±äº¬
    "05": "06",  # ä¸­å±±
    "02": "07",  # ä¸­äº¬
    "00": "08",  # äº¬éƒ½
    "01": "09",  # é˜ªç¥
    "03": "10",  # å°å€‰
}

def set_race_params(year, kai, place, day):
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°"""
    global YEAR, KAI, PLACE, DAY
    YEAR = str(year)
    KAI = str(kai).zfill(2)
    PLACE = str(place).zfill(2)
    DAY = str(day).zfill(2)

def get_current_params():
    """ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆUIè¡¨ç¤ºç”¨ï¼‰"""
    return YEAR, KAI, PLACE, DAY


# ==================================================
# â˜…netkeiba æŒ‡æ•°ã‚»ãƒ«æ­£è¦åŒ–
# ==================================================
def normalize_netkeiba_index_cell(raw: str) -> str:
    if raw is None:
        return "ç„¡"
    t = str(raw).replace("\xa0", " ").strip()
    if t == "":
        return "ç„¡"
    if "æœª" in t or "ï¼" in t or "-" in t:
        return "ç„¡"
    nums = re.findall(r"\d+", t)
    if not nums:
        return "ç„¡"
    if any(n == "1000" for n in nums):
        short = [n for n in nums if len(n) <= 3 and n != "1000"]
        return short[-1] if short else "ç„¡"
    short = [n for n in nums if len(n) <= 3]
    if short:
        return short[-1]
    return "ç„¡"


# ==================================================
# ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚³ãƒ”ãƒ¼
# ==================================================
def render_copy_button(text: str, label: str, dom_id: str):
    safe_text = json.dumps(text)
    html = f"""
    <div style="display:flex; gap:8px; align-items:center; flex-wrap:wrap;">
      <button id="{dom_id}" style="
        padding:8px 12px; border-radius:10px; border:1px solid #ddd;
        background:#fff; cursor:pointer; font-size:14px;
      ">{label}</button>
      <span id="{dom_id}-msg" style="font-size:12px; color:#666;"></span>
    </div>
    <script>
      (function() {{
        const btn = document.getElementById("{dom_id}");
        const msg = document.getElementById("{dom_id}-msg");
        if (!btn) return;
        btn.addEventListener("click", async () => {{
          try {{
            await navigator.clipboard.writeText({safe_text});
            msg.textContent = "ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";
            setTimeout(() => msg.textContent = "", 1200);
          }} catch (e) {{
            msg.textContent = "ã‚³ãƒ”ãƒ¼å¤±æ•—";
            setTimeout(() => msg.textContent = "", 2200);
          }}
        }});
      }})();
    </script>
    """
    components.html(html, height=54)


# ==================================================
# Selenium Driver (Botå¯¾ç­–å¼·åŒ–ç‰ˆ)
# ==================================================
def build_driver() -> webdriver.Chrome:
    options = Options()

    # Headlessãƒ¢ãƒ¼ãƒ‰è¨­å®šï¼ˆæœ€æ–°ç‰ˆï¼‰
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")

    # Botå¯¾ç­–è¨­å®š
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)

    # User-Agentå›ºå®š
    user_agent = (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
    options.add_argument(f"--user-agent={user_agent}")

    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(60)

    # navigator.webdriver ã‚’ undefined ã«å½è£…
    try:
        driver.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {
                "source": """
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
                window.navigator.chrome = { runtime: {} };
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['ja-JP', 'ja']
                });
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3]
                });
                """
            },
        )
    except Exception:
        pass

    return driver


# ==================================================
# ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†
# ==================================================
def login_keibabook(driver: webdriver.Chrome) -> None:
    if not KEIBA_ID or not KEIBA_PASS:
        raise RuntimeError("KEIBA_ID / KEIBA_PASS ãŒ secrets ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    driver.get(f"{BASE_URL}/login/login")
    wait = WebDriverWait(driver, 15)
    
    wait.until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
    wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))).send_keys(KEIBA_PASS)
    wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))).click()
    time.sleep(1.2)


def login_netkeiba(driver: webdriver.Chrome) -> bool:
    if not NETKEIBA_ID or not NETKEIBA_PASS:
        print("netkeiba ID/PASS not found.")
        return False

    # æ—¢ã«ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
    try:
        driver.get("https://www.netkeiba.com/")
        if "ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ" in driver.page_source or "action=logout" in driver.page_source:
            return True
    except Exception:
        pass

    try:
        # â˜…ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®URLã‚’ä½¿ç”¨
        login_url = "https://regist.netkeiba.com/account/?pid=login"
        driver.get(login_url)
        wait = WebDriverWait(driver, 10)

        # IDå…¥åŠ› (name="login_id" ãŒä¸€èˆ¬çš„ã ãŒå¿µã®ãŸã‚ wait)
        id_el = wait.until(EC.visibility_of_element_located((By.NAME, "login_id")))
        id_el.clear()
        id_el.send_keys(NETKEIBA_ID)

        # PWå…¥åŠ›
        pw_el = wait.until(EC.visibility_of_element_located((By.NAME, "pswd")))
        pw_el.clear()
        pw_el.send_keys(NETKEIBA_PASS)

        # ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³æŠ¼ä¸‹
        # ã“ã®ãƒšãƒ¼ã‚¸ã¯è¤‡æ•°ã®ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ å†…ã®é€ä¿¡ãƒœã‚¿ãƒ³ã‚’å„ªå…ˆ
        btn_candidates = [
            (By.CSS_SELECTOR, "input[type='image'][alt='ãƒ­ã‚°ã‚¤ãƒ³']"),
            (By.CSS_SELECTOR, "input[type='submit']"),
            (By.CSS_SELECTOR, "button[type='submit']"),
            (By.CSS_SELECTOR, ".Btn_Login"),
            (By.XPATH, "//button[contains(text(), 'ãƒ­ã‚°ã‚¤ãƒ³')]"),
            (By.XPATH, "//input[@value='ãƒ­ã‚°ã‚¤ãƒ³']"),
        ]
        
        clicked = False
        for how, sel in btn_candidates:
            try:
                # ãƒ•ã‚©ãƒ¼ãƒ è¿‘ãã®ãƒœã‚¿ãƒ³ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ãŸã„ãŒã€ã¾ãšã¯è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã‚’ã‚¯ãƒªãƒƒã‚¯
                btn = driver.find_element(how, sel)
                if btn.is_displayed() and btn.is_enabled():
                    btn.click()
                    clicked = True
                    break
            except Exception:
                continue
        
        if not clicked:
            print("Login button not found.")
            return False

        # ãƒšãƒ¼ã‚¸é·ç§»å¾…æ©Ÿï¼ˆURLå¤‰åŒ– or ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³å‡ºç¾ï¼‰
        # accountãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒˆãƒƒãƒ—ãªã©ã«é·ç§»ã™ã‚‹ã®ã‚’å¾…ã¤
        try:
            wait.until(lambda d: "pid=login" not in d.current_url)
        except TimeoutException:
            pass # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¦ã‚‚æˆåŠŸã—ã¦ã„ã‚‹å ´åˆãŒã‚ã‚‹

        time.sleep(2.0)

        html = driver.page_source
        if "ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ" in html or "action=logout" in html:
            return True
        
        # å¤±æ•—æ™‚ãƒ­ã‚°
        print(f"Login failed. Current URL: {driver.current_url}")
        return False

    except Exception as e:
        print(f"netkeiba login exception: {e}")
        return False


# ==================================================
# Parserï¼šç«¶é¦¬ãƒ–ãƒƒã‚¯
# ==================================================
def parse_race_info(html: str):
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"date_meet": "", "race_name": "", "cond1": "", "course_line": ""}

    racemei = racetitle.find("div", class_="racemei")
    date_meet = ""
    race_name = ""
    if racemei:
        ps = racemei.find_all("p")
        if len(ps) >= 1:
            date_meet = ps[0].get_text(strip=True)
        if len(ps) >= 2:
            race_name = ps[1].get_text(strip=True)

    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    cond1 = ""
    course_line = ""
    if racetitle_sub:
        sub_ps = racetitle_sub.find_all("p")
        if len(sub_ps) >= 1:
            cond1 = sub_ps[0].get_text(strip=True)
        if len(sub_ps) >= 2:
            course_line = sub_ps[1].get_text(" ", strip=True)

    return {"date_meet": date_meet, "race_name": race_name, "cond1": cond1, "course_line": course_line}

def parse_danwa_comments(html: str):
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table or not table.tbody:
        return {}
    danwa_dict = {}
    current_key = None
    for row in table.tbody.find_all("tr"):
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")
        if uma_td:
            text = re.sub(r"\D", "", uma_td.get_text(strip=True))
            if text:
                current_key = text
                continue
        if bamei_td and not current_key:
            text = bamei_td.get_text(strip=True)
            if text:
                current_key = text
                continue
        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current_key:
            danwa_dict[current_key] = danwa_td.get_text(strip=True)
            current_key = None
    return danwa_dict

def parse_zenkoso_interview(html: str):
    soup = BeautifulSoup(html, "html.parser")
    h2 = soup.find("h2", string=lambda s: s and "å‰èµ°" in s)
    if not h2: return {}
    table = h2.find_next("table", class_="syoin")
    if not table or not table.tbody: return {}
    rows = table.tbody.find_all("tr")
    result_dict = {}
    i = 0
    while i < len(rows):
        row = rows[i]
        if "spacer" in (row.get("class") or []):
            i += 1
            continue
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")
        if not (uma_td and bamei_td):
            i += 1
            continue
        umaban = re.sub(r"\D", "", uma_td.get_text(strip=True))
        name = bamei_td.get_text(strip=True)
        
        prev_date = ""
        prev_class = ""
        prev_finish = ""
        prev_comment = ""
        detail = rows[i+1] if i+1 < len(rows) else None
        if detail:
            syoin_td = detail.find("td", class_="syoin")
            if syoin_td:
                sdata = syoin_td.find("div", class_="syoindata")
                if sdata:
                    ps = sdata.find_all("p")
                    if ps: prev_date = ps[0].get_text(strip=True)
                    if len(ps) >= 2:
                        spans = ps[1].find_all("span")
                        if len(spans) >= 1: prev_class = spans[0].get_text(strip=True)
                        if len(spans) >= 2: prev_finish = spans[1].get_text(strip=True)
                direct = syoin_td.find_all("p", recursive=False)
                if direct:
                    txt = direct[0].get_text(strip=True)
                    if txt != "ï¼": prev_comment = txt
        if umaban:
            result_dict[umaban] = {
                "umaban": umaban, "name": name, "prev_date_course": prev_date,
                "prev_class": prev_class, "prev_finish": prev_finish, "prev_comment": prev_comment
            }
        i += 2
    return result_dict

def parse_cyokyo(html: str):
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}
    section = None
    h2 = soup.find("h2", string=lambda s: s and ("èª¿æ•™" in s or "ä¸­é–“" in s))
    if h2:
        midasi_div = h2.find_parent("div", class_="midasi")
        if midasi_div:
            section = midasi_div.find_next_sibling("div", class_="section")
    if section is None: section = soup
    tables = section.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody: continue
        rows = tbody.find_all("tr", recursive=False)
        if len(rows) < 1: continue
        header = rows[0]
        uma_td = header.find("td", class_="umaban")
        name_td = header.find("td", class_="kbamei")
        umaban = re.sub(r"\D", "", uma_td.get_text(strip=True)) if uma_td else ""
        bamei_hint = name_td.get_text(" ", strip=True) if name_td else ""
        tanpyo_td = header.find("td", class_="tanpyo")
        tanpyo = tanpyo_td.get_text(strip=True) if tanpyo_td else ""
        detail_row = rows[1] if len(rows) >= 2 else None
        detail_text = detail_row.get_text(" ", strip=True) if detail_row else ""
        payload = {"tanpyo": tanpyo, "detail": detail_text, "bamei_hint": bamei_hint}
        if umaban: cyokyo_dict[umaban] = payload
        elif bamei_hint: cyokyo_dict[bamei_hint] = payload
    return cyokyo_dict

def parse_syutuba(html: str):
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_=lambda c: c and "syutuba_sp" in c.split())
    if not table: table = soup.find("table", class_=lambda c: c and "syutuba" in c)
    if not table or not table.tbody: return {}
    result = {}
    for tr in table.tbody.find_all("tr", recursive=False):
        tds = tr.find_all("td", recursive=False)
        if not tds: continue
        umaban = re.sub(r"\D", "", tds[0].get_text(strip=True))
        if not umaban: continue
        kbamei_p = tr.find("p", class_="kbamei")
        bamei = kbamei_p.get_text(" ", strip=True) if kbamei_p else ""
        kisyu = ""
        kisyu_change = False
        kisyu_p = tr.find("p", class_="kisyu")
        if kisyu_p:
            a = kisyu_p.find("a")
            if a:
                norika = a.find("span", class_="norikawari")
                if norika:
                    kisyu_change = True
                    kisyu = norika.get_text(strip=True)
                else:
                    kisyu = a.get_text(strip=True)
            else:
                kisyu = kisyu_p.get_text(" ", strip=True)
        result[umaban] = {"umaban": umaban, "bamei": bamei, "kisyu": kisyu, "kisyu_change": kisyu_change}
    return result


# ==================================================
# Parserï¼šnetkeiba (Speed Index & Past)
# ==================================================
def parse_netkeiba_speed_index(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_=lambda c: c and ("SpeedIndex_Table" in c))
    if not table or not table.tbody: return {}
    out = {}
    for tr in table.tbody.find_all("tr", class_=lambda c: c and ("HorseList" in c.split()), recursive=False):
        um_td = tr.find("td", class_=lambda c: c and "sk__umaban" in c)
        if not um_td: continue
        umaban = re.sub(r"\D", "", um_td.get_text(" ", strip=True))
        if not umaban: continue
        def cell_text(cell_class):
            td = tr.find("td", class_=lambda c: c and cell_class in c.split())
            if not td: return "ç„¡"
            return normalize_netkeiba_index_cell(td.get_text(" ", strip=True))
        out[umaban] = {
            "index1": cell_text("sk__index1"),
            "index2": cell_text("sk__index2"),
            "index3": cell_text("sk__index3"),
            "course": cell_text("sk__max_course_index"),
            "avg5": cell_text("sk__average_index"),
        }
    return out

def parse_netkeiba_past_and_rest(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_=lambda c: c and ("SpeedIndex_Table" in c))
    if not table or not table.tbody: return {}
    out = {}
    for tr in table.tbody.find_all("tr", class_=lambda c: c and ("HorseList" in c.split()), recursive=False):
        um_td = tr.find("td", class_=lambda c: c and "sk__umaban" in c)
        if not um_td: continue
        umaban = re.sub(r"\D", "", um_td.get_text(" ", strip=True))
        if not umaban: continue
        
        rest_td = tr.find("td", class_="Rest")
        rest_text = ""
        if rest_td:
            items = [d.get_text(" ", strip=True) for d in rest_td.find_all("div", class_="Data01")]
            rest_text = " / ".join([x for x in items if x]).strip()

        past_list = []
        past_tds = tr.find_all("td", class_="Past", recursive=False)
        for past_td in past_tds:
            d01 = past_td.find("div", class_="Data01")
            date_place = d01.find("span").get_text(" ", strip=True) if d01 and d01.find("span") else ""
            final_num = d01.find("span", class_="Num").get_text(" ", strip=True) if d01 and d01.find("span", class_="Num") else ""
            
            d02 = past_td.find("div", class_="Data02")
            race_name = d02.get_text(" ", strip=True) if d02 else ""
            d05 = past_td.find("div", class_="Data05")
            course_time = d05.get_text(" ", strip=True) if d05 else ""
            d03 = past_td.find("div", class_="Data03")
            detail = d03.get_text(" ", strip=True) if d03 else ""
            d06 = past_td.find("div", class_="Data06")
            passage = d06.get_text(" ", strip=True) if d06 else ""
            if passage:
                passage += f" /æœ€çµ‚{final_num}ç€" if (final_num and final_num.isdigit()) else f" /æœ€çµ‚{final_num}"
            else:
                passage = f"ï¼ˆé€šéé †ãªã—ï¼‰ /æœ€çµ‚{final_num}" if final_num else ""
            d07 = past_td.find("div", class_="Data07")
            winner = d07.get_text(" ", strip=True) if d07 else ""

            if not (date_place or race_name or course_time): continue
            race_name = re.sub(r"\s+", " ", race_name).strip()
            past_list.append({
                "date_place": date_place, "race_name": race_name,
                "course_time": course_time, "detail": detail,
                "passage": passage, "winner": winner
            })
        out[umaban] = {"rest": rest_text, "past": past_list[:3]}
    return out


# ==================================================
# Fetchers (Seleniumä½¿ç”¨)
# ==================================================
def fetch_danwa_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/danwa/0/{race_id}"
    driver.get(url)
    time.sleep(0.8)
    html = driver.page_source
    return html, parse_race_info(html), parse_danwa_comments(html)

def fetch_zenkoso_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syoin/{race_id}"
    driver.get(url)
    time.sleep(0.8)
    return parse_zenkoso_interview(driver.page_source)

def fetch_cyokyo_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/cyokyo/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "table.cyokyo")))
    except: pass
    return parse_cyokyo(driver.page_source)

def fetch_syutuba_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syutuba/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "table.syutuba_sp, table.syutuba")))
    except: pass
    return parse_syutuba(driver.page_source)

def fetch_netkeiba_speed_html(driver, netkeiba_race_id: str) -> str:
    url = f"https://race.netkeiba.com/race/speed.html?race_id={netkeiba_race_id}&type=shutuba&mode=default"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "table.SpeedIndex_Table")))
    except: pass
    
    html = driver.page_source
    # ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ãã†ãªã‚‰å†ãƒ­ã‚°ã‚¤ãƒ³
    if ("ç„¡æ–™ä¼šå“¡ç™»éŒ²" in html or "ãƒ­ã‚°ã‚¤ãƒ³" in html) and NETKEIBA_ID and NETKEIBA_PASS:
        if login_netkeiba(driver):
            driver.get(url)
            time.sleep(1.0)
            html = driver.page_source
    return html

def keibabook_race_id_to_netkeiba_race_id(year, kai, place, day, race_num_2):
    nk_place = KEIBABOOK_TO_NETKEIBA_PLACE.get(place)
    if not nk_place: return ""
    return f"{str(year)}{nk_place}{str(kai).zfill(2)}{str(day).zfill(2)}{str(race_num_2).zfill(2)}"


# ==================================================
# è‡ªå‹•æ¤œå‡º
# ==================================================
def detect_meet_candidates():
    driver = build_driver()
    try:
        login_keibabook(driver)
        driver.get(f"{BASE_URL}/cyuou/")
        time.sleep(1.0)
        html = driver.page_source
        keys = re.findall(r"/cyuou/syutuba/(\d{12})", html)
        if not keys:
            keys = re.findall(r"/cyuou/thursday/(\d{12})", html)
        if not keys:
            driver.get(f"{BASE_URL}/")
            time.sleep(1.0)
            html2 = driver.page_source
            keys = re.findall(r"/cyuou/syutuba/(\d{12})", html2)
        
        meet10_set = set(k[:10] for k in keys if len(k) >= 10)
        candidates = []
        for m10 in sorted(meet10_set, reverse=True)[:12]:
            p = m10[6:8]
            candidates.append({
                "meet10": m10,
                "year": m10[0:4], "kai": m10[4:6], "place": p, "day": m10[8:10],
                "place_name": PLACE_NAMES.get(p, "ä¸æ˜")
            })
        return candidates
    except Exception:
        return []
    finally:
        driver.quit()


# ==================================================
# Dify Streaming
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ ã‚¨ãƒ©ãƒ¼: DIFY_API_KEY ãŒæœªè¨­å®š"
        return
    headers = {"Authorization": f"Bearer {DIFY_API_KEY}", "Content-Type": "application/json"}
    payload = {"inputs": {"text": full_text}, "response_mode": "streaming", "user": "keiba-bot-user"}
    try:
        res = requests.post("https://api.dify.ai/v1/workflows/run", headers=headers, json=payload, stream=True, timeout=300)
        if res.status_code != 200:
            yield f"âš ï¸ ã‚¨ãƒ©ãƒ¼: Dify API Error {res.status_code}"
            return
        for line in res.iter_lines():
            if not line: continue
            decoded = line.decode("utf-8", errors="ignore")
            if not decoded.startswith("data:"): continue
            json_str = decoded.replace("data: ", "")
            try:
                data = json.loads(json_str)
                event = data.get("event")
                if event == "workflow_finished":
                    outputs = data.get("data", {}).get("outputs", {})
                    if outputs:
                        txt = "\n".join([v for k,v in outputs.items() if isinstance(v, str)])
                        if txt.strip(): yield txt.strip()
                elif chunk := data.get("answer", ""):
                    yield chunk
            except: continue
    except Exception as e:
        yield f"âš ï¸ Request Error: {str(e)}"


# ==================================================
# ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯
# ==================================================
def run_all_races(target_races=None):
    race_numbers = list(range(1, 13)) if target_races is None else sorted({int(r) for r in target_races})
    base_id = f"{YEAR}{KAI}{PLACE}{DAY}"
    place_name = PLACE_NAMES.get(PLACE, "ä¸æ˜")
    
    st.markdown(f"### ğŸ {place_name}é–‹å‚¬ ãƒ‡ãƒ¼ã‚¿åˆ†æé–‹å§‹")
    
    driver = build_driver()
    combined_blocks = []

    try:
        st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        login_keibabook(driver)
        nk_login = login_netkeiba(driver)
        if nk_login: st.success("âœ… netkeiba ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ")
        else: st.warning("âš ï¸ netkeiba ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—ï¼ˆæŒ‡æ•°ã¯é–²è¦§ã§ãã‚‹ç¯„å›²ã§å–å¾—ã—ã¾ã™ï¼‰")

        for r in race_numbers:
            race_num = f"{r:02}"
            race_id = base_id + race_num
            nk_race_id = keibabook_race_id_to_netkeiba_race_id(YEAR, KAI, PLACE, DAY, race_num)
            
            st.markdown(f"#### {place_name} {r}R")
            status = st.empty()
            result_area = st.empty()
            status.info("ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")

            # Fetch
            _html, race_info, danwa = fetch_danwa_dict(driver, race_id)
            zenkoso = fetch_zenkoso_dict(driver, race_id)
            cyokyo = fetch_cyokyo_dict(driver, race_id)
            syutuba = fetch_syutuba_dict(driver, race_id)
            
            # Netkeiba
            speed_dict = {}
            past_rest_dict = {}
            if nk_race_id:
                html_spd = fetch_netkeiba_speed_html(driver, nk_race_id)
                speed_dict = parse_netkeiba_speed_index(html_spd)
                past_rest_dict = parse_netkeiba_past_and_rest(html_spd)

            # Merge
            merged = []
            umaban_list = sorted(syutuba.keys(), key=lambda x: int(x)) if syutuba else []
            if not umaban_list:
                status.warning("å‡ºé¦¬è¡¨ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚ã‚¹ã‚­ãƒƒãƒ—")
                continue

            for umaban in umaban_list:
                sb = syutuba.get(umaban, {})
                bamei = sb.get("bamei", "åç§°ä¸æ˜").strip()
                kisyu = sb.get("kisyu", "é¨æ‰‹ä¸æ˜")
                if sb.get("kisyu_change"): kisyu = f"æ›¿ãƒ»{kisyu}"
                
                # å©èˆã‚³ãƒ¡ãƒ³ãƒˆ
                d_com = danwa.get(umaban) or "æƒ…å ±ãªã—"
                
                # å‰èµ°
                z = zenkoso.get(umaban, {})
                z_txt = f"{z.get('prev_date_course','')} {z.get('prev_class','')} {z.get('prev_finish','')}".strip()
                z_com = z.get("prev_comment", "")
                prev_blk = f"  ã€å‰èµ°ã€‘ {z_txt or 'æ–°é¦¬/ä¸æ˜'}\n  ã€å‰èµ°è«‡è©±ã€‘ {z_com or 'ãªã—'}\n"
                
                # èª¿æ•™
                c = cyokyo.get(umaban, {})
                cyokyo_blk = f"  ã€èª¿æ•™ã€‘ çŸ­è©•:{c.get('tanpyo','-')} / è©³ç´°:{c.get('detail','-')}\n"
                
                # æŒ‡æ•°
                s = speed_dict.get(umaban, {})
                spd_blk = f"  ã€æŒ‡æ•°ã€‘ å‰:{s.get('index1','-')} 2èµ°:{s.get('index2','-')} 3èµ°:{s.get('index3','-')} ã‚³ãƒ¼ã‚¹:{s.get('course','-')} 5å¹³:{s.get('avg5','-')}\n"
                
                # éå»èµ°ï¼†ä¼‘é¤Š
                nr = past_rest_dict.get(umaban, {})
                rest = nr.get("rest", "")
                pasts = nr.get("past", [])
                past_lines = ["  ã€ç›´è¿‘èµ°ã€‘"]
                for i, p in enumerate(pasts, 1):
                    lbl = "å‰èµ°" if i==1 else ("2èµ°å‰" if i==2 else "3èµ°å‰")
                    line = f"ãƒ»{lbl}: {p['date_place']} / {p['race_name']} / {p['course_time']} / {p['detail']} / {p['passage']} / {p['winner']}"
                    past_lines.append("  " + line)
                    if i==1 and rest: past_lines.append(f"  ãƒ»ä¼‘é¤Š: {rest}")
                if not pasts and rest: past_lines.append(f"  ã€ä¼‘ã¿æ˜ã‘ã€‘ {rest}")
                past_blk = "\n".join(past_lines) + "\n"

                merged.append(f"â–¼[é¦¬ç•ª{umaban}] {bamei} / é¨æ‰‹:{kisyu}\n  ã€å©èˆã€‘ {d_com}\n{prev_blk}{cyokyo_blk}{spd_blk}{past_blk}")

            # AI Prompt
            header = f"{race_info.get('date_meet','')}\n{race_info.get('race_name','')}\n{race_info.get('cond1','')}\n{race_info.get('course_line','')}"
            full_prompt = f"â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n{header}\n\nä»¥ä¸‹ã¯{place_name}{r}Rã®å…¨é ­ãƒ‡ãƒ¼ã‚¿ã€‚\nâ– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿\n" + "\n".join(merged)
            
            status.info("ğŸ¤– AIåˆ†æä¸­...")
            full_ans = ""
            for chunk in stream_dify_workflow(full_prompt):
                if chunk:
                    full_ans += chunk
                    result_area.markdown(full_ans + "â–Œ")
            result_area.markdown(full_ans)
            
            if full_ans:
                status.success("å®Œäº†")
                # å±¥æ­´ä¿å­˜ãªã—
                combined_blocks.append(f"ã€{place_name} {r}Rã€‘\n{full_ans}\n")
            else:
                status.error("å›ç­”ç”Ÿæˆå¤±æ•—")

            st.write("---")

    except Exception as e:
        st.error(f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
    finally:
        driver.quit()
    
    if combined_blocks:
        st.subheader("ğŸ“Œ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚")
        all_txt = "\n".join(combined_blocks)
        render_copy_button(all_txt, "ğŸ“‹ å…¨ã¦ã‚³ãƒ”ãƒ¼", "copy_all_btn")
        st.download_button("â¬‡ï¸ txtä¿å­˜", all_txt, f"{place_name}_ALL.txt")


# ==================================================
# Streamlit Entry Point
# ==================================================
if __name__ == "__main__":
    st.set_page_config(page_title="Keiba AI Analyst", layout="wide")
    st.title("ğŸ‡ AIç«¶é¦¬äºˆæƒ³ã‚¢ãƒŠãƒªã‚¹ãƒˆ")

    with st.sidebar:
        st.header("é–‹å‚¬è¨­å®š")
        
        # è‡ªå‹•æ¤œå‡ºãƒœã‚¿ãƒ³
        if st.button("ğŸ”„ ç›´è¿‘ã®é–‹å‚¬ã‚’è‡ªå‹•æ¤œå‡º"):
            cands = detect_meet_candidates()
            if cands:
                st.session_state["candidates"] = cands
                st.success(f"{len(cands)}ä»¶æ¤œå‡º")
            else:
                st.warning("æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
        
        # å€™è£œãŒã‚ã‚‹å ´åˆã¯ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹
        candidates = st.session_state.get("candidates", [])
        if candidates:
            opts = [f"{c['meet10']} {c['place_name']}" for c in candidates]
            sel = st.selectbox("é–‹å‚¬é¸æŠ", opts)
            if sel:
                # é¸æŠã•ã‚ŒãŸå€™è£œã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡º
                idx = opts.index(sel)
                c = candidates[idx]
                s_year = c['year']
                s_kai = c['kai']
                s_place = c['place']
                s_day = c['day']
        else:
            # æ‰‹å‹•å…¥åŠ›
            s_year = st.text_input("å¹´ (YYYY)", YEAR)
            s_kai = st.text_input("å› (01~)", KAI)
            s_place = st.selectbox("å ´æ‰€", list(PLACE_NAMES.keys()), format_func=lambda x: f"{x}:{PLACE_NAMES[x]}", index=2)
            s_day = st.text_input("æ—¥ (01~)", DAY)

        target_races = st.multiselect("å¯¾è±¡ãƒ¬ãƒ¼ã‚¹ (ç©ºæ¬„ãªã‚‰å…¨ãƒ¬ãƒ¼ã‚¹)", [str(i) for i in range(1, 13)])
        
        if st.button("ğŸš€ åˆ†æé–‹å§‹", type="primary"):
            set_race_params(s_year, s_kai, s_place, s_day)
            races = [int(x) for x in target_races] if target_races else None
            run_all_races(races)
