import time
import json
import re
import requests
import streamlit as st
import streamlit.components.v1 as components
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š (ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°)
YEAR = "2026"
KAI = "01"
PLACE = "05"  # ä¸­å±±
DAY = "03"

BASE_URL = "https://s.keibabook.co.jp"

PLACE_NAMES = {
    "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰", "04": "æ±äº¬",
    "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ", "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
}

# ç«¶é¦¬ãƒ–ãƒƒã‚¯ PLACEã‚³ãƒ¼ãƒ‰ â†’ netkeiba ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
KEIBABOOK_TO_NETKEIBA_PLACE = {
    "08": "01", "09": "02", "06": "03", "07": "04", "04": "05",
    "05": "06", "02": "07", "00": "08", "01": "09", "03": "10",
}

# ==================================================
# ã€è¿½åŠ ã€‘å¤–éƒ¨å‘¼ã³å‡ºã—ç”¨ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šãƒ»å–å¾—é–¢æ•°
# ==================================================
def set_race_params(year, kai, place, day):
    global YEAR, KAI, PLACE, DAY
    YEAR = str(year)
    KAI = str(kai).zfill(2)
    PLACE = str(place).zfill(2)
    DAY = str(day).zfill(2)

def get_current_params():
    return YEAR, KAI, PLACE, DAY

# ==================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ==================================================
def _clean_text_ja(s: str) -> str:
    """å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹é™¤å»ãƒ»ç©ºç™½æ­£è¦åŒ–"""
    if not s:
        return ""
    s = s.replace("\u3000", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _is_missing_marker(s: str) -> bool:
    """æƒ…å ±ãªã—ãƒãƒ¼ã‚«ãƒ¼åˆ¤å®š"""
    t = _clean_text_ja(s)
    return t in {"ï¼", "-", "â€”", "â€•", "â€", ""}

def render_copy_button(text: str, label: str, dom_id: str):
    """ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³è¡¨ç¤º"""
    safe_text = json.dumps(text)
    html = f"""
    <div style="display:flex; gap:8px; align-items:center; flex-wrap:wrap;">
      <button id="{dom_id}" style="
        padding:8px 12px;
        border-radius:10px;
        border:1px solid #ddd;
        background:#fff;
        cursor:pointer;
        font-size:14px;
      ">{label}</button>
      <span id="{dom_id}-msg" style="font-size:12px; color:#666;"></span>
    </div>
    <script>
      (function() {{
        const btn = document.getElementById("{dom_id}");
        const msg = document.getElementById("{dom_id}-msg");
        if (!btn) return;
        btn.addEventListener("click", async () => {{
          try {{
            await navigator.clipboard.writeText({safe_text});
            msg.textContent = "ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";
            setTimeout(() => msg.textContent = "", 1200);
          }} catch (e) {{
            msg.textContent = "ã‚³ãƒ”ãƒ¼å¤±æ•—";
            setTimeout(() => msg.textContent = "", 2200);
          }}
        }});
      }})();
    </script>
    """
    components.html(html, height=54)

# ==================================================
# Selenium Setup
# ==================================================
def build_driver() -> webdriver.Chrome:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,2200")
    options.add_argument("--lang=ja-JP")
    options.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(60)
    return driver

def login_keibabook(driver: webdriver.Chrome) -> None:
    if not KEIBA_ID or not KEIBA_PASS:
        # secretsãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        pass 
    driver.get(f"{BASE_URL}/login/login")
    try:
        WebDriverWait(driver, 5).until(
            EC.visibility_of_element_located((By.NAME, "login_id"))
        ).send_keys(KEIBA_ID)
        WebDriverWait(driver, 5).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
        ).send_keys(KEIBA_PASS)
        WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))
        ).click()
        time.sleep(1.0)
    except:
        pass # æ—¢ã«ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ç­‰ã®å ´åˆ

# ==================================================
# ã€ä¿®æ­£ç‰ˆã€‘ç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼šå©èˆã®è©± (Danwa)
# èª²é¡Œâ‘ ï¼šé¦¬åãŒå–å¾—ã§ããªã„ â†’ HTMLæ§‹é€ ã«åˆã‚ã›ã¦å³å¯†ã«å–å¾—
# ==================================================
def parse_race_info_from_danwa(html: str) -> dict:
    """ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ã®å–å¾—"""
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"header_text": ""}

    racemei = racetitle.find("div", class_="racemei")
    header_parts = []
    if racemei:
        for p in racemei.find_all("p"):
            header_parts.append(p.get_text(strip=True))

    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    if racetitle_sub:
        for p in racetitle_sub.find_all("p"):
            header_parts.append(p.get_text(strip=True))
    
    return {"header_text": "\n".join(header_parts)}

def parse_danwa_horses(html: str) -> dict:
    """
    è¿”ã‚Šå€¤: { "1": {"name": "ãƒ­ãƒ¼ãƒ‰ã‚ªãƒ¼ãƒ«ãƒ©ã‚¤ãƒˆ", "danwa": "ã‚³ãƒ¡ãƒ³ãƒˆ..."}, ... }
    """
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_=lambda c: c and "danwa" in str(c))
    if not table or not table.tbody:
        return {}

    horses = {}
    current_umaban = None
    
    # trã‚’è¡Œã”ã¨ã«èµ°æŸ»
    rows = table.tbody.find_all("tr", recursive=False)
    
    for tr in rows:
        classes = tr.get("class", [])
        if "spacer" in classes:
            continue

        # --- é¦¬æƒ…å ±ã®è¡Œ (é¦¬ç•ªã¨é¦¬åãŒã‚ã‚‹) ---
        umaban_td = tr.find("td", class_="umaban")
        
        # é¦¬åã¯ class="left" ã¾ãŸã¯ class="left bamei" ã«ã‚ã‚‹
        # find("td", class_="left") ã¯ class="left bamei" ã‚‚ãƒ’ãƒƒãƒˆã™ã‚‹(BeautifulSoupä»•æ§˜)
        bamei_td = tr.find("td", class_="left") 

        if umaban_td and bamei_td:
            # é¦¬ç•ªå–å¾—
            raw_umaban = umaban_td.get_text(strip=True)
            current_umaban = re.sub(r"\D", "", raw_umaban)
            
            # é¦¬åå–å¾— (aã‚¿ã‚°ãŒã‚ã‚‹å ´åˆã¨ãªã„å ´åˆã«å¯¾å¿œ)
            anchor = bamei_td.find("a")
            if anchor:
                raw_name = anchor.get_text(strip=True)
            else:
                raw_name = bamei_td.get_text(strip=True)
            
            clean_name = _clean_text_ja(raw_name)
            
            if current_umaban:
                horses[current_umaban] = {"name": clean_name, "danwa": ""}
            continue

        # --- ã‚³ãƒ¡ãƒ³ãƒˆã®è¡Œ (class="danwa"ãŒã‚ã‚‹) ---
        danwa_td = tr.find("td", class_="danwa")
        if danwa_td and current_umaban:
            # å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—æ•´å½¢
            comment_text = danwa_td.get_text("\n", strip=True)
            comment_text = _clean_text_ja(comment_text)
            
            if horses[current_umaban]["danwa"]: 
                # ä¸‡ãŒä¸€è¤‡æ•°è¡Œã«åˆ†ã‹ã‚Œã¦ã„ãŸå ´åˆã®çµåˆ
                horses[current_umaban]["danwa"] += (" " + comment_text)
            else:
                horses[current_umaban]["danwa"] = comment_text

    return horses

def fetch_keibabook_danwa(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/danwa/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.default.danwa"))
        )
    except:
        pass
    html = driver.page_source
    return parse_race_info_from_danwa(html), parse_danwa_horses(html)


# ==================================================
# ã€ä¿®æ­£ç‰ˆã€‘ç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼šå‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ (Syoin)
# èª²é¡Œâ‘¡ï¼šã‚³ãƒ¡ãƒ³ãƒˆãŒã‚ã‚‹ã‚‚ã®ã ã‘æŠ½å‡ºã—ãŸã„
# è§£æ±ºç­–ï¼šdiv.syoindataï¼ˆãƒ¡ã‚¿æƒ…å ±ï¼‰ã‚’é™¤å»ã—ã¦ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆåˆ¤å®šã‚’è¡Œã†
# ==================================================
def parse_zenkoso_interview(html: str) -> dict:
    """
    è¿”ã‚Šå€¤: { "2": "ãƒãƒ¬ãƒ³ã‚¿ã‚¤ãƒ³ã‚¬ãƒ¼ãƒ«ï¼ˆï¼‘ç€ï¼‰å†…ç”°åšé¨æ‰‹...", ... }
    â€» ã‚³ãƒ¡ãƒ³ãƒˆãŒãªã„é¦¬ï¼ˆï¼è¡¨è¨˜ãªã©ï¼‰ã¯è¾æ›¸ã«å«ã¾ãªã„
    """
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_=lambda c: c and "syoin" in str(c))
    if not table or not table.tbody:
        return {}

    interview_data = {}
    current_umaban = None

    rows = table.tbody.find_all("tr", recursive=False)
    
    for tr in rows:
        classes = tr.get("class", [])
        if "spacer" in classes:
            continue

        # --- é¦¬æƒ…å ±ã®è¡Œ ---
        umaban_td = tr.find("td", class_="umaban")
        if umaban_td:
            raw_u = umaban_td.get_text(strip=True)
            current_umaban = re.sub(r"\D", "", raw_u)
            continue

        # --- ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼å†…å®¹ã®è¡Œ ---
        syoin_td = tr.find("td", class_="syoin")
        if syoin_td and current_umaban:
            # ä¸è¦ãªãƒ¡ã‚¿æƒ…å ± (div.syoindata) ã‚’ç‰¹å®šã—ã¦å‰Šé™¤
            meta_div = syoin_td.find("div", class_="syoindata")
            if meta_div:
                meta_div.decompose() # divã‚¿ã‚°ã¨ãã®ä¸­èº«ã‚’å®Œå…¨å‰Šé™¤

            # æ®‹ã£ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆã“ã‚ŒãŒç´”ç²‹ãªã‚³ãƒ¡ãƒ³ãƒˆéƒ¨åˆ†ï¼‰
            raw_text = syoin_td.get_text(" ", strip=True)
            clean_text = _clean_text_ja(raw_text)

            # ã€Œï¼ã€ã‚„ç©ºæ–‡å­—ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if not _is_missing_marker(clean_text) and len(clean_text) > 1:
                interview_data[current_umaban] = clean_text
            
            # é¦¬ç•ªãƒªã‚»ãƒƒãƒˆï¼ˆæ¬¡ã®è¡Œã®ãŸã‚ã«ï¼‰
            current_umaban = None

    return interview_data

def fetch_zenkoso_interview(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syoin/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.default.syoin"))
        )
    except:
        pass
    return parse_zenkoso_interview(driver.page_source)


# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼šCPUäºˆæƒ³ (ãã®ã¾ã¾åˆ©ç”¨)
# ==================================================
def parse_keibabook_cpu(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    data = {}

    # ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°
    speed_tbl = soup.find("table", id="cpu_speed_sort_table")
    if speed_tbl and speed_tbl.tbody:
        for tr in speed_tbl.tbody.find_all("tr"):
            umaban_td = tr.find("td", class_="umaban")
            if not umaban_td: continue
            umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))
            if not umaban: continue
            
            tds = tr.find_all("td")
            if len(tds) < 8: continue
            
            # æŒ‡æ•°å–å¾—é–¢æ•°ï¼ˆæ•°å€¤ä»¥å¤–ã¯0ï¼‰
            def get_v(idx):
                p = tds[idx].find("p")
                txt = re.sub(r"\D", "", p.get_text(strip=True)) if p else ""
                val = int(txt) if txt else 0
                return val if val < 900 else 0 # 1000ãªã©ã¯é™¤å¤–

            last = get_v(-1)
            two = get_v(-2)
            thr = get_v(-3)
            vals = [x for x in [last, two, thr] if x > 0]
            avg = round(sum(vals)/len(vals)) if vals else 0
            
            data[umaban] = {
                "sp_last": str(last) if last else "-",
                "sp_2": str(two) if two else "-",
                "sp_3": str(thr) if thr else "-",
                "sp_avg": str(avg) if avg else "-"
            }

    # ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
    factor_tbl = None
    for t in soup.find_all("table"):
        c = t.find("caption")
        if c and "ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼" in c.get_text():
            factor_tbl = t
            break
            
    if factor_tbl and factor_tbl.tbody:
        for tr in factor_tbl.tbody.find_all("tr"):
            umaban_td = tr.find("td", class_="umaban")
            if not umaban_td: continue
            umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))
            if not umaban: continue
            
            tds = tr.find_all("td")
            if len(tds) < 8: continue
            
            def get_m(idx):
                p = tds[idx].find("p")
                t = p.get_text(strip=True) if p else ""
                return t if t else "-"

            if umaban not in data: data[umaban] = {}
            data[umaban].update({
                "fac_crs": get_m(5), "fac_dis": get_m(6), "fac_zen": get_m(7)
            })

    return data

def fetch_keibabook_cpu_data(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/cpu/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "cpu_speed_sort_table")))
    except: pass
    return parse_keibabook_cpu(driver.page_source)


# ==================================================
# Netkeiba (æ—¢å­˜æ©Ÿèƒ½ç¶­æŒ)
# ==================================================
def fetch_netkeiba_data(driver, year, kai, place, day, race_num):
    # ç°¡æ˜“ç‰ˆï¼šIDå¤‰æ›ã—ã¦æˆ¦ç¸¾ãƒšãƒ¼ã‚¸ã¸
    nk_place = KEIBABOOK_TO_NETKEIBA_PLACE.get(place, "")
    if not nk_place: return {}
    
    nk_race_id = f"{year}{nk_place}{kai.zfill(2)}{day.zfill(2)}{race_num.zfill(2)}"
    url = f"https://race.netkeiba.com/race/shutuba_past.html?race_id={nk_race_id}&rf=shutuba_submenu"
    
    driver.get(url)
    try:
        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, "Shutuba_Past5_Table")))
    except:
        return {}
    
    soup = BeautifulSoup(driver.page_source, "html.parser")
    data = {}
    
    # é¦¬æŸ±ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰é¨æ‰‹ã¨éå»èµ°ã‚’å–å¾—
    rows = soup.find_all("tr", class_="HorseList")
    for tr in rows:
        # é¦¬ç•ª
        waku_tds = tr.find_all("td", class_="Waku")
        # æ ç•ªåˆ—ã¨é¦¬ç•ªåˆ—ãŒã‚ã‚‹ãŸã‚ã€æ•°å­—ãŒå…¥ã£ã¦ã„ã‚‹æ–¹ã‚’æ¢ã™
        umaban = ""
        for td in waku_tds:
            txt = re.sub(r"\D", "", td.get_text(strip=True))
            if txt:
                umaban = txt
                break
        if not umaban: continue

        # é¨æ‰‹
        jockey_td = tr.find("td", class_="Jockey")
        jockey = _clean_text_ja(jockey_td.get_text(strip=True)) if jockey_td else "ä¸æ˜"
        jockey = re.sub(r"\d+.*", "", jockey) # æ–¤é‡ã‚«ãƒƒãƒˆ

        # éå»èµ° (Pastã‚¯ãƒ©ã‚¹ã®tdã‚’åé›†)
        past_tds = tr.find_all("td", class_="Past")
        past_list = []
        for td in past_tds[:3]: # æœ€æ–°3èµ°
            # é–‹å‚¬æ—¥ã€ãƒ¬ãƒ¼ã‚¹åã€ç€é †ãªã©ã‚’ç°¡æ˜“å–å¾—
            txt = _clean_text_ja(td.get_text(" ", strip=True))
            if len(txt) > 5:
                # ç°¡æ˜“çš„ãªæŠ½å‡º: æ—¥ä»˜ã¨ç€é †ã‚’ç›®å°ã«
                past_list.append(txt[:50] + "...") # é•·ã™ãã‚‹ã®ã§ã‚«ãƒƒãƒˆ
            else:
                past_list.append("-")
        
        data[umaban] = {"jockey": jockey, "past": past_list}
        
    return data


# ==================================================
# Dify Streaming
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ DIFY_API_KEY æœªè¨­å®š"
        return

    payload = {
        "inputs": {"text": full_text},
        "response_mode": "streaming",
        "user": "keiba-bot",
    }
    headers = {"Authorization": f"Bearer {DIFY_API_KEY}", "Content-Type": "application/json"}
    
    try:
        res = requests.post("https://api.dify.ai/v1/workflows/run", headers=headers, json=payload, stream=True)
        for line in res.iter_lines():
            if not line: continue
            decoded = line.decode("utf-8").replace("data: ", "")
            try:
                data = json.loads(decoded)
                event = data.get("event")
                if event == "workflow_finished":
                    outputs = data.get("data", {}).get("outputs", {})
                    # Difyã®å‡ºåŠ›ã‚­ãƒ¼ã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„ (ä¾‹: 'result', 'text' etc)
                    for val in outputs.values():
                        if isinstance(val, str): yield val
                if event == "message" or "answer" in data:
                    yield data.get("answer", "")
            except: pass
    except Exception as e:
        yield f"Error: {e}"


# ==================================================
# Main Execution (Appå´ã‹ã‚‰å‘¼ã°ã‚Œã‚‹å ´åˆã«ã‚‚å¯¾å¿œ)
# ==================================================
def run_all_races(target_races=None):
    # Streamlitã®UIè¡¨ç¤ºãŒã‚ã‚‹ãŸã‚ã€app.pyã‹ã‚‰å‘¼ã¶å ´åˆã¯
    # st.*** ã®å‘¼ã³å‡ºã—å…ˆãŒapp.pyã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ãªã‚‹
    
    # å¼•æ•° target_races ãŒã‚ã‚Œã°ãã‚Œã ã‘å®Ÿè¡Œ
    race_nums = target_races if target_races else list(range(1, 13))
    race_nums = [int(r) for r in race_nums]
    
    base_id = f"{YEAR}{KAI}{PLACE}{DAY}" 
    
    driver = build_driver()
    try:
        st.info(f"ãƒ­ã‚°ã‚¤ãƒ³ä¸­... (ID: {KEIBA_ID[:2]}**)")
        login_keibabook(driver)
        st.success("ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ")
        
        combined_text = ""

        for r in race_nums:
            race_num_str = f"{r:02}"
            race_id = base_id + race_num_str
            
            st.markdown(f"### {PLACE_NAMES.get(PLACE, 'å ´')} {r}R")
            status = st.empty()
            status.text("ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")

            # 1. å©èˆã®è©± (åŸºæœ¬æƒ…å ± + ã‚³ãƒ¡ãƒ³ãƒˆ)
            header_info, danwa_data = fetch_keibabook_danwa(driver, race_id)
            if not danwa_data:
                st.error("é¦¬ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ (å©èˆã®è©±ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—)")
                continue

            # 2. CPUäºˆæƒ³
            cpu_data = fetch_keibabook_cpu_data(driver, race_id)

            # 3. å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼
            interview_data = fetch_zenkoso_interview(driver, race_id)

            # 4. Netkeiba (é¨æ‰‹ãƒ»æˆ¦ç¸¾)
            nk_data = fetch_netkeiba_data(driver, YEAR, KAI, PLACE, DAY, race_num_str)

            # --- ãƒ‡ãƒ¼ã‚¿çµ±åˆ ---
            lines = []
            # é¦¬ç•ªé †ã«ã‚½ãƒ¼ãƒˆ
            for umaban in sorted(danwa_data.keys(), key=int):
                d_info = danwa_data[umaban]
                c_info = cpu_data.get(umaban, {})
                i_text = interview_data.get(umaban, "ãªã—")
                n_info = nk_data.get(umaban, {})

                # æˆ¦ç¸¾ãƒ†ã‚­ã‚¹ãƒˆ
                past_list = n_info.get("past", [])
                past_str = " / ".join(past_list) if past_list else "æƒ…å ±ãªã—"
                
                # æŒ‡æ•°ãƒ†ã‚­ã‚¹ãƒˆ
                cpu_str = (f"æŒ‡æ•°(å‰/2/3/å¹³):{c_info.get('sp_last','-')}/{c_info.get('sp_2','-')}/"
                           f"{c_info.get('sp_3','-')}/{c_info.get('sp_avg','-')} "
                           f"F(ã‚³/è·/å‰):{c_info.get('fac_crs','-')}/{c_info.get('fac_dis','-')}/{c_info.get('fac_zen','-')}")

                line = (
                    f"â–¼é¦¬ç•ª{umaban} {d_info['name']} (é¨æ‰‹:{n_info.get('jockey','-')})\n"
                    f"ã€å©èˆã®è©±ã€‘{d_info['danwa']}\n"
                    f"ã€å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã€‘{i_text}\n"
                    f"ã€ãƒ‡ãƒ¼ã‚¿ã€‘{cpu_str}\n"
                    f"ã€è¿‘èµ°ã€‘{past_str}\n"
                )
                lines.append(line)

            full_prompt = (
                f"â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n{header_info['header_text']}\n\n"
                f"â– å„é¦¬è©³ç´°\n" + "\n".join(lines)
            )

            # AIç”Ÿæˆ
            status.text("AIåˆ†æä¸­...")
            result_area = st.empty()
            ai_output = ""
            for chunk in stream_dify_workflow(full_prompt):
                ai_output += chunk
                result_area.markdown(ai_output + "â–Œ")
            result_area.markdown(ai_output)
            
            combined_text += f"\n\n--- {r}R ---\n{ai_output}"
            
            # ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³
            render_copy_button(ai_output, f"{r}R ã‚³ãƒ”ãƒ¼", f"copy_btn_{r}")
            status.success("å®Œäº†")

        st.subheader("å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚")
        render_copy_button(combined_text, "å…¨ãƒ¬ãƒ¼ã‚¹ã‚³ãƒ”ãƒ¼", "copy_btn_all")
        st.text_area("å‡ºåŠ›çµæœ", combined_text, height=300)

    finally:
        driver.quit()

if __name__ == "__main__":
    st.title("ğŸ‡ ç«¶é¦¬AIäºˆæƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ")
    run_all_races()
