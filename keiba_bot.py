import time
import json
import re
import math
import requests
import streamlit as st
import streamlit.components.v1 as components
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup, NavigableString

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

BASE_URL = "https://s.keibabook.co.jp"

# ç«¶é¦¬ãƒ–ãƒƒã‚¯ PLACEã‚³ãƒ¼ãƒ‰ â†’ netkeiba ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
KEIBABOOK_TO_NETKEIBA_PLACE = {
    "08": "01", "09": "02", "06": "03", "07": "04", "04": "05",
    "05": "06", "02": "07", "00": "08", "01": "09", "03": "10",
}

# ==================================================
# é¦¬å ´ãƒã‚¤ã‚¢ã‚¹è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ (æ—¢å­˜ã®ã¾ã¾)
# ==================================================
BABA_BIAS_DATA = {
    "ä¸­å±±ãƒ€ãƒ¼ãƒˆ1200": {5: [6, 7, 8], 2: [5]},
    "ä¸­äº¬ãƒ€ãƒ¼ãƒˆ1400": {5: [6, 7, 8], 2: [3, 5]},
    "äº¬éƒ½ãƒ€ãƒ¼ãƒˆ1200": {5: [6, 7, 8]},
    "ä¸­å±±èŠ1200": {5: [1, 2, 3]},
    "é˜ªç¥èŠ1600": {5: [1, 2, 3]},
    "é˜ªç¥èŠ1400": {5: [1, 2, 3]},
    "é˜ªç¥èŠ1200": {5: [1, 2, 3], 2: [4]},
    "å‡½é¤¨èŠ1800": {5: [1, 2, 3]},
    "æ±äº¬èŠ2000": {5: [5], 2: [1]},
    "æ–°æ½ŸèŠ1000": {5: [7, 8], 3: [6]},
    "æ±äº¬ãƒ€ãƒ¼ãƒˆ1600": {5: [6, 8], 3: [7], 2: [5]},
    "æ±äº¬èŠ1600": {5: [6, 8]},
    "æœ­å¹Œãƒ€ãƒ¼ãƒˆ1000": {5: [7, 8]},
    "é˜ªç¥ãƒ€ãƒ¼ãƒˆ1400": {5: [8], 3: [4, 6], 2: [4, 6]},
    "æ±äº¬èŠ1400": {5: [8]},
    "äº¬éƒ½èŠ1600å†…": {5: [6]},
    "ä¸­å±±ãƒ€ãƒ¼ãƒˆ1800": {5: [7, 8], 2: [4, 5]},
    "ä¸­å±±èŠ2500": {5: [5], 3: [6, 8]},
    "ä¸­äº¬èŠ1200": {5: [2, 3], 3: [1], 2: [4, 5]},
    "äº¬éƒ½ãƒ€ãƒ¼ãƒˆ1800": {5: [6]},
    "äº¬éƒ½ãƒ€ãƒ¼ãƒˆ1900": {5: [3]},
    "äº¬éƒ½èŠ1200": {5: [7]},
    "äº¬éƒ½èŠ2400": {5: [2, 4]},
    "å°å€‰èŠ1200": {5: [7], 3: [8], 2: [6]},
    "æ–°æ½Ÿãƒ€ãƒ¼ãƒˆ1200": {5: [6, 7], 2: [4, 8]},
    "æ–°æ½ŸèŠ1600": {5: [5, 7]},
    "æ±äº¬ãƒ€ãƒ¼ãƒˆ1400": {5: [6, 7], 3: [4, 8]},
    "é˜ªç¥ãƒ€ãƒ¼ãƒˆ1800": {5: [6, 7]},
    "é˜ªç¥ãƒ€ãƒ¼ãƒˆ1200": {5: [8], 3: [5, 6, 7], 2: [4]},
    "ä¸­äº¬ãƒ€ãƒ¼ãƒˆ1200": {3: [1, 6]},
    "ä¸­å±±èŠ1600": {5: [1], 3: [2, 3, 4]},
    "ä¸­äº¬èŠ1400": {5: [3], 3: [1, 4]},
    "æ±äº¬èŠ2400": {3: [1, 3]},
    "é˜ªç¥èŠ1800": {5: [1, 3], 3: [2, 4]},
    "å‡½é¤¨èŠ2000": {5: [2], 3: [1, 5], 2: [4, 6]},
    "æœ­å¹ŒèŠ2000": {5: [1, 5], 3: [2, 3]},
    "æœ­å¹ŒèŠ1200": {3: [1, 8], 2: [6, 7]},
}


# ==================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ==================================================
def _clean_text_ja(s: str) -> str:
    if not s:
        return ""
    s = s.replace("\u3000", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _is_missing_marker(s: str) -> bool:
    t = _clean_text_ja(s)
    return t in {"ï¼", "-", "â€”", "â€•", "â€", ""}


def _safe_int(s, default=0) -> int:
    try:
        if s is None:
            return default
        if isinstance(s, (int, float)):
            return int(s)
        ss = str(s).strip()
        ss = re.sub(r"[^0-9\-]", "", ss) # æ•°å­—ã¨ãƒã‚¤ãƒŠã‚¹ä»¥å¤–å‰Šé™¤
        if ss in {"", "-", "ï¼"}:
            return default
        return int(ss)
    except:
        return default

def extract_distance_int(dist_str: str) -> int:
    """ 'ãƒ€1900' ã‚„ 'èŠ1600' ã‹ã‚‰ 1900 ç­‰ã®æ•°å€¤ã‚’æŠ½å‡º """
    match = re.search(r'(\d{3,4})', str(dist_str))
    if match:
        return int(match.group(1))
    return 0

def render_copy_button(text: str, label: str, dom_id: str):
    safe_text = json.dumps(text)
    html = f"""
    <div style="margin:5px 0;">
    <button onclick="copyToClipboard_{dom_id}()" 
            style="padding:6px 12px; background:#4CAF50; color:white; border:none; 
                   border-radius:4px; cursor:pointer; font-size:12px;">
        {label}
    </button>
    </div>
    <script>
    function copyToClipboard_{dom_id}() {{
        const text = {safe_text};
        navigator.clipboard.writeText(text).then(() => {{
        }}).catch(err => {{
        }});
    }}
    </script>
    """
    components.html(html, height=40)


# ==================================================
# ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°ãƒ»ãƒã‚¤ã‚¢ã‚¹è¨ˆç®—
# ==================================================
def compute_speed_metrics(cpu_data: dict, w_max: float = 2.0, w_last: float = 1.8, w_avg: float = 1.2) -> dict:
    raw_scores = {}
    for umaban, d in cpu_data.items():
        last = _safe_int(d.get("sp_last"), 0)
        two = _safe_int(d.get("sp_2"), 0)
        thr = _safe_int(d.get("sp_3"), 0)
        vals = [v for v in [last, two, thr] if v > 0]
        if not vals:
            continue
        
        avg = sum(vals) / len(vals)
        max_v = max(vals)
        
        denom = (w_max + w_last + w_avg)
        raw = (max_v * w_max + last * w_last + avg * w_avg) / denom
        raw_scores[umaban] = raw

    if not raw_scores:
        return {}

    values = list(raw_scores.values())
    mean = sum(values) / len(values)
    std = math.sqrt(sum((v - mean) ** 2 for v in values) / len(values)) if len(values) > 1 else 0

    out = {}
    for umaban, raw in raw_scores.items():
        if std == 0:
            hensachi = 50.0
        else:
            hensachi = 50.0 + 10.0 * (raw - mean) / std
        
        out[umaban] = {
            "raw_ability": round(raw, 2),
            "speed_index": round(hensachi, 1)
        }

    return out


def extract_race_info(race_title: str) -> dict:
    result = {
        "place": None,
        "distance": None,
        "track_type": None,
        "day": None,
        "course_variant": ""
    }
    place_day_pattern = r'(\d+)å›([^0-9]+?)(\d+)æ—¥ç›®'
    place_day_match = re.search(place_day_pattern, race_title)
    if place_day_match:
        result["place"] = place_day_match.group(2).strip()
        result["day"] = int(place_day_match.group(3))
    distance_pattern = r'(\d{3,4})m'
    distance_match = re.search(distance_pattern, race_title)
    if distance_match:
        result["distance"] = distance_match.group(1)
    if 'ãƒ€ãƒ¼ãƒˆ' in race_title:
        result["track_type"] = "dirt"
    elif 'èŠ' in race_title:
        result["track_type"] = "turf"
    if 'å†…' in race_title:
        result["course_variant"] = "å†…"
    elif 'å¤–' in race_title:
        result["course_variant"] = "å¤–"
    return result


def calculate_baba_bias(waku: int, race_title: str) -> dict:
    kaisai_bias = 0
    course_bias = 0
    race_info = extract_race_info(race_title)
    place_name = race_info["place"]
    distance = race_info["distance"]
    track_type = race_info["track_type"]
    race_day = race_info["day"]
    course_variant = race_info["course_variant"]
    if track_type == "turf" and race_day in [1, 2]:
        if waku == 1: kaisai_bias = 5
        elif waku == 2: kaisai_bias = 3
        elif waku == 3: kaisai_bias = 2
    if place_name and distance and track_type:
        track_str = "èŠ" if track_type == "turf" else "ãƒ€ãƒ¼ãƒˆ"
        course_key = f"{place_name}{track_str}{distance}{course_variant}"
        if course_key in BABA_BIAS_DATA:
            bias_data = BABA_BIAS_DATA[course_key]
            for points in [5, 3, 2]:
                if points in bias_data and waku in bias_data[points]:
                    course_bias = points
                    break
    return {
        "kaisai_bias": kaisai_bias,
        "course_bias": course_bias,
        "total": kaisai_bias + course_bias
    }


# ==================================================
# Selenium Setup
# ==================================================
def build_driver() -> webdriver.Chrome:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,2200")
    options.add_argument("--lang=ja-JP")
    options.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(60)
    return driver


def login_keibabook(driver: webdriver.Chrome) -> None:
    if not KEIBA_ID or not KEIBA_PASS:
        return
    driver.get(f"{BASE_URL}/login/login")
    try:
        WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
        WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))).send_keys(KEIBA_PASS)
        WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))).click()
        time.sleep(1.0)
    except:
        pass


# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•°
# ==================================================
def fetch_keibabook_danwa(driver, race_id: str):
    # (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜)
    url = f"{BASE_URL}/cyuou/danwa/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "table.default.danwa")))
    except: pass
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    header_parts = []
    if racetitle:
        for p in racetitle.find_all("p"):
            header_parts.append(p.get_text(strip=True))
    header_info = {"header_text": "\n".join(header_parts)}
    
    table = soup.find("table", class_=lambda c: c and "danwa" in str(c))
    horses = {}
    if table and table.tbody:
        current_umaban = None
        current_waku = None
        for tr in table.tbody.find_all("tr", recursive=False):
            if "spacer" in tr.get("class", []): continue
            waku_td = tr.find("td", class_="waku")
            umaban_td = tr.find("td", class_="umaban")
            bamei_td = tr.find("td", class_="left")
            if waku_td and umaban_td and bamei_td:
                waku_p = waku_td.find("p")
                if waku_p:
                    for cls in waku_p.get("class", []):
                        if cls.startswith("waku"):
                            current_waku = re.sub(r"\D", "", cls)
                            break
                current_umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))
                clean_name = _clean_text_ja(bamei_td.get_text(strip=True))
                horses[current_umaban] = {"name": clean_name, "waku": current_waku or "?", "danwa": ""}
                continue
            danwa_td = tr.find("td", class_="danwa")
            if danwa_td and current_umaban:
                txt = _clean_text_ja(danwa_td.get_text("\n", strip=True))
                horses[current_umaban]["danwa"] = (horses[current_umaban]["danwa"] + " " + txt).strip()
    return header_info, horses


def fetch_keibabook_chokyo(driver, race_id: str):
    # (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜)
    url = f"{BASE_URL}/cyuou/cyokyo/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, "cyokyo")))
    except: pass
    soup = BeautifulSoup(driver.page_source, "html.parser")
    data = {}
    tables = soup.find_all("table", class_="cyokyo")
    for tbl in tables:
        umaban_td = tbl.find("td", class_="umaban")
        if not umaban_td: continue
        umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))
        tanpyo_td = tbl.find("td", class_="tanpyo")
        tanpyo = _clean_text_ja(tanpyo_td.get_text(strip=True)) if tanpyo_td else "ãªã—"
        detail_cell = tbl.find("td", colspan="5")
        details_text_parts = []
        if detail_cell:
            current_header_info = ""
            for child in detail_cell.children:
                if isinstance(child, NavigableString): continue
                if child.name == 'dl' and 'dl-table' in child.get('class', []):
                    dt_texts = [dt.get_text(" ", strip=True) for dt in child.find_all('dt')]
                    current_header_info = " ".join([t for t in dt_texts if t])
                elif child.name == 'table' and 'cyokyodata' in child.get('class', []):
                    time_tr = child.find('tr', class_='time')
                    time_str = ""
                    if time_tr:
                        times = []
                        for td in time_tr.find_all('td'):
                            txt = td.get_text(strip=True)
                            if txt: times.append(txt)
                        time_str = "-".join(times)
                    awase_tr = child.find('tr', class_='awase')
                    awase_str = ""
                    if awase_tr:
                        awase_txt = _clean_text_ja(awase_tr.get_text(strip=True))
                        if awase_txt: awase_str = f" (ä½µã›: {awase_txt})"
                    if current_header_info or time_str:
                        details_text_parts.append(f"[{current_header_info}] {time_str}{awase_str}")
                    current_header_info = ""
        full_details = "\n".join(details_text_parts) if details_text_parts else "è©³ç´°ãªã—"
        data[umaban] = {"tanpyo": tanpyo, "details": full_details}
    return data


def fetch_zenkoso_interview(driver, race_id: str):
    # (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜)
    url = f"{BASE_URL}/cyuou/syoin/{race_id}"
    driver.get(url)
    try: WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "table.default.syoin")))
    except: pass
    soup = BeautifulSoup(driver.page_source, "html.parser")
    interview_data = {}
    table = soup.find("table", class_=lambda c: c and "syoin" in str(c))
    if table and table.tbody:
        current_umaban = None
        for tr in table.tbody.find_all("tr", recursive=False):
            umaban_td = tr.find("td", class_="umaban")
            if umaban_td:
                current_umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))
                continue
            syoin_td = tr.find("td", class_="syoin")
            if syoin_td and current_umaban:
                meta = syoin_td.find("div", class_="syoindata")
                if meta: meta.decompose()
                txt = _clean_text_ja(syoin_td.get_text(" ", strip=True))
                if not _is_missing_marker(txt): interview_data[current_umaban] = txt
    return interview_data


def fetch_keibabook_cpu_data(driver, race_id: str, is_shinba: bool = False):
    # (æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜)
    url = f"{BASE_URL}/cyuou/cpu/{race_id}"
    driver.get(url)
    try: WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, "main")))
    except: pass
    soup = BeautifulSoup(driver.page_source, "html.parser")
    data = {}
    speed_tbl = soup.find("table", id="cpu_speed_sort_table")
    if speed_tbl and speed_tbl.tbody:
        for tr in speed_tbl.tbody.find_all("tr"):
            umaban_td = tr.find("td", class_="umaban")
            if not umaban_td: continue
            umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))
            tds = tr.find_all("td")
            if len(tds) < 8: continue
            def get_v(idx):
                p = tds[idx].find("p")
                txt = re.sub(r"\D", "", p.get_text(strip=True)) if p else ""
                val = int(txt) if txt else 0
                return val if val < 900 else 0
            data[umaban] = {"sp_last": get_v(-1), "sp_2": get_v(-2), "sp_3": get_v(-3)}
    
    factor_tbl = None
    for t in soup.find_all("table"):
        cap = t.find("caption")
        if cap and "ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼" in cap.get_text():
            factor_tbl = t; break
    if factor_tbl and factor_tbl.tbody:
        for tr in factor_tbl.tbody.find_all("tr"):
            umaban_td = tr.find("td", class_="umaban")
            if not umaban_td: continue
            umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))
            tds = tr.find_all("td")
            if len(tds) < 6: continue
            def get_m(idx):
                p = tds[idx].find("p")
                return p.get_text(strip=True) if p else "-"
            if umaban not in data: data[umaban] = {}
            if is_shinba:
                data[umaban].update({"fac_deashi": get_m(5), "fac_kettou": get_m(6), "fac_ugoki": get_m(8)})
            else:
                data[umaban].update({"fac_crs": get_m(5), "fac_dis": get_m(6), "fac_zen": get_m(7)})
    return data


# ==================================================
# Netkeiba & è¿‘èµ°æŒ‡æ•° & å¯¾æˆ¦è¡¨ç”¨ãƒ‡ãƒ¼ã‚¿åé›†
# ==================================================
def calculate_passing_order_bonus(pass_str: str, final_rank: int) -> float:
    """
    é€šéé †ã«ã‚ˆã‚‹è¿‘èµ°æŒ‡æ•°ãƒœãƒ¼ãƒŠã‚¹è¨ˆç®—
    ä¾‹: 10-10-14 -> 6
    10->14ã§4ã¤ä¸‹ãŒã£ãŸãŒã€æœ€çµ‚ç€é †6(14ã‚ˆã‚Šè‰¯ã„)ãªã®ã§ãƒœãƒ¼ãƒŠã‚¹8.0
    """
    if not pass_str or pass_str == "-":
        return 0.0
    
    # 10-10-14 (38.7) ã®ã‚ˆã†ãªå½¢å¼ã‹ã‚‰æ•°å€¤ãƒªã‚¹ãƒˆã‚’æŠ½å‡º
    # ã‚«ãƒƒã‚³æ›¸ããªã©ã‚’é™¤å»ã—ã¦ãƒã‚¤ãƒ•ãƒ³åŒºåˆ‡ã‚Šã ã‘è¦‹ã‚‹
    clean_pass = re.sub(r"\(.*?\)", "", pass_str).strip()
    parts = clean_pass.split("-")
    
    # æ•°å€¤ã«å¤‰æ›
    positions = []
    for p in parts:
        try:
            positions.append(int(p))
        except:
            pass
            
    if len(positions) < 2:
        return 0.0
    
    max_bonus = 0.0
    
    # é€šéé †ãƒªã‚¹ãƒˆã‚’èµ°æŸ»ã—ã¦ã€Œä¸‹ãŒã‚Šã€ã‚’æ¤œçŸ¥
    for i in range(1, len(positions)):
        prev = positions[i-1]
        curr = positions[i]
        
        # é †ä½ãŒä¸‹ãŒã£ãŸï¼ˆæ•°å€¤ãŒå¤§ãããªã£ãŸï¼‰å ´åˆ
        drop = curr - prev
        
        if drop > 0:
            # æ¡ä»¶â‘ : 4ã¤ä»¥ä¸Šä¸‹ãŒã£ãŸã®ã«ã€æœ€çµ‚ç€é †ãŒãã®ã€Œä¸‹ãŒã£ãŸä½ç½®ã€ã‚ˆã‚Šè‰¯ã„
            if drop >= 4 and final_rank < curr:
                # 8.0ç‚¹ (å„ªå…ˆåº¦é«˜)
                return 8.0
            
            # æ¡ä»¶â‘¡: 2ã¤ä»¥ä¸Šä¸‹ãŒã£ãŸã®ã«ã€æœ€çµ‚ç€é †ãŒãã®ã€Œä¸‹ãŒã£ãŸä½ç½®ã€ã‚ˆã‚Šè‰¯ã„
            if drop >= 2 and final_rank < curr:
                # 5.0ç‚¹ (8.0ãŒãªã‘ã‚Œã°é©ç”¨)
                max_bonus = max(max_bonus, 5.0)
                
    return max_bonus


def fetch_netkeiba_data(driver, year, kai, place, day, race_num, horse_name_map):
    """
    horse_name_map: {umaban_str: horse_name} ã‚’å—ã‘å–ã‚Šã€å¯¾æˆ¦è¡¨ä½œæˆç”¨ã«åå‰ã‚‚ä¿æŒã™ã‚‹
    """
    nk_place = KEIBABOOK_TO_NETKEIBA_PLACE.get(place, "")
    if not nk_place: return {}
    nk_race_id = f"{year}{nk_place}{kai.zfill(2)}{day.zfill(2)}{race_num.zfill(2)}"
    url = f"https://race.netkeiba.com/race/shutuba_past.html?race_id={nk_race_id}"
    driver.get(url)
    try: WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, "Shutuba_Past5_Table")))
    except: return {}
    soup = BeautifulSoup(driver.page_source, "html.parser")
    data = {}
    
    for tr in soup.find_all("tr", class_="HorseList"):
        umaban_tds = tr.find_all("td", class_="Waku")
        umaban = ""
        for td in umaban_tds:
            txt = re.sub(r"\D", "", td.get_text(strip=True))
            if txt: umaban = txt; break
        if not umaban: continue
        
        jockey_td = tr.find("td", class_="Jockey")
        jockey = _clean_text_ja(jockey_td.get_text(strip=True)) if jockey_td else "ä¸æ˜"
        
        past_str_list = []
        history_raw_data = [] # å¯¾æˆ¦è¡¨ç”¨ç”Ÿãƒ‡ãƒ¼ã‚¿
        valid_runs = []
        
        # é¦¬åå–å¾—ï¼ˆå¯¾æˆ¦è¡¨ãƒãƒƒãƒãƒ³ã‚°ç”¨ï¼‰
        horse_name = horse_name_map.get(umaban, "Unknown")

        # ç›´è¿‘3èµ°ã‚’å–å¾—
        for td in tr.find_all("td", class_="Past")[:3]:
            if "Rest" in td.get("class", []):
                past_str_list.append("(æ”¾ç‰§/ä¼‘é¤Š)")
            else:
                # --- ãƒ‡ãƒ¼ã‚¿æŠ½å‡º ---
                # æ—¥ä»˜ãƒ»å ´æ‰€
                d01 = td.find("div", class_="Data01")
                date_place = ""
                # ãƒ¬ãƒ¼ã‚¹åãƒ»è·é›¢
                d02 = td.find("div", class_="Data02")
                race_name_dist = ""
                
                # ãƒ¬ãƒ¼ã‚¹IDæŠ½å‡ºï¼ˆãƒªãƒ³ã‚¯ã‹ã‚‰ï¼‰
                past_race_id = None
                
                if d01:
                    first_span = d01.find("span")
                    if first_span: date_place = _clean_text_ja(first_span.get_text(strip=True))
                    else: date_place = _clean_text_ja(d01.get_text(strip=True))
                
                if d02:
                    race_name_dist = _clean_text_ja(d02.get_text(strip=True))
                    # ãƒªãƒ³ã‚¯å–å¾—
                    a_tag = d02.find("a")
                    if a_tag and a_tag.get("href"):
                        href = a_tag.get("href")
                        # href="../race/result.html?race_id=202506050911&rf=race_list"
                        rid_match = re.search(r'race_id=(\d+)', href)
                        if rid_match:
                            past_race_id = rid_match.group(1)

                # ç€é †
                rank_tag = td.find("span", class_="Num") or td.find("div", class_="Rank")
                rank = rank_tag.get_text(strip=True) if rank_tag else "?"
                
                # é€šéé †
                d06 = td.find("div", class_="Data06")
                passing_order = ""
                if d06:
                    raw_d06 = d06.get_text(strip=True)
                    match = re.match(r'^([\d\-]+)', raw_d06)
                    if match: passing_order = match.group(1)
                
                pass_str = f" {passing_order}â†’" if passing_order else " "
                txt = f"[{date_place} {race_name_dist}{pass_str}{rank}ç€]"
                past_str_list.append(txt)
                
                rank_int = 99
                try:
                    rank_int = int(re.sub(r"\D", "", rank))
                    
                    # --- è¿‘èµ°æŒ‡æ•°ç”¨ãƒ‡ãƒ¼ã‚¿è“„ç© ---
                    # â‘  é€šå¸¸ç‚¹: 3ç€ä»¥å†…ãªã‚‰3ç‚¹ (å¾Œã§minã‚’ã¨ã‚‹) -> å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ç¶­æŒ
                    # â‘¡ ãƒœãƒ¼ãƒŠã‚¹è¨ˆç®—
                    bonus = calculate_passing_order_bonus(passing_order, rank_int)
                    valid_runs.append({"rank_int": rank_int, "bonus": bonus})
                    
                    # --- å¯¾æˆ¦è¡¨ç”¨ãƒ‡ãƒ¼ã‚¿è“„ç© ---
                    if past_race_id:
                        history_raw_data.append({
                            "race_id": past_race_id,
                            "date": date_place.split(" ")[0] if " " in date_place else date_place, # æ—¥ä»˜ç°¡æ˜“å–å¾—
                            "name": race_name_dist,
                            "rank": rank, # è¡¨ç¤ºç”¨æ–‡å­—åˆ—
                            "rank_int": rank_int, # ã‚½ãƒ¼ãƒˆç”¨
                            "horse_name": horse_name
                        })

                except: pass
        
        # è¿‘èµ°æŒ‡æ•°è¨ˆç®—
        # åŸºæœ¬ç‚¹: 3ç€ä»¥å†…å›æ•° * 3 (MAX 9) â€»å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ãŒ sum(3 for...) ã ã£ãŸã®ã§ãã‚Œã«åˆã‚ã›ã‚‹ãŒ
        # ã“ã“ã§ã¯ã€Œç€é †ãŒè‰¯ã„ã€ã“ã¨ã®è©•ä¾¡ã¨ã€Œå·»ãè¿”ã—ã€ã®è©•ä¾¡ã‚’åˆã‚ã›ã‚‹
        
        # å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯: sum(3 for r in valid_runs if r["rank_int"] <= 3) -> æœ€å¤§9ç‚¹
        base_score = sum(3 for r in valid_runs if r["rank_int"] <= 3)
        
        # ãƒœãƒ¼ãƒŠã‚¹ã®æœ€å¤§å€¤ã‚’åŠ ç®—ï¼ˆ1ãƒ¬ãƒ¼ã‚¹ã§ã‚‚å‡„ã¾ã˜ã„å·»ãè¿”ã—ãŒã‚ã‚Œã°è©•ä¾¡ï¼‰
        max_bonus = max([r["bonus"] for r in valid_runs], default=0.0)
        
        final_index = float(min(base_score + max_bonus, 10.0)) # ä¸Šé™10ã§ã‚­ãƒ£ãƒƒãƒ—ã™ã‚‹å ´åˆ
        # â€»ã‚‚ã—ãƒœãƒ¼ãƒŠã‚¹ã§10ã‚’è¶…ãˆã¦ã‚¢ãƒ”ãƒ¼ãƒ«ã—ãŸã„å ´åˆã¯ã‚­ãƒ£ãƒƒãƒ—ã‚’å¤–ã—ã¦ã‚‚è‰¯ã„ã§ã™ãŒã€
        # å…ƒã®ã‚³ãƒ¼ãƒ‰ã«åˆã‚ã›ã¦ä¸€æ—¦ min(..., 10) ã¨ã—ã¦ãŠãã¾ã™ã€‚
        # ãŸã ã—ãƒœãƒ¼ãƒŠã‚¹8ç‚¹ãŒå…¥ã‚‹ã¨ã™ãã«ã‚«ãƒ³ã‚¹ãƒˆã™ã‚‹ã®ã§ã€ä¸Šé™ã‚’å°‘ã—è§£æ”¾ã™ã‚‹ã‹ã€
        # ã‚ã‚‹ã„ã¯ã€ŒæŒ‡æ•°ã€ã¨ã—ã¦æ‰±ã†ãªã‚‰ãã®ã¾ã¾è¶³ã—ã¦ã‚‚è‰¯ã„ã§ã™ã€‚
        # ã“ã“ã§ã¯ã€Œæœ€å¤§å€¤10ã®æŒ‡æ•°ã€ã¨ã„ã†å‰æã‚’å°‘ã—ç·©ã‚ã€ãƒœãƒ¼ãƒŠã‚¹åˆ†ã¯ç´ ç›´ã«ä¹—ã›ã¾ã™(ãŸã ã—è¡¨ç¤ºç­‰ã®éƒ½åˆã§èª¿æ•´)
        
        final_index_val = base_score + max_bonus
        
        data[umaban] = {
            "jockey": jockey, 
            "past": past_str_list, 
            "kinsou_index": final_index_val,
            "history_raw": history_raw_data # å¯¾æˆ¦è¡¨ç”Ÿæˆç”¨ã«ä¿æŒ
        }
    return data


# ==================================================
# å¯¾æˆ¦è¡¨ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
# ==================================================
def generate_battle_matrix(all_horses_data: dict, current_race_distance_str: str) -> str:
    """
    å…¨é¦¬ã®éå»èµ°ãƒ‡ãƒ¼ã‚¿(history_raw)ã‚’é›†è¨ˆã—ã€åŒä¸€ãƒ¬ãƒ¼ã‚¹ã«å‡ºèµ°ã—ãŸé¦¬ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆã™ã‚‹
    """
    # ç¾åœ¨ã®ãƒ¬ãƒ¼ã‚¹ã®è·é›¢æ•°å€¤
    current_dist = extract_distance_int(current_race_distance_str)
    
    # å…¨éå»ãƒ¬ãƒ¼ã‚¹ã‚’ RaceID ã‚’ã‚­ãƒ¼ã«é›†ç´„
    # map: { race_id: { "info": {date, name}, "horses": [ {name, rank, rank_int}, ... ] } }
    race_map = {}
    
    for umaban, d in all_horses_data.items():
        history = d.get("history_raw", [])
        for h in history:
            rid = h["race_id"]
            if rid not in race_map:
                race_map[rid] = {
                    "info": {"date": h["date"], "name": h["name"], "id": rid},
                    "horses": []
                }
            race_map[rid]["horses"].append({
                "name": h["horse_name"],
                "rank": h["rank"],
                "rank_int": h["rank_int"]
            })
            
    # å‡ºèµ°é¦¬ãŒ2é ­ä»¥ä¸Šã®ãƒ¬ãƒ¼ã‚¹ã®ã¿æŠ½å‡º
    battles = []
    for rid, data in race_map.items():
        if len(data["horses"]) >= 2:
            # ç€é †ã§ã‚½ãƒ¼ãƒˆ
            sorted_horses = sorted(data["horses"], key=lambda x: x["rank_int"])
            battles.append({
                "info": data["info"],
                "horses": sorted_horses
            })
            
    if not battles:
        return "å¯¾æˆ¦ãƒ‡ãƒ¼ã‚¿ãªã—"

    # æ—¥ä»˜é †ï¼ˆæ–°ã—ã„é †ï¼‰ç­‰ã§ã‚½ãƒ¼ãƒˆã—ãŸã„ãŒã€æ—¥ä»˜æ–‡å­—åˆ—è§£æãŒè¤‡é›‘ãªãŸã‚
    # IDã®é™é †ï¼ˆæ¦‚ã­æ–°ã—ã„é †ï¼‰ã§ç°¡æ˜“ã‚½ãƒ¼ãƒˆ
    battles.sort(key=lambda x: x["info"]["id"], reverse=True)
    
    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
    output_lines = ["\nã€å¯¾æˆ¦è¡¨ã€‘"]
    for battle in battles:
        info = battle["info"]
        
        # è·é›¢å·®è¨ˆç®—
        past_dist = extract_distance_int(info["name"])
        diff = past_dist - current_dist
        diff_str = f"{diff:+}m" if diff != 0 else "Â±0m"
        
        # URLç”Ÿæˆ
        # https://race.netkeiba.com/race/result.html?race_id=202608010202&rf=race_list
        race_url = f"https://race.netkeiba.com/race/result.html?race_id={info['id']}&rf=race_list"
        
        header = f"ãƒ»{info['date']} {info['name']} ({diff_str})"
        url_line = f"URLï¼š{race_url}"
        
        # ç€é †ãƒªã‚¹ãƒˆ
        # 4ç€ãƒ•ã‚£ã‚µãƒ–ãƒ­ã‚¹ã€€5ç€ãƒ†ã‚¹ã‚¿ãƒ´ã‚§ãƒ­ãƒ¼ãƒã‚§
        horse_results = []
        for h in battle["horses"]:
            horse_results.append(f"{h['rank']}ç€{h['name']}")
        
        results_line = "ç€é †ï¼š" + "ã€€".join(horse_results)
        
        output_lines.append(header)
        output_lines.append(url_line)
        output_lines.append(results_line)
        output_lines.append("") # ç©ºè¡Œ
        
    return "\n".join(output_lines)


# ==================================================
# Dify Streaming
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ DIFY_API_KEY æœªè¨­å®š"
        return
    payload = {"inputs": {"text": full_text}, "response_mode": "streaming", "user": "keiba-bot"}
    headers = {"Authorization": f"Bearer {DIFY_API_KEY}", "Content-Type": "application/json"}
    try:
        res = requests.post("https://api.dify.ai/v1/workflows/run", headers=headers, json=payload, stream=True, timeout=90)
        for line in res.iter_lines():
            if not line: continue
            decoded = line.decode("utf-8").replace("data: ", "")
            try:
                data = json.loads(decoded)
                if data.get("event") == "workflow_finished":
                    outputs = data.get("data", {}).get("outputs", {})
                    for val in outputs.values():
                        if isinstance(val, str): yield val
                elif "answer" in data:
                    yield data.get("answer", "")
            except: pass
    except Exception as e:
        yield f"Error: {e}"


# ==================================================
# Main Execution (Batch)
# ==================================================
def run_batch_prediction(jobs_config):
    full_output_log = ""
    for job_idx, job in enumerate(jobs_config):
        driver = build_driver()
        try:
            st.info(f"[{job_idx+1}/{len(jobs_config)}] ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†ä¸­...")
            login_keibabook(driver)
            
            year = job["year"]
            kai = str(job["kai"]).zfill(2)
            place = str(job["place"]).zfill(2)
            day = str(job["day"]).zfill(2)
            place_name = job["place_name"]
            base_id = f"{year}{kai}{place}{day}"
            
            st.markdown(f"## ğŸ {place_name}é–‹å‚¬")
            full_output_log += f"\n\n--- {place_name} ---\n"

            for r in sorted(job["races"]):
                race_num_str = f"{r:02}"
                race_id = base_id + race_num_str
                st.markdown(f"### {place_name} {r}R")
                status = st.empty()
                status.text("ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
                
                # 1. ç«¶é¦¬ãƒ–ãƒƒã‚¯è«‡è©±ï¼ˆé¦¬åãƒ»æ é †å–å¾—ï¼‰
                header_info, danwa_data = fetch_keibabook_danwa(driver, race_id)
                if not danwa_data:
                    st.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {race_id}")
                    continue
                
                race_title = header_info.get("header_text", "")
                is_shinba = any(x in race_title for x in ["æ–°é¦¬", "ãƒ¡ã‚¤ã‚¯ãƒ‡ãƒ“ãƒ¥ãƒ¼"])
                
                # 2. ç«¶é¦¬ãƒ–ãƒƒã‚¯CPUæŒ‡æ•°
                cpu_data = fetch_keibabook_cpu_data(driver, race_id, is_shinba=is_shinba)
                speed_metrics = compute_speed_metrics(cpu_data)
                
                # 3. ç«¶é¦¬ãƒ–ãƒƒã‚¯ãã®ä»–
                interview_data = fetch_zenkoso_interview(driver, race_id)
                chokyo_data = fetch_keibabook_chokyo(driver, race_id)
                
                # 4. Netkeibaãƒ‡ãƒ¼ã‚¿ï¼ˆè¿‘èµ°æŒ‡æ•°ãƒ»å¯¾æˆ¦è¡¨ç”¨å±¥æ­´å«ã‚€ï¼‰
                # é¦¬åãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆï¼ˆé¦¬ç•ª -> é¦¬åï¼‰
                horse_name_map = {u: d["name"] for u, d in danwa_data.items()}
                nk_data = fetch_netkeiba_data(driver, year, kai, place, day, race_num_str, horse_name_map)
                
                lines = []
                for umaban in sorted(danwa_data.keys(), key=int):
                    d = danwa_data[umaban]
                    sm = speed_metrics.get(umaban, {})
                    n = nk_data.get(umaban, {})
                    c = cpu_data.get(umaban, {})
                    k = chokyo_data.get(umaban, {"tanpyo": "-", "details": "-"})
                    bias = calculate_baba_bias(int(d["waku"]) if d["waku"].isdigit() else 0, race_title)
                    
                    sp_val = sm.get("speed_index", "-")
                    sp_str = f"ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°(åå·®å€¤):{sp_val}"
                    
                    kinsou_idx = n.get("kinsou_index", 0.0)
                    fac_str = f"F:{c.get('fac_deashi','-')}/{c.get('fac_kettou','-')}" if is_shinba else f"F:{c.get('fac_crs','-')}/{c.get('fac_dis','-')}"
                    
                    line = (
                        f"â–¼{d['waku']}æ {umaban}ç•ª {d['name']} (é¨æ‰‹:{n.get('jockey','-')})\n"
                        f"ã€ãƒ‡ãƒ¼ã‚¿ã€‘{sp_str} ãƒã‚¤ã‚¢ã‚¹:{bias['total']} è¿‘èµ°æŒ‡æ•°:{kinsou_idx:.1f} {fac_str}\n"
                        f"ã€å©èˆã€‘{d['danwa']}\n"
                        f"ã€å‰èµ°ã€‘{interview_data.get(umaban, 'ãªã—')}\n"
                        f"ã€èª¿æ•™ã€‘{k['tanpyo']} \n{k['details']}\n"
                        f"ã€è¿‘èµ°ã€‘{' / '.join(n.get('past', []))}\n"
                    )
                    lines.append(line)

                # 5. å¯¾æˆ¦è¡¨ç”Ÿæˆ
                battle_matrix_text = generate_battle_matrix(nk_data, extract_race_info(race_title).get("distance", ""))

                full_prompt = f"â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n{race_title}\n\nâ– å„é¦¬è©³ç´°\n" + "\n".join(lines) + "\n" + battle_matrix_text
                
                status.text("AIåˆ†æä¸­...")
                result_area = st.empty()
                ai_output = ""
                
                # Difyã‹ã‚‰ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å›ç­”ã‚’è¡¨ç¤º
                for chunk in stream_dify_workflow(full_prompt):
                    ai_output += chunk
                    result_area.markdown(ai_output + "â–Œ")
                result_area.markdown(ai_output)
                
                # çµæœãƒ­ã‚°ã¨ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³
                # å¯¾æˆ¦è¡¨ã‚‚AIã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã«ã¯å«ã¾ã‚Œã‚‹ã‚ˆã†ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æŠ•ã’ã¦ã„ã‚‹ãŒã€
                # ã‚‚ã—AIãŒãã‚Œã‚’ç„¡è¦–ã—ãŸå ´åˆã§ã‚‚ã€ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¦‹ã‚Œã‚‹ã‚ˆã†ã«
                # å¿…è¦ã§ã‚ã‚Œã°ã“ã“ã«è¿½åŠ è¡¨ç¤ºã—ã¦ã‚‚ã‚ˆã„ã€‚ä»Šå›ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã¦AIã«è§£æã•ã›ã‚‹æƒ³å®šã€‚
                full_output_log += f"\n{race_title}\n{ai_output}\n"
                render_copy_button(ai_output, f"{r}Rã‚³ãƒ”ãƒ¼", f"cp_{base_id}_{r}")
                status.success("å®Œäº†")
                
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            driver.quit()
    return full_output_log

# Streamlit UI
if __name__ == "__main__":
    st.set_page_config(page_title="AIç«¶é¦¬äºˆæƒ³", layout="wide")
    st.title("AIç«¶é¦¬äºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ  Pro")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    st.sidebar.header("é–‹å‚¬è¨­å®š")
    s_year = st.sidebar.text_input("å¹´(YYYY)", "2026")
    s_kai = st.sidebar.text_input("å›(æ•°å­—)", "1")
    s_place = st.sidebar.selectbox("å ´æ‰€", list(KEIBABOOK_TO_NETKEIBA_PLACE.keys()), format_func=lambda x: requests.utils.unquote(x) if False else x + " (ã‚³ãƒ¼ãƒ‰)")
    s_day = st.sidebar.text_input("æ—¥ç›®(æ•°å­—)", "1")
    s_place_name = st.sidebar.text_input("å ´æ‰€å(è¡¨ç¤ºç”¨)", "äº¬éƒ½")
    s_races = st.sidebar.text_input("ãƒ¬ãƒ¼ã‚¹ç•ªå·(ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", "1,2,3")
    
    if st.sidebar.button("äºˆæƒ³é–‹å§‹"):
        try:
            r_list = [int(x.strip()) for x in s_races.split(",") if x.strip()]
            job = {
                "year": s_year,
                "kai": s_kai,
                "place": s_place,
                "day": s_day,
                "place_name": s_place_name,
                "races": r_list
            }
            run_batch_prediction([job])
        except Exception as e:
            st.error(f"è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
