import time
import json
import re
import requests
import streamlit as st
import streamlit.components.v1 as components
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
YEAR = "2026"
KAI = "01"
PLACE = "05"  # ä¸­å±±
DAY = "03"

BASE_URL = "https://s.keibabook.co.jp"

PLACE_NAMES = {
    "00": "äº¬éƒ½",
    "01": "é˜ªç¥",
    "02": "ä¸­äº¬",
    "03": "å°å€‰",
    "04": "æ±äº¬",
    "05": "ä¸­å±±",
    "06": "ç¦å³¶",
    "07": "æ–°æ½Ÿ",
    "08": "æœ­å¹Œ",
    "09": "å‡½é¤¨",
}

# ç«¶é¦¬ãƒ–ãƒƒã‚¯ PLACEã‚³ãƒ¼ãƒ‰ â†’ netkeiba ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
KEIBABOOK_TO_NETKEIBA_PLACE = {
    "08": "01",  # æœ­å¹Œ
    "09": "02",  # å‡½é¤¨
    "06": "03",  # ç¦å³¶
    "07": "04",  # æ–°æ½Ÿ
    "04": "05",  # æ±äº¬
    "05": "06",  # ä¸­å±±
    "02": "07",  # ä¸­äº¬
    "00": "08",  # äº¬éƒ½
    "01": "09",  # é˜ªç¥
    "03": "10",  # å°å€‰
}


def set_race_params(year, kai, place, day):
    global YEAR, KAI, PLACE, DAY
    YEAR = str(year)
    KAI = str(kai).zfill(2)
    PLACE = str(place).zfill(2)
    DAY = str(day).zfill(2)


def get_current_params():
    return YEAR, KAI, PLACE, DAY


# ==================================================
# ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚³ãƒ”ãƒ¼
# ==================================================
def render_copy_button(text: str, label: str, dom_id: str):
    safe_text = json.dumps(text)
    html = f"""
    <div style="display:flex; gap:8px; align-items:center; flex-wrap:wrap;">
      <button id="{dom_id}" style="
        padding:8px 12px;
        border-radius:10px;
        border:1px solid #ddd;
        background:#fff;
        cursor:pointer;
        font-size:14px;
      ">{label}</button>
      <span id="{dom_id}-msg" style="font-size:12px; color:#666;"></span>
    </div>
    <script>
      (function() {{
        const btn = document.getElementById("{dom_id}");
        const msg = document.getElementById("{dom_id}-msg");
        if (!btn) return;
        btn.addEventListener("click", async () => {{
          try {{
            await navigator.clipboard.writeText({safe_text});
            msg.textContent = "ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";
            setTimeout(() => msg.textContent = "", 1200);
          }} catch (e) {{
            msg.textContent = "ã‚³ãƒ”ãƒ¼å¤±æ•—";
            setTimeout(() => msg.textContent = "", 2200);
          }}
        }});
      }})();
    </script>
    """
    components.html(html, height=54)


# ==================================================
# Selenium / Login
# ==================================================
def build_driver() -> webdriver.Chrome:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,2200")
    options.add_argument("--lang=ja-JP")
    options.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(60)
    return driver


def login_keibabook(driver: webdriver.Chrome) -> None:
    if not KEIBA_ID or not KEIBA_PASS:
        raise RuntimeError("KEIBA_ID / KEIBA_PASS ãŒæœªè¨­å®šã§ã™ï¼ˆst.secretsï¼‰")
    driver.get(f"{BASE_URL}/login/login")
    WebDriverWait(driver, 15).until(
        EC.visibility_of_element_located((By.NAME, "login_id"))
    ).send_keys(KEIBA_ID)
    WebDriverWait(driver, 15).until(
        EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
    ).send_keys(KEIBA_PASS)
    WebDriverWait(driver, 15).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))
    ).click()
    time.sleep(1.0)


# ==================================================
# Utility
# ==================================================
def _clean_text_ja(s: str) -> str:
    if not s:
        return ""
    s = s.replace("\u3000", " ")  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _is_missing_marker(s: str) -> bool:
    t = _clean_text_ja(s)
    return t in {"ï¼", "-", "â€”", "â€•", "â€", ""}


# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼šå©èˆã®è©±ï¼ˆdanwaï¼‰â†’ ãƒ¬ãƒ¼ã‚¹æƒ…å ± + é¦¬ç•ª/é¦¬å/å©èˆã‚³ãƒ¡ãƒ³ãƒˆ
#  HTMLæ§‹é€ ã‚’ç©æ¥µåˆ©ç”¨ã—ã¦ç¢ºå®Ÿã«æŠ½å‡º
# ==================================================
def parse_race_info_from_danwa(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"date_meet": "", "race_name": "", "cond1": "", "course_line": ""}

    racemei = racetitle.find("div", class_="racemei")
    date_meet, race_name = "", ""
    if racemei:
        ps = racemei.find_all("p")
        if len(ps) >= 1:
            date_meet = ps[0].get_text(strip=True)
        if len(ps) >= 2:
            race_name = ps[1].get_text(strip=True)

    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    cond1, course_line = "", ""
    if racetitle_sub:
        sub_ps = racetitle_sub.find_all("p")
        if len(sub_ps) >= 1:
            cond1 = sub_ps[0].get_text(strip=True)
        if len(sub_ps) >= 2:
            course_line = racetitle_sub.find_all("p")[1].get_text(" ", strip=True)

    return {
        "date_meet": _clean_text_ja(date_meet),
        "race_name": _clean_text_ja(race_name),
        "cond1": _clean_text_ja(cond1),
        "course_line": _clean_text_ja(course_line),
    }


def parse_danwa_horses(html: str) -> dict:
    """
    è¿”ã‚Šå€¤:
      {
        "1": {"name":"ãƒ©ãƒ•ãƒ¬ãƒ¼ãƒ‰ãƒ”ã‚¨ãƒ«", "danwa":"â—‹ãƒ©ãƒ•ãƒ¬ãƒ¼ãƒ‰ãƒ”ã‚¨ãƒ«..."},
        ...
      }
    """
    soup = BeautifulSoup(html, "html.parser")

    # ã¾ãšãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®šï¼ˆclass="default danwa" æƒ³å®šï¼‰
    table = soup.find("table", class_=lambda c: c and "danwa" in str(c))
    if not table or not table.tbody:
        return {}

    horses = {}
    current_umaban = None

    # danwaã¯ã€Œé¦¬è¡Œã€â†’ã€Œã‚³ãƒ¡ãƒ³ãƒˆè¡Œã€â†’ spacer ã®ç¹°ã‚Šè¿”ã—ãŒå¤šã„
    rows = table.tbody.find_all("tr", recursive=False)
    if not rows:
        rows = table.tbody.find_all("tr")

    for tr in rows:
        if "spacer" in (tr.get("class") or []):
            continue

        # é¦¬è¡Œï¼štd.umaban + td.left(é¦¬åãƒªãƒ³ã‚¯)
        umaban_td = tr.find("td", class_="umaban")
        bamei_td = tr.find("td", class_=lambda c: c and "left" in str(c))
        if umaban_td and bamei_td:
            umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))
            a = bamei_td.find("a")
            name = a.get_text(strip=True) if a else bamei_td.get_text(strip=True)
            name = _clean_text_ja(name)
            if umaban:
                current_umaban = umaban
                horses[current_umaban] = {"name": name, "danwa": ""}
            continue

        # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œï¼štd.danwa
        danwa_td = tr.find("td", class_="danwa")
        if danwa_td and current_umaban:
            p = danwa_td.find("p")
            txt = p.get_text("\n", strip=True) if p else danwa_td.get_text("\n", strip=True)
            txt = _clean_text_ja(txt)
            horses[current_umaban]["danwa"] = txt if txt else "(æƒ…å ±ãªã—)"
            current_umaban = None
            continue

    return horses


def fetch_keibabook_danwa(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/danwa/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 12).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.default.danwa"))
        )
    except Exception:
        pass

    html = driver.page_source
    race_info = parse_race_info_from_danwa(html)
    horses = parse_danwa_horses(html)
    return race_info, horses


# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼šå‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ï¼ˆsyoinï¼‰
# æç¤ºã•ã‚ŒãŸHTMLæ§‹é€ ã‚’å‰æã«ã€ä»¥ä¸‹ã‚’å³å¯†ã«å®šç¾©
# - td.syoin å†…ã«ã‚ã‚‹ã€Œdiv.syoindataã€ã¯ãƒ¡ã‚¿æƒ…å ±ï¼ˆå‰èµ°ãƒ¬ãƒ¼ã‚¹ï¼‰â†’ã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡ºå¯¾è±¡å¤–
# - ã‚³ãƒ¡ãƒ³ãƒˆã¯ div.syoindata ã®å¤–ã«ã‚ã‚‹ <p> ã®ã¿
# - <p>ï¼</p> ã¯ã€Œã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ãªã—ã€
# ==================================================
def parse_zenkoso_interview(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_=lambda c: c and "syoin" in str(c))
    if not table or not table.tbody:
        return {}

    interview = {}
    current_umaban = None

    rows = table.tbody.find_all("tr", recursive=False)
    if not rows:
        rows = table.tbody.find_all("tr")

    for tr in rows:
        if "spacer" in (tr.get("class") or []):
            continue

        # é¦¬è¡Œï¼štd.umaban + td.left.bamei
        umaban_td = tr.find("td", class_="umaban")
        bamei_td = tr.find("td", class_=lambda c: c and "bamei" in str(c))
        if umaban_td and bamei_td:
            u = re.sub(r"\D", "", umaban_td.get_text(strip=True))
            current_umaban = u if u else None
            continue

        # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œï¼štd.syoin
        syoin_td = tr.find("td", class_="syoin")
        if syoin_td and current_umaban:
            # ã‚³ãƒ¡ãƒ³ãƒˆå€™è£œã¯ã€Œdiv.syoindata ã®å¤–ã€ã«ã‚ã‚‹ <p> ã®ã¿
            candidates = []
            for p in syoin_td.find_all("p"):
                # syoindataå†…ã®pï¼ˆå‰èµ°æ—¥ä»˜ç­‰ï¼‰ã‚’é™¤å¤–
                if p.find_parent("div", class_="syoindata") is not None:
                    continue
                t = _clean_text_ja(p.get_text(" ", strip=True))
                candidates.append(t)

            # æœ‰åŠ¹ãªã‚³ãƒ¡ãƒ³ãƒˆã®ã¿æ¡ç”¨ï¼ˆï¼ã¯ç„¡è¦–ï¼‰
            chosen = ""
            for t in candidates:
                if _is_missing_marker(t):
                    continue
                # ã‚³ãƒ¡ãƒ³ãƒˆã‚‰ã—ã•æœ€ä½ãƒ©ã‚¤ãƒ³ï¼ˆçŸ­ã„ã‚´ãƒŸé™¤å»ï¼‰
                if len(t) < 8:
                    continue
                chosen = t
                break

            if chosen:
                interview[current_umaban] = chosen

            current_umaban = None

    return interview


def fetch_zenkoso_interview(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syoin/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 12).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.default.syoin"))
        )
    except Exception:
        pass
    return parse_zenkoso_interview(driver.page_source)


# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼šCPUäºˆæƒ³ï¼ˆæŒ‡æ•°/ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼‰
# ==================================================
def parse_keibabook_cpu(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    data = {}

    # ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°
    speed_table = soup.find("table", id="cpu_speed_sort_table")
    if speed_table and speed_table.tbody:
        for tr in speed_table.tbody.find_all("tr"):
            umaban_td = tr.find("td", class_="umaban")
            if not umaban_td:
                continue
            umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))
            if not umaban:
                continue

            tds = tr.find_all("td")
            if len(tds) < 8:
                continue

            def get_val(idx):
                p = tds[idx].find("p")
                if not p:
                    return 0
                txt = re.sub(r"\D", "", p.get_text(strip=True))
                if not txt:
                    return 0
                v = int(txt)
                # ã€Œæœªã€ã‚„ã€Œ-ã€ãŒ 1000 è¡¨è¨˜ã«ãªã‚‹ã‚±ãƒ¼ã‚¹ â†’ ç„¡æ‰±ã„
                if v == 1000:
                    return 0
                return v

            val_last = get_val(-1)
            val_2ago = get_val(-2)
            val_3ago = get_val(-3)
            valid_scores = [v for v in [val_last, val_2ago, val_3ago] if v > 0]
            avg = round(sum(valid_scores) / len(valid_scores)) if valid_scores else 0

            if umaban not in data:
                data[umaban] = {}
            data[umaban].update(
                {
                    "speed_last": val_last if val_last > 0 else "ç„¡",
                    "speed_2ago": val_2ago if val_2ago > 0 else "ç„¡",
                    "speed_3ago": val_3ago if val_3ago > 0 else "ç„¡",
                    "speed_avg": avg if avg > 0 else "ç„¡",
                }
            )

    # ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆcaptionã«ã€Œãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã€ï¼‰
    factor_table = None
    for tbl in soup.find_all("table"):
        cap = tbl.find("caption")
        if cap and "ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼" in cap.get_text():
            factor_table = tbl
            break

    if factor_table and factor_table.tbody:
        for tr in factor_table.tbody.find_all("tr"):
            umaban_td = tr.find("td", class_="umaban")
            if not umaban_td:
                continue
            umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))
            if not umaban:
                continue

            tds = tr.find_all("td")
            if len(tds) < 8:
                continue

            def get_mark(idx):
                p = tds[idx].find("p")
                if not p:
                    return "ç„¡"
                txt = p.get_text(strip=True)
                return txt if txt else "ç„¡"

            if umaban not in data:
                data[umaban] = {}
            data[umaban].update(
                {
                    "factor_course": get_mark(5),
                    "factor_dist": get_mark(6),
                    "factor_zenso": get_mark(7),
                }
            )

    return data


def fetch_keibabook_cpu_data(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/cpu/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, "cpu_speed_sort_table"))
        )
    except Exception:
        pass
    return parse_keibabook_cpu(driver.page_source)


# ==================================================
# netkeibaï¼ˆå‡ºé¦¬è¡¨ï¼šç¾åœ¨é¨æ‰‹ + éå»èµ°ï¼‰
# ==================================================
def keibabook_race_id_to_netkeiba_race_id(year, kai, place, day, race_num_2):
    nk_place = KEIBABOOK_TO_NETKEIBA_PLACE.get(place)
    if not nk_place:
        return ""
    return (
        f"{str(year)}{nk_place}{str(kai).zfill(2)}{str(day).zfill(2)}"
        f"{str(race_num_2).zfill(2)}"
    )


def _clean_jockey_name(name: str) -> str:
    return re.sub(r"^[â–²â–³â˜†â—‡â˜…â—â—‹â–²â–³\s]+", "", name).strip()


def _extract_jockey_from_data03(data03_tag) -> str:
    if data03_tag is None:
        return ""
    # 1) ãƒªãƒ³ã‚¯å„ªå…ˆ
    for a in data03_tag.find_all("a"):
        txt = a.get_text(strip=True)
        href = (a.get("href") or "")
        if txt and ("/jockey/" in href or "jockey" in href):
            return _clean_jockey_name(txt)

    # 2) ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    d3_text = data03_tag.get_text(" ", strip=True)
    if not d3_text:
        return ""
    weights = list(re.finditer(r"\b\d{2,3}\.\d\b", d3_text))
    if weights:
        w = weights[-1]
        before = d3_text[: w.start()].strip()
        parts = before.split()
        if parts:
            cand = _clean_jockey_name(parts[-1])
            return cand
    return ""


def _extract_race_result_from_past_td(past_td) -> str:
    if past_td is None:
        return ""
    text = past_td.get_text(" ", strip=True)
    if len(text) < 5:
        return ""

    date_match = re.search(r"(\d{4}[./]\d{1,2}[./]\d{1,2})", text)
    date_str = date_match.group(1).replace("/", ".") if date_match else ""

    rank_match = re.search(r"(\d{1,2})ç€", text)
    if not rank_match:
        num_span = past_td.find("span", class_="Num")
        rank = num_span.get_text(strip=True) if num_span else ""
    else:
        rank = rank_match.group(1)

    data02 = past_td.find("div", class_="Data02")
    race_name = data02.get_text(strip=True) if data02 else ""

    data05 = past_td.find("div", class_="Data05")
    course = ""
    if data05:
        c_text = data05.get_text(strip=True)
        cm = re.search(r"(èŠ|ãƒ€)\d+", c_text)
        if cm:
            course = cm.group(0)

    data03 = past_td.find("div", class_="Data03")
    jockey = _extract_jockey_from_data03(data03)

    data06 = past_td.find("div", class_="Data06")
    passing = ""
    if data06:
        pm = re.search(r"(\d{1,2}-\d{1,2}(?:-\d{1,2})*)", data06.get_text(strip=True))
        if pm:
            passing = pm.group(1)

    parts = []
    if date_str:
        parts.append(date_str)
    if race_name:
        parts.append(race_name)
    if course:
        parts.append(course)
    if jockey:
        parts.append(f"[{jockey}]")
    if passing:
        parts.append(f"({passing})")
    if rank:
        parts.append(f"{rank}ç€")

    return " ".join(parts) if parts else ""


def parse_netkeiba_shutuba_past(html: str, take_last_n: int = 3) -> dict:
    """
    è¿”ã‚Šå€¤:
      {
        "1": {"jockey": "ä¸¹å†…", "past3": ["å‰èµ°..", "2èµ°å‰..", "3èµ°å‰.."]},
        ...
      }
    """
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_=lambda c: c and "Shutuba_Past5_Table" in str(c))
    if not table:
        return {}

    out = {}
    rows = table.find_all("tr", class_=lambda c: c and "HorseList" in str(c))
    for tr in rows:
        # é¦¬ç•ª
        waku_tds = tr.find_all("td", class_=lambda c: c and "Waku" in str(c))
        umaban = ""
        for td in waku_tds:
            if td.get("class") == ["Waku"]:
                umaban = re.sub(r"\D", "", td.get_text(strip=True))
                break
        if not umaban and len(waku_tds) >= 2:
            umaban = re.sub(r"\D", "", waku_tds[1].get_text(strip=True))
        if not umaban:
            continue

        # ç¾åœ¨ã®é¨æ‰‹ï¼ˆtd.Jockey aï¼‰
        jockey = ""
        jockey_td = tr.find("td", class_=lambda c: c and "Jockey" in str(c))
        if jockey_td:
            a = jockey_td.find("a")
            if a:
                jockey = a.get_text(strip=True)
        jockey = _clean_jockey_name(jockey)

        # éå»èµ°
        past_tds = tr.find_all("td", class_=lambda c: c and "Past" in str(c))
        past_tds = [td for td in past_tds if "Rest" not in str(td.get("class", []))]

        summaries = []
        for td in past_tds[:take_last_n]:
            summaries.append(_extract_race_result_from_past_td(td))
        while len(summaries) < take_last_n:
            summaries.append("")

        out[umaban] = {"jockey": jockey, "past3": summaries}

    return out


def fetch_netkeiba_shutuba_past(driver, netkeiba_race_id: str) -> dict:
    url = (
        "https://race.netkeiba.com/race/shutuba_past.html"
        f"?race_id={netkeiba_race_id}&rf=shutuba_submenu"
    )
    driver.get(url)
    try:
        WebDriverWait(driver, 12).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.Shutuba_Past5_Table"))
        )
    except Exception:
        pass
    return parse_netkeiba_shutuba_past(driver.page_source, take_last_n=3)


# ==================================================
# Dify Streaming
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ ã‚¨ãƒ©ãƒ¼: DIFY_API_KEY ãŒæœªè¨­å®š"
        return

    payload = {
        "inputs": {"text": full_text},
        "response_mode": "streaming",
        "user": "keiba-bot-user",
    }
    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}",
        "Content-Type": "application/json",
    }
    try:
        res = requests.post(
            "https://api.dify.ai/v1/workflows/run",
            headers=headers,
            json=payload,
            stream=True,
            timeout=600,
        )
        if res.status_code != 200:
            yield f"âš ï¸ ã‚¨ãƒ©ãƒ¼: Dify API Error {res.status_code}\n{res.text}"
            return

        for line in res.iter_lines():
            if not line:
                continue
            decoded = line.decode("utf-8", errors="ignore")
            if not decoded.startswith("data:"):
                continue
            json_str = decoded.replace("data: ", "")
            try:
                data = json.loads(json_str)
            except Exception:
                continue

            # workflow_finished ã® outputs ãŒã‚ã‚‹å ´åˆ
            event = data.get("event")
            if event == "workflow_finished":
                outputs = data.get("data", {}).get("outputs", {})
                for _, value in outputs.items():
                    if isinstance(value, str) and value:
                        yield value + "\n"

            # streaming answer
            chunk = data.get("answer", "")
            if chunk:
                yield chunk

    except Exception as e:
        yield f"\n\nâš ï¸ Request Error: {str(e)}"


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç† (run_all_races)
# è¨ªã‚Œã‚‹ãƒšãƒ¼ã‚¸:
# - keibabook: å©èˆã®è©±(danwa) / å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼(syoin) / CPUäºˆæƒ³(cpu)
# - netkeiba: é¦¬æŸ±(shutuba_past)
# ==================================================
def run_all_races(target_races=None):
    race_numbers = (
        list(range(1, 13))
        if target_races is None
        else sorted({int(r) for r in target_races})
    )
    base_id = f"{YEAR}{KAI}{PLACE}{DAY}"
    place_name = PLACE_NAMES.get(PLACE, "ä¸æ˜")

    combined_blocks = []
    driver = build_driver()

    try:
        st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...(ç«¶é¦¬ãƒ–ãƒƒã‚¯)")
        login_keibabook(driver)
        st.success("âœ… ç«¶é¦¬ãƒ–ãƒƒã‚¯ ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†")

        for r in race_numbers:
            race_num = f"{r:02}"
            race_id = base_id + race_num  # Keibabook Race ID
            netkeiba_race_id = keibabook_race_id_to_netkeiba_race_id(
                YEAR, KAI, PLACE, DAY, race_num
            )

            st.markdown(f"### {place_name} {r}R")
            status_area = st.empty()
            result_container = st.container()
            full_answer = ""

            try:
                status_area.info(f"ğŸ“¡ {place_name}{r}R ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")

                # 1) å©èˆã®è©±ï¼šãƒ¬ãƒ¼ã‚¹æƒ…å ± + é¦¬å + å©èˆã‚³ãƒ¡ãƒ³ãƒˆï¼ˆã“ã“ã‚’ä¸»ã‚­ãƒ¼ã«ã™ã‚‹ï¼‰
                status_area.info("ğŸ—£ï¸ ç«¶é¦¬ãƒ–ãƒƒã‚¯ å©èˆã®è©±ï¼ˆãƒ¬ãƒ¼ã‚¹æƒ…å ±/é¦¬å/ã‚³ãƒ¡ãƒ³ãƒˆï¼‰å–å¾—ä¸­...")
                race_info, danwa_horses = fetch_keibabook_danwa(driver, race_id)

                if not danwa_horses:
                    status_area.error("å©èˆã®è©±ã‹ã‚‰é¦¬ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆtable.danwaæœªæ¤œå‡ºï¼‰")
                    st.write("---")
                    continue

                # 2) CPUï¼ˆæŒ‡æ•°/ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼‰
                cpu_dict = {}
                try:
                    status_area.info("ğŸ“Š ç«¶é¦¬ãƒ–ãƒƒã‚¯ æŒ‡æ•°ãƒ»ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼å–å¾—ä¸­...")
                    cpu_dict = fetch_keibabook_cpu_data(driver, race_id)
                except Exception as e:
                    print(f"CPU fetch error: {e}")

                # 3) å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆãŒã‚ã‚‹ã‚‚ã®ã ã‘ï¼‰
                zenkoso_dict = {}
                try:
                    status_area.info("ğŸ¤ å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼å–å¾—ä¸­...")
                    zenkoso_dict = fetch_zenkoso_interview(driver, race_id)
                except Exception as e:
                    print(f"Zenkoso fetch error: {e}")

                # 4) netkeibaï¼šç¾åœ¨é¨æ‰‹ + éå»èµ°
                nk_dict = {}
                if netkeiba_race_id:
                    try:
                        status_area.info("ğŸ“ netkeibaï¼ˆç¾åœ¨é¨æ‰‹ãƒ»æˆ¦ç¸¾ï¼‰å–å¾—ä¸­...")
                        nk_dict = fetch_netkeiba_shutuba_past(driver, netkeiba_race_id)
                    except Exception as e:
                        print(f"Netkeiba fetch error: {e}")

                # ---------
                # çµåˆï¼ˆé¦¬ç•ªé †ï¼šå©èˆã®è©±ã«å‡ºã¦ã„ã‚‹é¦¬ç•ªã‚’æ­£ï¼‰
                # ---------
                umaban_list = sorted(danwa_horses.keys(), key=lambda x: int(x))

                merged = []
                for umaban in umaban_list:
                    bamei = (danwa_horses.get(umaban, {}) or {}).get("name") or "åç§°ä¸æ˜"
                    kyusha_comment = (danwa_horses.get(umaban, {}) or {}).get("danwa") or "(æƒ…å ±ãªã—)"

                    # é¨æ‰‹ï¼šnetkeiba å„ªå…ˆ
                    kisyu = (nk_dict.get(umaban, {}) or {}).get("jockey", "") if nk_dict else ""
                    kisyu = kisyu if kisyu else "ä¸æ˜"

                    # æŒ‡æ•°ãƒ»ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆCPUï¼‰
                    cpu = cpu_dict.get(umaban, {}) if cpu_dict else {}
                    sp_last = cpu.get("speed_last", "ç„¡")
                    sp_2ago = cpu.get("speed_2ago", "ç„¡")
                    sp_3ago = cpu.get("speed_3ago", "ç„¡")
                    sp_avg = cpu.get("speed_avg", "ç„¡")
                    fac_crs = cpu.get("factor_course", "ç„¡")
                    fac_dis = cpu.get("factor_dist", "ç„¡")
                    fac_zen = cpu.get("factor_zenso", "ç„¡")

                    index_line = (
                        f"  ã€æŒ‡æ•°ã€‘ å‰èµ°:{sp_last}ã€2èµ°å‰:{sp_2ago}ã€3èµ°å‰:{sp_3ago}"
                        f"ï¼ˆ3èµ°å¹³å‡:{sp_avg}ï¼‰ã€ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã€‘ ã‚³ãƒ¼ã‚¹ï¼š{fac_crs} è·é›¢ï¼š{fac_dis} å‰èµ°ï¼š{fac_zen}\n"
                    )

                    # å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ï¼ˆ<p>ï¼</p> ã¯å…¥ã£ã¦ã“ãªã„è¨­è¨ˆï¼‰
                    zenkoso_line = ""
                    if zenkoso_dict and umaban in zenkoso_dict:
                        zenkoso_line = f"  ã€å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã€‘ {zenkoso_dict[umaban]}\n"

                    # æˆ¦ç¸¾ï¼ˆnetkeibaï¼‰
                    if nk_dict and umaban in nk_dict:
                        p_info = nk_dict[umaban].get("past3", ["", "", ""])
                    else:
                        p_info = ["", "", ""]
                    labels = ["å‰èµ°", "2èµ°å‰", "3èµ°å‰"]
                    recs = []
                    for lab, rec in zip(labels, p_info):
                        if rec:
                            recs.append(f"{lab}:{rec}")
                    senreki_line = "  ã€æˆ¦ç¸¾ã€‘ " + (" ".join(recs) if recs else "(æƒ…å ±ãªã—)") + "\n"

                    text = (
                        f"â–¼[é¦¬ç•ª{umaban}] {bamei} / é¨æ‰‹:{kisyu}\n"
                        f"  ã€å©èˆã®è©±ã€‘ {kyusha_comment}\n"
                        f"{zenkoso_line}"
                        f"{index_line}"
                        f"{senreki_line}"
                    )
                    merged.append(text)

                # AIé€ä¿¡ãƒ†ã‚­ã‚¹ãƒˆä½œæˆï¼ˆãƒ¬ãƒ¼ã‚¹æƒ…å ±ã¯å©èˆã®è©±ãƒšãƒ¼ã‚¸ã‹ã‚‰ï¼‰
                header_txt = (
                    f"{race_info.get('date_meet','')}\n"
                    f"{race_info.get('race_name','')}\n"
                    f"{race_info.get('cond1','')}\n"
                    f"{race_info.get('course_line','')}"
                )
                full_text = (
                    f"â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n{header_txt}\n\n"
                    f"ä»¥ä¸‹ã¯{place_name}{r}Rã®å…¨é ­ãƒ‡ãƒ¼ã‚¿ã€‚\n"
                    f"â– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿\n" + "\n".join(merged)
                )

                status_area.info("ğŸ¤– AIåˆ†æä¸­...")
                with result_container:
                    r_area = st.empty()
                    for chunk in stream_dify_workflow(full_text):
                        full_answer += chunk
                        r_area.markdown(full_answer + "â–Œ")
                    r_area.markdown(full_answer)

                if full_answer:
                    status_area.success("âœ… å®Œäº†")
                    combined_blocks.append(f"ã€{place_name} {r}Rã€‘\n{full_answer.strip()}\n")
                    render_copy_button(full_answer.strip(), f"ğŸ“‹ {place_name}{r}R ã‚³ãƒ”ãƒ¼", f"copy_{race_id}")
                else:
                    status_area.error("å›ç­”ãªã—")

            except Exception as e:
                status_area.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                traceback.print_exc()

            st.write("---")

        if combined_blocks:
            combined_text = "\n".join(combined_blocks).strip()
            st.subheader("ğŸ“Œ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚")
            render_copy_button(combined_text, "ğŸ“‹ å…¨ã¦ã‚³ãƒ”ãƒ¼", "copy_all_races")
            st.text_area("ã¾ã¨ã‚", combined_text, height=300)

    finally:
        try:
            driver.quit()
        except Exception:
            pass
