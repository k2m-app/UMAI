import time
import json
import re
import requests
import streamlit as st
import streamlit.components.v1 as components
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# â˜…è¿½åŠ ï¼šnetkeiba
NETKEIBA_ID = st.secrets.get("NETKEIBA_ID", "")
NETKEIBA_PASS = st.secrets.get("NETKEIBA_PASS", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆapp.py å´ã§ set_race_params ãŒå‘¼ã°ã‚Œã‚‹ã¨æ›¸ãæ›ã‚ã‚‹ï¼‰
YEAR = "2025"
KAI = "04"
PLACE = "02"
DAY = "02"

BASE_URL = "https://s.keibabook.co.jp"

PLACE_NAMES = {
    "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰", "04": "æ±äº¬",
    "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ", "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
}

# â˜…è¿½åŠ ï¼šç«¶é¦¬ãƒ–ãƒƒã‚¯ PLACEã‚³ãƒ¼ãƒ‰ â†’ netkeiba ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
# ãƒ¦ãƒ¼ã‚¶ãƒ¼æç¤ºï¼š
# 01æœ­å¹Œ 02å‡½é¤¨ 03ç¦å³¶ 04æ–°æ½Ÿ 05æ±äº¬ 06ä¸­å±± 07ä¸­äº¬ 08äº¬éƒ½ 09é˜ªç¥ 10å°å€‰
KEIBABOOK_TO_NETKEIBA_PLACE = {
    "08": "01",  # æœ­å¹Œ
    "09": "02",  # å‡½é¤¨
    "06": "03",  # ç¦å³¶
    "07": "04",  # æ–°æ½Ÿ
    "04": "05",  # æ±äº¬
    "05": "06",  # ä¸­å±±
    "02": "07",  # ä¸­äº¬
    "00": "08",  # äº¬éƒ½
    "01": "09",  # é˜ªç¥
    "03": "10",  # å°å€‰
}

def set_race_params(year, kai, place, day):
    """app.py ã‹ã‚‰é–‹å‚¬æƒ…å ±ã‚’å·®ã—æ›¿ãˆã‚‹ãŸã‚ã®é–¢æ•°"""
    global YEAR, KAI, PLACE, DAY
    YEAR = str(year)
    KAI = str(kai).zfill(2)
    PLACE = str(place).zfill(2)
    DAY = str(day).zfill(2)

def get_current_params():
    """ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆUIè¡¨ç¤ºç”¨ï¼‰"""
    return YEAR, KAI, PLACE, DAY


# ==================================================
# ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚³ãƒ”ãƒ¼ï¼ˆcomponents.html + clipboardï¼‰
# ==================================================
def render_copy_button(text: str, label: str, dom_id: str):
    safe_text = json.dumps(text)
    html = f"""
    <div style="display:flex; gap:8px; align-items:center; flex-wrap:wrap;">
      <button id="{dom_id}" style="
        padding:8px 12px;
        border-radius:10px;
        border:1px solid #ddd;
        background:#fff;
        cursor:pointer;
        font-size:14px;
      ">{label}</button>
      <span id="{dom_id}-msg" style="font-size:12px; color:#666;"></span>
    </div>
    <script>
      (function() {{
        const btn = document.getElementById("{dom_id}");
        const msg = document.getElementById("{dom_id}-msg");
        if (!btn) return;

        btn.addEventListener("click", async () => {{
          try {{
            await navigator.clipboard.writeText({safe_text});
            msg.textContent = "ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";
            setTimeout(() => msg.textContent = "", 1200);
          }} catch (e) {{
            msg.textContent = "ã‚³ãƒ”ãƒ¼ã«å¤±æ•—ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶åˆ¶é™ã®å¯èƒ½æ€§ï¼‰";
            setTimeout(() => msg.textContent = "", 2200);
          }}
        }});
      }})();
    </script>
    """
    components.html(html, height=54)


# ==================================================
# Supabase
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client | None:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)

def save_history(
    year: str,
    kai: str,
    place_code: str,
    place_name: str,
    day: str,
    race_num_str: str,
    race_id: str,
    ai_answer: str,
) -> None:
    supabase = get_supabase_client()
    if supabase is None:
        return

    data = {
        "year": str(year),
        "kai": str(kai),
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }

    try:
        supabase.table("history").insert(data).execute()
    except Exception as e:
        print("Supabase insert error:", e)


# ==================================================
# Selenium
# ==================================================
def build_driver() -> webdriver.Chrome:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,2200")
    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(60)
    return driver


def login_keibabook(driver: webdriver.Chrome) -> None:
    if not KEIBA_ID or not KEIBA_PASS:
        raise RuntimeError("KEIBA_ID / KEIBA_PASS ãŒ secrets ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    driver.get(f"{BASE_URL}/login/login")

    WebDriverWait(driver, 15).until(
        EC.visibility_of_element_located((By.NAME, "login_id"))
    ).send_keys(KEIBA_ID)

    WebDriverWait(driver, 15).until(
        EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
    ).send_keys(KEIBA_PASS)

    WebDriverWait(driver, 15).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))
    ).click()

    time.sleep(1.2)


# â˜…è¿½åŠ ï¼šnetkeibaãƒ­ã‚°ã‚¤ãƒ³ï¼ˆå¿…è¦ãªã¨ãã ã‘ï¼‰
def login_netkeiba(driver: webdriver.Chrome) -> bool:
    """
    æˆåŠŸã—ãŸã‚‰ Trueã€å¤±æ•—/æœªè¨­å®šãªã‚‰ False
    netkeibaã®ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã¯å¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§
    ã§ãã‚‹ã ã‘ã€Œå£Šã‚Œã«ãã„ã€æ›¸ãæ–¹ã«ã—ã¦ã„ã¾ã™ã€‚
    """
    if not NETKEIBA_ID or not NETKEIBA_PASS:
        return False

    try:
        # loginãƒšãƒ¼ã‚¸ï¼ˆnetkeibaã¯ã“ã“ãŒå®šç•ªï¼‰
        driver.get("https://regist.netkeiba.com/?pid=stage_login")
        time.sleep(0.8)

        # å…¥åŠ›æ¬„ï¼ˆnameã‚„idãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§è¤‡æ•°å€™è£œï¼‰
        # ã¾ãšã¯ã‚ˆãã‚ã‚‹ form æ§‹é€ ã‚’æ¢ã™
        id_candidates = [
            (By.NAME, "login_id"),
            (By.NAME, "userid"),
            (By.NAME, "id"),
            (By.CSS_SELECTOR, "input[type='text']"),
        ]
        pass_candidates = [
            (By.NAME, "pswd"),
            (By.NAME, "password"),
            (By.CSS_SELECTOR, "input[type='password']"),
        ]

        id_el = None
        for how, sel in id_candidates:
            try:
                id_el = WebDriverWait(driver, 5).until(EC.visibility_of_element_located((how, sel)))
                if id_el:
                    break
            except Exception:
                continue

        pw_el = None
        for how, sel in pass_candidates:
            try:
                pw_el = WebDriverWait(driver, 5).until(EC.visibility_of_element_located((how, sel)))
                if pw_el:
                    break
            except Exception:
                continue

        if not id_el or not pw_el:
            return False

        id_el.clear()
        id_el.send_keys(NETKEIBA_ID)
        pw_el.clear()
        pw_el.send_keys(NETKEIBA_PASS)

        # submit
        # loginãƒœã‚¿ãƒ³ã‚‚è¤‡æ•°å€™è£œ
        btn_candidates = [
            (By.CSS_SELECTOR, "input[type='submit']"),
            (By.CSS_SELECTOR, "button[type='submit']"),
            (By.CSS_SELECTOR, ".Btn_Login, .btn_login, .btn"),
        ]
        clicked = False
        for how, sel in btn_candidates:
            try:
                btn = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((how, sel)))
                btn.click()
                clicked = True
                break
            except Exception:
                continue

        if not clicked:
            return False

        time.sleep(1.2)

        # ãƒ­ã‚°ã‚¤ãƒ³åˆ¤å®šï¼šãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒªãƒ³ã‚¯ãŒå‡ºã‚‹ / no_login_show ãŒæ¶ˆãˆã‚‹ç­‰
        html = driver.page_source
        if "ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ" in html or "action=logout" in html:
            return True

        # ã†ã¾ãåˆ¤å®šã§ããªãã¦ã‚‚ cookies ã¯å…¥ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ True å¯„ã‚Šã«ã—ãªã„
        return False

    except Exception:
        return False


# ==================================================
# Parserï¼šå…±é€šï¼ˆç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼‰
# ==================================================
def parse_race_info(html: str):
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"date_meet": "", "race_name": "", "cond1": "", "course_line": ""}

    racemei = racetitle.find("div", class_="racemei")
    date_meet = ""
    race_name = ""
    if racemei:
        ps = racemei.find_all("p")
        if len(ps) >= 1:
            date_meet = ps[0].get_text(strip=True)
        if len(ps) >= 2:
            race_name = ps[1].get_text(strip=True)

    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    cond1 = ""
    course_line = ""
    if racetitle_sub:
        sub_ps = racetitle_sub.find_all("p")
        if len(sub_ps) >= 1:
            cond1 = sub_ps[0].get_text(strip=True)
        if len(sub_ps) >= 2:
            course_line = sub_ps[1].get_text(" ", strip=True)

    return {
        "date_meet": date_meet,
        "race_name": race_name,
        "cond1": cond1,
        "course_line": course_line,
    }


def parse_danwa_comments(html: str):
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table or not table.tbody:
        return {}

    danwa_dict = {}
    current_key = None

    for row in table.tbody.find_all("tr"):
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")

        if uma_td:
            text = re.sub(r"\D", "", uma_td.get_text(strip=True))
            if text:
                current_key = text
                continue

        if bamei_td and not current_key:
            text = bamei_td.get_text(strip=True)
            if text:
                current_key = text
                continue

        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current_key:
            danwa_dict[current_key] = danwa_td.get_text(strip=True)
            current_key = None

    return danwa_dict


def parse_zenkoso_interview(html: str):
    soup = BeautifulSoup(html, "html.parser")
    h2 = soup.find("h2", string=lambda s: s and "å‰èµ°" in s)
    if not h2:
        return {}

    table = h2.find_next("table", class_="syoin")
    if not table or not table.tbody:
        return {}

    rows = table.tbody.find_all("tr")
    result_dict = {}

    i = 0
    while i < len(rows):
        row = rows[i]
        if "spacer" in (row.get("class") or []):
            i += 1
            continue

        waku_td = row.find("td", class_="waku")
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")

        if not (waku_td and uma_td and bamei_td):
            i += 1
            continue

        waku = waku_td.get_text(strip=True)
        umaban = re.sub(r"\D", "", uma_td.get_text(strip=True))
        name = bamei_td.get_text(strip=True)

        prev_date = ""
        prev_class = ""
        prev_finish = ""
        prev_comment = ""

        detail = rows[i + 1] if i + 1 < len(rows) else None
        if detail:
            syoin_td = detail.find("td", class_="syoin")
            if syoin_td:
                sdata = syoin_td.find("div", class_="syoindata")
                if sdata:
                    ps = sdata.find_all("p")
                    if ps:
                        prev_date = ps[0].get_text(strip=True)
                    if len(ps) >= 2:
                        spans = ps[1].find_all("span")
                        if len(spans) >= 1:
                            prev_class = spans[0].get_text(strip=True)
                        if len(spans) >= 2:
                            prev_finish = spans[1].get_text(strip=True)

                direct = syoin_td.find_all("p", recursive=False)
                if direct:
                    txt = direct[0].get_text(strip=True)
                    if txt != "ï¼":
                        prev_comment = txt

        if umaban:
            result_dict[umaban] = {
                "waku": waku,
                "umaban": umaban,
                "name": name,
                "prev_date_course": prev_date,
                "prev_class": prev_class,
                "prev_finish": prev_finish,
                "prev_comment": prev_comment,
            }

        i += 2

    return result_dict


def parse_cyokyo(html: str):
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}

    section = None
    h2 = soup.find("h2", string=lambda s: s and ("èª¿æ•™" in s or "ä¸­é–“" in s))
    if h2:
        midasi_div = h2.find_parent("div", class_="midasi")
        if midasi_div:
            section = midasi_div.find_next_sibling("div", class_="section")
    if section is None:
        section = soup

    tables = section.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody:
            continue
        rows = tbody.find_all("tr", recursive=False)
        if len(rows) < 1:
            continue

        header = rows[0]
        uma_td = header.find("td", class_="umaban")
        name_td = header.find("td", class_="kbamei")

        umaban_text = uma_td.get_text(strip=True) if uma_td else ""
        umaban = re.sub(r"\D", "", umaban_text)

        bamei_hint = ""
        if name_td:
            bamei_hint = name_td.get_text(" ", strip=True)

        tanpyo_td = header.find("td", class_="tanpyo")
        tanpyo = tanpyo_td.get_text(strip=True) if tanpyo_td else ""

        detail_row = rows[1] if len(rows) >= 2 else None
        detail_text = detail_row.get_text(" ", strip=True) if detail_row else ""

        payload = {"tanpyo": tanpyo, "detail": detail_text, "bamei_hint": bamei_hint}

        if umaban:
            cyokyo_dict[umaban] = payload
        else:
            if bamei_hint:
                cyokyo_dict[bamei_hint] = payload

    return cyokyo_dict


def parse_syutuba(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")

    table = soup.find("table", class_=lambda c: c and "syutuba_sp" in c.split())
    if not table:
        table = soup.find("table", class_=lambda c: c and "syutuba" in c)

    if not table or not table.tbody:
        return {}

    result = {}
    for tr in table.tbody.find_all("tr", recursive=False):
        tds = tr.find_all("td", recursive=False)
        if not tds:
            continue

        umaban_raw = tds[0].get_text(strip=True)
        umaban = re.sub(r"\D", "", umaban_raw)
        if not umaban:
            continue

        bamei = ""
        kbamei_p = tr.find("p", class_="kbamei")
        if kbamei_p:
            bamei = kbamei_p.get_text(" ", strip=True)

        kisyu = ""
        kisyu_change = False

        kisyu_p = tr.find("p", class_="kisyu")
        if kisyu_p:
            a = kisyu_p.find("a")
            if a:
                norika = a.find("span", class_="norikawari")
                if norika:
                    kisyu_change = True
                    kisyu = norika.get_text(strip=True)
                else:
                    kisyu = a.get_text(strip=True)
            else:
                norika = kisyu_p.find("span", class_="norikawari")
                if norika:
                    kisyu_change = True
                    kisyu = norika.get_text(strip=True)
                else:
                    kisyu = kisyu_p.get_text(" ", strip=True)

        result[umaban] = {
            "umaban": umaban,
            "bamei": bamei,
            "kisyu": kisyu,
            "kisyu_change": kisyu_change,
        }

    return result


# ==================================================
# â˜…è¿½åŠ ï¼šnetkeiba ã‚¿ã‚¤ãƒ æŒ‡æ•° parser
# ==================================================
def parse_netkeiba_speed_index(html: str) -> dict:
    """
    netkeiba speed.html ã®å‡ºé¦¬è¡¨ã‹ã‚‰æŒ‡æ•°ã‚’æŠœãã€‚
    æˆ»ã‚Šå€¤ï¼š{ "1": {"index1":"83","index2":"65","index3":"83","course":"83","avg5":"81"}, ... }
    å–ã‚Œãªã„å€¤ã¯ "" ã«ã™ã‚‹ï¼ˆå¾Œã§è¡¨ç¤ºå´ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    """
    soup = BeautifulSoup(html, "html.parser")

    # ä»£è¡¨çš„ãªãƒ†ãƒ¼ãƒ–ãƒ« classï¼ˆã‚ãªãŸã®HTMLé€šã‚Šï¼‰
    table = soup.find("table", class_=lambda c: c and ("SpeedIndex_Table" in c))
    if not table or not table.tbody:
        return {}

    out = {}

    for tr in table.tbody.find_all("tr", class_=lambda c: c and ("HorseList" in c.split()), recursive=False):
        # é¦¬ç•ª
        um_td = tr.find("td", class_=lambda c: c and "sk__umaban" in c)
        if not um_td:
            continue
        umaban = re.sub(r"\D", "", um_td.get_text(" ", strip=True))
        if not umaban:
            continue

        def cell_text(cell_class: str) -> str:
            td = tr.find("td", class_=lambda c: c and cell_class in c.split())
            if not td:
                return ""
            txt = td.get_text(" ", strip=True)
            txt = txt.replace("\xa0", " ").strip()
            # ã€Œæœªã€ã‚„ã€Œ-ã€ã‚‚ãã®ã¾ã¾è¿”ã™ï¼ˆè¡¨ç¤ºå´ã§ä½¿ãˆã‚‹ï¼‰
            return txt

        out[umaban] = {
            "index1": cell_text("sk__index1"),           # å‰èµ°
            "index2": cell_text("sk__index2"),           # 2èµ°
            "index3": cell_text("sk__index3"),           # 3èµ°
            "course": cell_text("sk__max_course_index"), # ã‚³ãƒ¼ã‚¹æœ€é«˜
            "avg5": cell_text("sk__average_index"),      # 5èµ°å¹³å‡
        }

    return out


def fetch_netkeiba_speed_dict(driver: webdriver.Chrome, netkeiba_race_id: str) -> dict:
    """
    netkeiba speed.html ã‚’é–‹ã„ã¦æŒ‡æ•°è¾æ›¸ã‚’è¿”ã™
    """
    url = f"https://race.netkeiba.com/race/speed.html?race_id={netkeiba_race_id}&type=shutuba&mode=default"
    driver.get(url)

    # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå‡ºã‚‹ã¾ã§å°‘ã—å¾…ã¤ï¼ˆJSä¾å­˜ãŒè–„ã„ãŒã€ä¿é™ºï¼‰
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.SpeedIndex_Table"))
        )
    except Exception:
        pass

    html = driver.page_source

    # ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ãã†ãªã‚‰ã€1å›ã ã‘ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦å†å–å¾—
    # ï¼ˆã€Œãƒ­ã‚°ã‚¤ãƒ³ã€ãƒœã‚¿ãƒ³ã‚„ no_login_show ãŒå‡ºã‚‹ç­‰ã®ç°¡æ˜“åˆ¤å®šï¼‰
    if ("ç„¡æ–™ä¼šå“¡ç™»éŒ²" in html or "ãƒ­ã‚°ã‚¤ãƒ³" in html) and NETKEIBA_ID and NETKEIBA_PASS:
        ok = login_netkeiba(driver)
        if ok:
            driver.get(url)
            time.sleep(0.8)
            html = driver.page_source

    return parse_netkeiba_speed_index(html)


def keibabook_race_id_to_netkeiba_race_id(year: str, kai: str, place: str, day: str, race_num_2: str) -> str:
    """
    ç«¶é¦¬ãƒ–ãƒƒã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆYEAR,KAI,PLACE,DAYï¼‰ã‹ã‚‰ netkeiba race_id ã‚’ä½œã‚‹ã€‚
    netkeiba race_id = YYYY + (netkeibaå ´ã‚³ãƒ¼ãƒ‰2æ¡) + å›2æ¡ + æ—¥2æ¡ + R2æ¡
    """
    nk_place = KEIBABOOK_TO_NETKEIBA_PLACE.get(place)
    if not nk_place:
        # å¤‰æ›ã§ããªã„ã¨ãã¯ç©ºæ‰±ã„ï¼ˆå‘¼ã³å‡ºã—å´ã§ã‚±ã‚¢ï¼‰
        return ""
    return f"{str(year)}{nk_place}{str(kai).zfill(2)}{str(day).zfill(2)}{str(race_num_2).zfill(2)}"


# ==================================================
# fetchï¼ˆSeleniumï¼‰ç«¶é¦¬ãƒ–ãƒƒã‚¯
# ==================================================
def fetch_danwa_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/danwa/0/{race_id}"
    driver.get(url)
    time.sleep(0.8)
    html = driver.page_source
    return html, parse_race_info(html), parse_danwa_comments(html)

def fetch_zenkoso_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syoin/{race_id}"
    driver.get(url)
    time.sleep(0.8)
    return parse_zenkoso_interview(driver.page_source)

def fetch_cyokyo_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/cyokyo/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.cyokyo"))
        )
    except Exception:
        pass
    return parse_cyokyo(driver.page_source)

def fetch_syutuba_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syutuba/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.syutuba_sp, table.syutuba"))
        )
    except Exception:
        pass
    return parse_syutuba(driver.page_source)


# ==================================================
# ç›´è¿‘é–‹å‚¬ï¼šè¤‡æ•°å€™è£œæ¤œå‡º
# ==================================================
def detect_meet_candidates(driver, max_candidates: int = 12):
    driver.get(f"{BASE_URL}/cyuou/")
    time.sleep(1.0)
    html = driver.page_source

    keys12 = re.findall(r"/cyuou/syutuba/(\d{12})", html)
    if not keys12:
        keys12 = re.findall(r"/cyuou/thursday/(\d{12})", html)

    if not keys12:
        driver.get(f"{BASE_URL}/")
        time.sleep(1.0)
        html2 = driver.page_source
        keys12 = re.findall(r"/cyuou/syutuba/(\d{12})", html2)
        if not keys12:
            keys12 = re.findall(r"/cyuou/thursday/(\d{12})", html2)

    if not keys12:
        return []

    meet10_set = set(k[:10] for k in keys12 if len(k) >= 10)
    meet10_sorted = sorted(meet10_set, reverse=True)

    candidates = []
    for m10 in meet10_sorted[:max_candidates]:
        year = m10[0:4]
        kai = m10[4:6]
        place = m10[6:8]
        day = m10[8:10]
        candidates.append({
            "meet10": m10,
            "year": year,
            "kai": kai,
            "place": place,
            "day": day,
            "place_name": PLACE_NAMES.get(place, "ä¸æ˜"),
        })

    return candidates

def auto_detect_meet_candidates():
    driver = build_driver()
    try:
        login_keibabook(driver)
        return detect_meet_candidates(driver)
    finally:
        try:
            driver.quit()
        except Exception:
            pass


# ==================================================
# Difyï¼ˆStreamingï¼‰
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ ã‚¨ãƒ©ãƒ¼: DIFY_API_KEY ãŒæœªè¨­å®š"
        return

    payload = {
        "inputs": {"text": full_text},
        "response_mode": "streaming",
        "user": "keiba-bot-user",
    }

    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}",
        "Content-Type": "application/json",
    }

    try:
        res = requests.post(
            "https://api.dify.ai/v1/workflows/run",
            headers=headers,
            json=payload,
            stream=True,
            timeout=300,
        )

        if res.status_code != 200:
            yield f"âš ï¸ ã‚¨ãƒ©ãƒ¼: Dify API Error {res.status_code}\n{res.text}"
            return

        for line in res.iter_lines():
            if not line:
                continue
            decoded = line.decode("utf-8", errors="ignore")
            if not decoded.startswith("data:"):
                continue

            json_str = decoded.replace("data: ", "")
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError:
                continue

            event = data.get("event")
            if event in ["workflow_started", "node_started", "node_finished"]:
                continue

            chunk = data.get("answer", "")
            if chunk:
                yield chunk

            if event == "workflow_finished":
                outputs = data.get("data", {}).get("outputs", {})
                if outputs:
                    found_text = ""
                    for _, value in outputs.items():
                        if isinstance(value, str):
                            found_text += value + "\n"
                    if found_text.strip():
                        yield found_text.strip()

    except Exception as e:
        yield f"âš ï¸ Request Error: {str(e)}"


# ==================================================
# çµåˆç”¨ï¼šé¦¬åã‚­ãƒ¼æ•‘æ¸ˆ
# ==================================================
def _find_by_name_key(d: dict, bamei: str):
    if not bamei:
        return None
    if bamei in d:
        return d[bamei]
    for k, v in d.items():
        if (not str(k).isdigit()) and (str(k).strip() == bamei.strip()):
            return v
    return None


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆè¤‡æ•°ãƒ¬ãƒ¼ã‚¹ï¼‰
# ==================================================
def run_all_races(target_races=None):
    race_numbers = (
        list(range(1, 13))
        if target_races is None
        else sorted({int(r) for r in target_races})
    )

    base_id = f"{YEAR}{KAI}{PLACE}{DAY}"
    place_name = PLACE_NAMES.get(PLACE, "ä¸æ˜")

    combined_blocks: list[str] = []

    driver = build_driver()

    try:
        st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...ï¼ˆç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼‰")
        login_keibabook(driver)
        st.success("âœ… ç«¶é¦¬ãƒ–ãƒƒã‚¯ ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†")

        # â˜…netkeibaã¯ã€Œå¿…è¦ãªã‚‰ã€ãƒ­ã‚°ã‚¤ãƒ³ï¼ˆå¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œï¼‰
        netkeiba_logged_in = False
        if NETKEIBA_ID and NETKEIBA_PASS:
            st.info("ğŸ”‘ netkeiba ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèªä¸­ï¼ˆå¿…è¦ãªã‚‰ï¼‰...")
            netkeiba_logged_in = login_netkeiba(driver)
            if netkeiba_logged_in:
                st.success("âœ… netkeiba ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†")
            else:
                st.warning("âš ï¸ netkeiba ãƒ­ã‚°ã‚¤ãƒ³ã¯æœªç¢ºèªï¼ˆæŒ‡æ•°ãƒšãƒ¼ã‚¸ãŒé–²è¦§å¯èƒ½ãªã‚‰å–å¾—ã§ãã¾ã™ï¼‰")

        for r in race_numbers:
            race_num = f"{r:02}"
            race_id = base_id + race_num

            # â˜…netkeiba race_id
            netkeiba_race_id = keibabook_race_id_to_netkeiba_race_id(YEAR, KAI, PLACE, DAY, race_num)

            st.markdown(f"### {place_name} {r}R")
            status_area = st.empty()
            result_area = st.empty()
            full_answer = ""

            try:
                status_area.info(f"ğŸ“¡ {place_name}{r}R ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")

                # A-1 danwa + race_info
                _html_danwa, race_info, danwa_dict = fetch_danwa_dict(driver, race_id)

                # A-2 syoin
                zenkoso_dict = fetch_zenkoso_dict(driver, race_id)

                # A-3 cyokyo
                cyokyo_dict = fetch_cyokyo_dict(driver, race_id)

                # A-3.5 syutubaï¼ˆé¦¬ç•ªãƒ»é¦¬åãƒ»é¨æ‰‹ï¼‰
                syutuba_dict = fetch_syutuba_dict(driver, race_id)

                if not syutuba_dict:
                    status_area.warning("âš ï¸ å‡ºé¦¬è¡¨ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆå…¨é ­ä¿è¨¼ã§ããªã„å¯èƒ½æ€§ï¼‰ã€‚")

                # â˜…A-4 netkeiba æŒ‡æ•°ï¼ˆå–ã‚Œãªãã¦ã‚‚ç¶šè¡Œï¼‰
                speed_dict = {}
                if netkeiba_race_id:
                    try:
                        speed_dict = fetch_netkeiba_speed_dict(driver, netkeiba_race_id)
                    except Exception as e:
                        print("netkeiba fetch error:", e)
                        speed_dict = {}

                # A-5 çµåˆï¼ˆå‡ºé¦¬è¡¨ãƒ™ãƒ¼ã‚¹ï¼‰
                merged = []
                umaban_list = (
                    sorted(syutuba_dict.keys(), key=lambda x: int(x))
                    if syutuba_dict
                    else sorted(
                        list(set(danwa_dict.keys()) | set(zenkoso_dict.keys()) | set(cyokyo_dict.keys())),
                        key=lambda x: int(x) if str(x).isdigit() else 999
                    )
                )

                for umaban in umaban_list:
                    sb = syutuba_dict.get(umaban, {})
                    bamei = (sb.get("bamei") or "").strip() or "åç§°ä¸æ˜"

                    kisyu_raw = (sb.get("kisyu") or "").strip()
                    kisyu_change = bool(sb.get("kisyu_change"))
                    if kisyu_raw:
                        kisyu = f"æ›¿ãƒ»{kisyu_raw}" if kisyu_change else kisyu_raw
                    else:
                        kisyu = "ï¼ˆé¨æ‰‹ä¸æ˜ï¼‰"

                    # å©èˆã®è©±
                    d_comment = danwa_dict.get(umaban)
                    if not d_comment:
                        alt = _find_by_name_key(danwa_dict, bamei)
                        d_comment = alt if isinstance(alt, str) else None
                    if not d_comment:
                        d_comment = "ï¼ˆæƒ…å ±ãªã—ï¼‰"

                    # å‰èµ°
                    z_data = zenkoso_dict.get(umaban)
                    if not z_data:
                        alt = _find_by_name_key(zenkoso_dict, bamei)
                        z_data = alt if isinstance(alt, dict) else None
                    z_data = z_data or {}

                    z_prev_info = ""
                    z_comment = ""
                    if z_data:
                        z_prev_info = f"{z_data.get('prev_date_course','')} {z_data.get('prev_class','')} {z_data.get('prev_finish','')}".strip()
                        z_comment = (z_data.get("prev_comment") or "").strip()

                    if z_prev_info or z_comment:
                        prev_block = (
                            f"  ã€å‰èµ°æƒ…å ±ã€‘ {z_prev_info or 'ï¼ˆæƒ…å ±ãªã—ï¼‰'}\n"
                            f"  ã€å‰èµ°è«‡è©±ã€‘ {z_comment or 'ï¼ˆç„¡ã—ï¼‰'}\n"
                        )
                    else:
                        prev_block = "  ã€å‰èµ°ã€‘ æ–°é¦¬ï¼ˆå‰èµ°æƒ…å ±ãªã—ï¼‰\n"

                    # èª¿æ•™
                    c = cyokyo_dict.get(umaban)
                    if not c:
                        c = _find_by_name_key(cyokyo_dict, bamei)
                    c = c or {}

                    c_tanpyo = (c.get("tanpyo") or "").strip()
                    c_detail = (c.get("detail") or "").strip()

                    if c_tanpyo or c_detail:
                        cyokyo_block = f"  ã€èª¿æ•™ã€‘ çŸ­è©•:{c_tanpyo or 'ï¼ˆãªã—ï¼‰'} / è©³ç´°:{c_detail or 'ï¼ˆãªã—ï¼‰'}\n"
                    else:
                        cyokyo_block = "  ã€èª¿æ•™ã€‘ ï¼ˆæƒ…å ±ãªã—ï¼‰\n"

                    # â˜…æŒ‡æ•°ï¼ˆnetkeibaï¼‰
                    sp = speed_dict.get(umaban, {}) if isinstance(speed_dict, dict) else {}
                    idx1 = (sp.get("index1") or "").strip()
                    idx2 = (sp.get("index2") or "").strip()
                    idx3 = (sp.get("index3") or "").strip()
                    course = (sp.get("course") or "").strip()
                    avg5 = (sp.get("avg5") or "").strip()

                    if idx1 or idx2 or idx3 or course or avg5:
                        speed_line = f"  ã€æŒ‡æ•°ã€‘ å‰èµ°{idx1 or 'ï¼'}ã€2èµ°{idx2 or 'ï¼'}ã€3èµ°{idx3 or 'ï¼'}ã€ã‚³ãƒ¼ã‚¹æœ€é«˜{course or 'ï¼'}ã€5èµ°å¹³å‡{avg5 or 'ï¼'}\n"
                    else:
                        speed_line = "  ã€æŒ‡æ•°ã€‘ ï¼ˆæƒ…å ±ãªã—ï¼‰\n"

                    text = (
                        f"â–¼[é¦¬ç•ª{umaban}] {bamei} / é¨æ‰‹:{kisyu}\n"
                        f"  ã€å©èˆã®è©±ã€‘ {d_comment}\n"
                        f"{prev_block}"
                        f"{cyokyo_block}"
                        f"{speed_line}"
                    )
                    merged.append(text)

                if not merged:
                    status_area.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    st.write("---")
                    continue

                # ãƒ¬ãƒ¼ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼
                race_header_lines = []
                if race_info.get("date_meet"):
                    race_header_lines.append(race_info["date_meet"])
                if race_info.get("race_name"):
                    race_header_lines.append(race_info["race_name"])
                if race_info.get("cond1"):
                    race_header_lines.append(race_info["cond1"])
                if race_info.get("course_line"):
                    race_header_lines.append(race_info["course_line"])
                race_header = "\n".join(race_header_lines)

                merged_text = "\n".join(merged)

                full_text = (
                    "â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n"
                    f"{race_header}\n\n"
                    f"ä»¥ä¸‹ã¯{place_name}{r}Rã®å…¨é ­ãƒ‡ãƒ¼ã‚¿ã€‚\n"
                    "â– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿\n"
                    + merged_text
                )

                status_area.info("ğŸ¤– AIãŒåˆ†æãƒ»åŸ·ç­†ä¸­ã§ã™...")

                for chunk in stream_dify_workflow(full_text):
                    if chunk:
                        full_answer += chunk
                        result_area.markdown(full_answer + "â–Œ")

                result_area.markdown(full_answer)

                if full_answer.strip():
                    status_area.success("âœ… åˆ†æå®Œäº†")
                    save_history(YEAR, KAI, PLACE, place_name, DAY, race_num, race_id, full_answer)

                    with st.expander("ğŸ“‹ ã“ã®ãƒ¬ãƒ¼ã‚¹ã®å‡ºåŠ›ã‚’ã‚³ãƒ”ãƒ¼/ä¿å­˜", expanded=False):
                        dom_id = f"copy_race_{race_id}_{int(time.time()*1000)}"
                        render_copy_button(
                            text=full_answer.strip(),
                            label=f"ğŸ“‹ {place_name}{r}R ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ï¼‰",
                            dom_id=dom_id,
                        )
                        st.download_button(
                            label=f"â¬‡ï¸ {place_name}{r}R ã‚’txtä¿å­˜",
                            data=full_answer.strip(),
                            file_name=f"{YEAR}{KAI}{PLACE}{DAY}_{place_name}_{r}R.txt",
                            mime="text/plain",
                            key=f"dl_race_{race_id}",
                        )

                    combined_blocks.append(f"ã€{place_name} {r}Rã€‘\n{full_answer.strip()}\n")

                else:
                    status_area.error("âš ï¸ AIã‹ã‚‰ã®å›ç­”ãŒç©ºã§ã—ãŸã€‚")

            except Exception as e:
                err_msg = f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ ({place_name} {r}R): {str(e)}"
                print(err_msg)
                status_area.error(err_msg)

            st.write("---")

        if combined_blocks:
            combined_text = "\n".join(combined_blocks).strip()
            st.session_state["combined_output"] = combined_text

            st.subheader("ğŸ“Œ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ï¼ˆè¦æ±‚ã—ãŸãƒ¬ãƒ¼ã‚¹ã‚’å…¨éƒ¨ã¾ã¨ã‚ã¦ã‚³ãƒ”ãƒ¼ï¼‰")

            dom_id_all = f"copy_all_{base_id}_{int(time.time()*1000)}"
            render_copy_button(
                text=combined_text,
                label="ğŸ“‹ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ï¼‰",
                dom_id=dom_id_all,
            )

            st.download_button(
                label="â¬‡ï¸ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ã‚’txtä¿å­˜",
                data=combined_text,
                file_name=f"{YEAR}{KAI}{PLACE}{DAY}_{place_name}_ALL.txt",
                mime="text/plain",
                key=f"dl_all_{base_id}",
            )

            with st.expander("ğŸ‘€ ã¾ã¨ã‚è¡¨ç¤ºï¼ˆé–²è¦§ç”¨ï¼‰", expanded=False):
                st.text_area(
                    "å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ãƒ†ã‚­ã‚¹ãƒˆ",
                    value=combined_text,
                    height=420,
                    key=f"ta_all_{base_id}",
                )
        else:
            st.info("ã¾ã¨ã‚å¯¾è±¡ã®å‡ºåŠ›ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    finally:
        try:
            driver.quit()
        except Exception:
            pass
