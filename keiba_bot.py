import time
import json
import re
import random
import requests
import streamlit as st
import streamlit.components.v1 as components
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# â˜…è¿½åŠ :netkeiba
NETKEIBA_ID = st.secrets.get("NETKEIBA_ID", "")
NETKEIBA_PASS = st.secrets.get("NETKEIBA_PASS", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š(app.py å´ã§ set_race_params ãŒå‘¼ã°ã‚Œã‚‹ã¨æ›¸ãæ›ã‚ã‚‹)
YEAR = "2026"
KAI = "01"
PLACE = "05"
DAY = "01"

BASE_URL = "https://s.keibabook.co.jp"

PLACE_NAMES = {
    "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰", "04": "æ±äº¬",
    "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ", "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
}

# â˜…è¿½åŠ :ç«¶é¦¬ãƒ–ãƒƒã‚¯ PLACEã‚³ãƒ¼ãƒ‰ â†’ netkeiba ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
# 01æœ­å¹Œ 02å‡½é¤¨ 03ç¦å³¶ 04æ–°æ½Ÿ 05æ±äº¬ 06ä¸­å±± 07ä¸­äº¬ 08äº¬éƒ½ 09é˜ªç¥ 10å°å€‰
KEIBABOOK_TO_NETKEIBA_PLACE = {
    "08": "01",  # æœ­å¹Œ
    "09": "02",  # å‡½é¤¨
    "06": "03",  # ç¦å³¶
    "07": "04",  # æ–°æ½Ÿ
    "04": "05",  # æ±äº¬
    "05": "06",  # ä¸­å±±
    "02": "07",  # ä¸­äº¬
    "00": "08",  # äº¬éƒ½
    "01": "09",  # é˜ªç¥
    "03": "10",  # å°å€‰
}

# ==================================================
# netkeiba: User-Agentï¼ˆ2024/11ä»¥é™ã®400å¯¾ç­–ï¼‰
# ==================================================
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:115.0) Gecko/20100101 Firefox/115.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:115.0) Gecko/20100101 Firefox/115.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Edg/115.0.0.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 OPR/85.0.4341.72",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 OPR/85.0.4341.72",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Vivaldi/5.3.2679.55",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Vivaldi/5.3.2679.55",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Brave/1.40.107",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Brave/1.40.107",
]

def nk_random_ua() -> str:
    return random.choice(USER_AGENTS)

def nk_sleep(base_sec: float = 2.4, jitter: float = 1.2):
    """netkeibaã®ã‚¢ã‚¯ã‚»ã‚¹é–“éš”: 2ã€œ3ç§’ä»¥ä¸Šã‚’åŸºæœ¬ã«ãƒ©ãƒ³ãƒ€ãƒ """
    time.sleep(base_sec + random.random() * jitter)

def looks_like_netkeiba_block(html: str, current_url: str = "") -> bool:
    """400 / ãƒ–ãƒ­ãƒƒã‚¯ / ãƒ­ã‚°ã‚¤ãƒ³è¦æ±‚ã£ã½ã„ãƒšãƒ¼ã‚¸ã‚’ã–ã£ãã‚Šæ¤œçŸ¥"""
    if not html:
        return True
    h = html.lower()
    if "400 bad request" in h:
        return True
    if "bad request" in h and "400" in h:
        return True
    if "ã‚¢ã‚¯ã‚»ã‚¹ãŒé›†ä¸­" in html or "ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦" in html:
        return True
    if "é€šä¿¡åˆ¶é™" in html:
        return True
    # ãƒ­ã‚°ã‚¤ãƒ³èª˜å°ã«é£›ã°ã•ã‚Œã¦ã‚‹
    if "stage_login" in (current_url or ""):
        return True
    return False

def apply_netkeiba_headers(driver: webdriver.Chrome, ua: str | None = None):
    """
    Chrome DevTools Protocol ã§UAç­‰ã‚’ä¸Šæ›¸ãï¼ˆSelenium Optionsã ã‘ã‚ˆã‚ŠåŠ¹ãã“ã¨ãŒã‚ã‚‹ï¼‰
    """
    if ua is None:
        ua = nk_random_ua()

    try:
        driver.execute_cdp_cmd("Network.enable", {})
        driver.execute_cdp_cmd("Network.setUserAgentOverride", {
            "userAgent": ua,
            "acceptLanguage": "ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7",
            "platform": "Windows",
        })
    except Exception:
        # ç’°å¢ƒã«ã‚ˆã£ã¦ã¯å¤±æ•—ã™ã‚‹ã®ã§æ¡ã‚Šã¤ã¶ã™ï¼ˆOptionså´UAã¯åŠ¹ã„ã¦ã‚‹æƒ³å®šï¼‰
        pass


def set_race_params(year, kai, place, day):
    """app.py ã‹ã‚‰é–‹å‚¬æƒ…å ±ã‚’å·®ã—æ›¿ãˆã‚‹ãŸã‚ã®é–¢æ•°"""
    global YEAR, KAI, PLACE, DAY
    YEAR = str(year)
    KAI = str(kai).zfill(2)
    PLACE = str(place).zfill(2)
    DAY = str(day).zfill(2)

def get_current_params():
    """ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿(UIè¡¨ç¤ºç”¨)"""
    return YEAR, KAI, PLACE, DAY


# ==================================================
# â˜…ä¿®æ­£:netkeiba æŒ‡æ•°ã‚»ãƒ«æ­£è¦åŒ–(1000ã¯å¿…ãšã€Œç„¡ã€)
# ==================================================
def normalize_netkeiba_index_cell(raw: str) -> str:
    """
    - netkeibaæŒ‡æ•°ãŒã€Œæœªã€ã€Œ-ã€ã®ã¨ãå†…éƒ¨çš„ã«1000ã«ãªã‚‹ã‚±ãƒ¼ã‚¹ã‚’ã€Œç„¡ã€ã«çµ±ä¸€
    - "1070 70" ç­‰ã®æ··åœ¨ã‹ã‚‰æœ¬å‘½å€¤ã ã‘ã‚’æŠ½å‡º
    ãƒ«ãƒ¼ãƒ«:
    - "1000" ã¯ç„¡æ¡ä»¶ã§ã€Œç„¡ã€
    - "æœª" / "-" / "ï¼" / ç©º ã¯ã€Œç„¡ã€
    - æ•°å­—ã¯ 3æ¡ä»¥ä¸‹ã‚’å„ªå…ˆã—ã¦æœ«å°¾ã‚’æ¡ç”¨(70, 54, 107ãªã©)
    - ãã‚Œã‚‚ç„¡ã‘ã‚Œã°ã€Œç„¡ã€
    """
    if raw is None:
        return "ç„¡"

    t = str(raw).replace("\xa0", " ").strip()
    if t == "":
        return "ç„¡"

    if "æœª" in t:
        return "ç„¡"
    if "ï¼" in t or "-" in t:
        return "ç„¡"

    nums = re.findall(r"\d+", t)
    if not nums:
        return "ç„¡"

    if any(n == "1000" for n in nums):
        short = [n for n in nums if len(n) <= 3 and n != "1000"]
        return short[-1] if short else "ç„¡"

    short = [n for n in nums if len(n) <= 3]
    if short:
        return short[-1]

    return "ç„¡"


# ==================================================
# ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚³ãƒ”ãƒ¼
# ==================================================
def render_copy_button(text: str, label: str, dom_id: str):
    safe_text = json.dumps(text)
    html = f"""
    <div style="display:flex; gap:8px; align-items:center; flex-wrap:wrap;">
      <button id="{dom_id}" style="
        padding:8px 12px;
        border-radius:10px;
        border:1px solid #ddd;
        background:#fff;
        cursor:pointer;
        font-size:14px;
      ">{label}</button>
      <span id="{dom_id}-msg" style="font-size:12px; color:#666;"></span>
    </div>
    <script>
      (function() {{
        const btn = document.getElementById("{dom_id}");
        const msg = document.getElementById("{dom_id}-msg");
        if (!btn) return;

        btn.addEventListener("click", async () => {{
          try {{
            await navigator.clipboard.writeText({safe_text});
            msg.textContent = "ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";
            setTimeout(() => msg.textContent = "", 1200);
          }} catch (e) {{
            msg.textContent = "ã‚³ãƒ”ãƒ¼ã«å¤±æ•—(ãƒ–ãƒ©ã‚¦ã‚¶åˆ¶é™ã®å¯èƒ½æ€§)";
            setTimeout(() => msg.textContent = "", 2200);
          }}
        }});
      }})();
    </script>
    """
    components.html(html, height=54)


# ==================================================
# Supabase
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client | None:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)

def save_history(
    year: str,
    kai: str,
    place_code: str,
    place_name: str,
    day: str,
    race_num_str: str,
    race_id: str,
    ai_answer: str,
) -> None:
    supabase = get_supabase_client()
    if supabase is None:
        return

    data = {
        "year": str(year),
        "kai": str(kai),
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }

    try:
        supabase.table("history").insert(data).execute()
    except Exception as e:
        print("Supabase insert error:", e)


# ==================================================
# Selenium
# ==================================================
def build_driver() -> webdriver.Chrome:
    ua = nk_random_ua()

    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,2200")

    # â˜…UAè¨­å®šï¼ˆnetkeiba 400å¯¾ç­–ã®æœ¬ä¸¸ï¼‰
    options.add_argument(f"--user-agent={ua}")
    options.add_argument("--lang=ja-JP")
    options.add_argument("--accept-lang=ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7")

    # â˜…è‡ªå‹•åŒ–ãƒ•ãƒ©ã‚°ã‚’å°‘ã—ã ã‘å¼±ã‚ã‚‹ï¼ˆåŠ¹ãç’°å¢ƒã‚‚ã‚ã‚‹ï¼‰
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)

    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(60)

    # â˜…webdriveræ¤œçŸ¥ã‚’å¼±ã‚ã‚‹ï¼ˆä¸‡èƒ½ã§ã¯ãªã„ãŒã‚„ã‚‹ä¾¡å€¤ã‚ã‚Šï¼‰
    try:
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": """
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                Object.defineProperty(navigator, 'languages', {get: () => ['ja-JP','ja','en-US','en']});
                Object.defineProperty(navigator, 'platform', {get: () => 'Win32'});
            """
        })
    except Exception:
        pass

    # CDPä¸Šæ›¸ãã‚‚åˆå›é©ç”¨
    apply_netkeiba_headers(driver, ua=ua)
    return driver


def login_keibabook(driver: webdriver.Chrome) -> None:
    if not KEIBA_ID or not KEIBA_PASS:
        raise RuntimeError("KEIBA_ID / KEIBA_PASS ãŒ secrets ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    driver.get(f"{BASE_URL}/login/login")

    WebDriverWait(driver, 15).until(
        EC.visibility_of_element_located((By.NAME, "login_id"))
    ).send_keys(KEIBA_ID)

    WebDriverWait(driver, 15).until(
        EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
    ).send_keys(KEIBA_PASS)

    WebDriverWait(driver, 15).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))
    ).click()

    time.sleep(1.2)


# ==================================================
# â˜…å¼·åŒ–: netkeibaãƒ­ã‚°ã‚¤ãƒ³
# ==================================================
def login_netkeiba(driver: webdriver.Chrome) -> bool:
    """
    æˆåŠŸã—ãŸã‚‰ Trueã€å¤±æ•—/æœªè¨­å®šãªã‚‰ False
    - UA/Language ã‚’CDPã§ä¸Šæ›¸ãã—ã¦ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹
    - 400/ãƒ–ãƒ­ãƒƒã‚¯ã£ã½ã„å ´åˆã¯å°‘ã—å¾…ã£ã¦å†è©¦è¡Œ
    """
    if not NETKEIBA_ID or not NETKEIBA_PASS:
        print("netkeiba login: IDã¾ãŸã¯ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæœªè¨­å®š")
        return False

    try:
        # UAã‚’æ¯å›æ›´æ–°ï¼ˆåŒä¸€UAå›ºå®šã§å¼¾ã‹ã‚Œã‚‹ç’°å¢ƒãŒã‚ã‚‹ï¼‰
        apply_netkeiba_headers(driver, ua=nk_random_ua())

        login_url = "https://regist.netkeiba.com/?pid=stage_login"
        driver.get(login_url)
        nk_sleep()

        # 400/ãƒ–ãƒ­ãƒƒã‚¯æ¤œçŸ¥
        if looks_like_netkeiba_block(driver.page_source, driver.current_url):
            print("netkeiba login: ãƒ–ãƒ­ãƒƒã‚¯/400ã£ã½ã„ã€‚å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™ã€‚")
            nk_sleep(base_sec=6.0, jitter=4.0)
            apply_netkeiba_headers(driver, ua=nk_random_ua())
            driver.get(login_url)
            nk_sleep()

        html = driver.page_source
        if looks_like_netkeiba_block(html, driver.current_url):
            print("netkeiba login: ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸è‡ªä½“ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã¾ã™ï¼ˆ400/åˆ¶é™ã®å¯èƒ½æ€§ï¼‰")
            return False

        # IDå…¥åŠ›æ¬„
        id_candidates = [
            (By.NAME, "login_id"),
            (By.NAME, "userid"),
            (By.NAME, "id"),
            (By.ID, "login_id"),
            (By.CSS_SELECTOR, "input[type='text'][name*='login']"),
            (By.CSS_SELECTOR, "input[type='email']"),
            (By.CSS_SELECTOR, "input[type='text']"),
        ]
        id_el = None
        for how, sel in id_candidates:
            try:
                id_el = WebDriverWait(driver, 6).until(
                    EC.visibility_of_element_located((how, sel))
                )
                if id_el:
                    break
            except Exception:
                continue

        # PWå…¥åŠ›æ¬„
        pass_candidates = [
            (By.NAME, "pswd"),
            (By.NAME, "password"),
            (By.ID, "pswd"),
            (By.CSS_SELECTOR, "input[type='password']"),
        ]
        pw_el = None
        for how, sel in pass_candidates:
            try:
                pw_el = WebDriverWait(driver, 6).until(
                    EC.visibility_of_element_located((how, sel))
                )
                if pw_el:
                    break
            except Exception:
                continue

        if not id_el or not pw_el:
            print("netkeiba login: å…¥åŠ›æ¬„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False

        id_el.clear()
        id_el.send_keys(NETKEIBA_ID)
        time.sleep(0.2)

        pw_el.clear()
        pw_el.send_keys(NETKEIBA_PASS)
        time.sleep(0.2)

        # ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³
        btn_candidates = [
            (By.CSS_SELECTOR, "input[type='submit'][value*='ãƒ­ã‚°ã‚¤ãƒ³']"),
            (By.CSS_SELECTOR, "button[type='submit']"),
            (By.CSS_SELECTOR, "input[type='submit']"),
            (By.CSS_SELECTOR, ".Btn_Login"),
            (By.CSS_SELECTOR, ".btn_login"),
            (By.XPATH, "//input[@type='submit' and contains(@value,'ãƒ­ã‚°ã‚¤ãƒ³')]"),
        ]
        clicked = False
        for how, sel in btn_candidates:
            try:
                btn = WebDriverWait(driver, 6).until(
                    EC.element_to_be_clickable((how, sel))
                )
                btn.click()
                clicked = True
                break
            except Exception:
                continue

        if not clicked:
            print("netkeiba login: ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False

        nk_sleep(base_sec=2.2, jitter=1.5)

        # æˆåŠŸåˆ¤å®šï¼ˆè¤‡åˆï¼‰
        html2 = driver.page_source
        url2 = driver.current_url

        ok_signals = 0
        if "ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ" in html2 or "action=logout" in html2 or "logout" in html2.lower():
            ok_signals += 1
        if "stage_login" not in url2:
            ok_signals += 1
        if "ãƒã‚¤ãƒšãƒ¼ã‚¸" in html2:
            ok_signals += 1

        if ok_signals >= 2:
            print("netkeiba login: ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ")
            return True

        # ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—ï¼ˆã‚¨ãƒ©ãƒ¼æ–‡è¨€ãŒã‚ã‚Œã°è¡¨ç¤ºï¼‰
        if "ã‚¨ãƒ©ãƒ¼" in html2 or "å…¥åŠ›" in html2 and "ä¸æ­£" in html2:
            print("netkeiba login: ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—ï¼ˆèªè¨¼ã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ï¼‰")

        print(f"netkeiba login: ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—ã®å¯èƒ½æ€§ url={url2}")
        return False

    except Exception as e:
        print(f"netkeiba login: ä¾‹å¤–ç™ºç”Ÿ: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


# ==================================================
# Parser:å…±é€š(ç«¶é¦¬ãƒ–ãƒƒã‚¯)
# ==================================================
def parse_race_info(html: str):
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"date_meet": "", "race_name": "", "cond1": "", "course_line": ""}

    racemei = racetitle.find("div", class_="racemei")
    date_meet = ""
    race_name = ""
    if racemei:
        ps = racemei.find_all("p")
        if len(ps) >= 1:
            date_meet = ps[0].get_text(strip=True)
        if len(ps) >= 2:
            race_name = ps[1].get_text(strip=True)

    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    cond1 = ""
    course_line = ""
    if racetitle_sub:
        sub_ps = racetitle_sub.find_all("p")
        if len(sub_ps) >= 1:
            cond1 = sub_ps[0].get_text(strip=True)
        if len(sub_ps) >= 2:
            course_line = sub_ps[1].get_text(" ", strip=True)

    return {
        "date_meet": date_meet,
        "race_name": race_name,
        "cond1": cond1,
        "course_line": course_line,
    }


def parse_danwa_comments(html: str):
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table or not table.tbody:
        return {}

    danwa_dict = {}
    current_key = None

    for row in table.tbody.find_all("tr"):
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")

        if uma_td:
            text = re.sub(r"\D", "", uma_td.get_text(strip=True))
            if text:
                current_key = text
                continue

        if bamei_td and not current_key:
            text = bamei_td.get_text(strip=True)
            if text:
                current_key = text
                continue

        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current_key:
            danwa_dict[current_key] = danwa_td.get_text(strip=True)
            current_key = None

    return danwa_dict


def parse_zenkoso_interview(html: str):
    soup = BeautifulSoup(html, "html.parser")
    h2 = soup.find("h2", string=lambda s: s and "å‰èµ°" in s)
    if not h2:
        return {}

    table = h2.find_next("table", class_="syoin")
    if not table or not table.tbody:
        return {}

    rows = table.tbody.find_all("tr")
    result_dict = {}

    i = 0
    while i < len(rows):
        row = rows[i]
        if "spacer" in (row.get("class") or []):
            i += 1
            continue

        waku_td = row.find("td", class_="waku")
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")

        if not (waku_td and uma_td and bamei_td):
            i += 1
            continue

        waku = waku_td.get_text(strip=True)
        umaban = re.sub(r"\D", "", uma_td.get_text(strip=True))
        name = bamei_td.get_text(strip=True)

        prev_date = ""
        prev_class = ""
        prev_finish = ""
        prev_comment = ""

        detail = rows[i + 1] if i + 1 < len(rows) else None
        if detail:
            syoin_td = detail.find("td", class_="syoin")
            if syoin_td:
                sdata = syoin_td.find("div", class_="syoindata")
                if sdata:
                    ps = sdata.find_all("p")
                    if ps:
                        prev_date = ps[0].get_text(strip=True)
                    if len(ps) >= 2:
                        spans = ps[1].find_all("span")
                        if len(spans) >= 1:
                            prev_class = spans[0].get_text(strip=True)
                        if len(spans) >= 2:
                            prev_finish = spans[1].get_text(strip=True)

                direct = syoin_td.find_all("p", recursive=False)
                if direct:
                    txt = direct[0].get_text(strip=True)
                    if txt != "ï¼":
                        prev_comment = txt

        if umaban:
            result_dict[umaban] = {
                "waku": waku,
                "umaban": umaban,
                "name": name,
                "prev_date_course": prev_date,
                "prev_class": prev_class,
                "prev_finish": prev_finish,
                "prev_comment": prev_comment,
            }

        i += 2

    return result_dict


def parse_cyokyo(html: str):
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}

    section = None
    h2 = soup.find("h2", string=lambda s: s and ("èª¿æ•™" in s or "ä¸­é–“" in s))
    if h2:
        midasi_div = h2.find_parent("div", class_="midasi")
        if midasi_div:
            section = midasi_div.find_next_sibling("div", class_="section")
    if section is None:
        section = soup

    tables = section.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody:
            continue
        rows = tbody.find_all("tr", recursive=False)
        if len(rows) < 1:
            continue

        header = rows[0]
        uma_td = header.find("td", class_="umaban")
        name_td = header.find("td", class_="kbamei")

        umaban_text = uma_td.get_text(strip=True) if uma_td else ""
        umaban = re.sub(r"\D", "", umaban_text)

        bamei_hint = ""
        if name_td:
            bamei_hint = name_td.get_text(" ", strip=True)

        tanpyo_td = header.find("td", class_="tanpyo")
        tanpyo = tanpyo_td.get_text(strip=True) if tanpyo_td else ""

        detail_row = rows[1] if len(rows) >= 2 else None
        detail_text = detail_row.get_text(" ", strip=True) if detail_row else ""

        payload = {"tanpyo": tanpyo, "detail": detail_text, "bamei_hint": bamei_hint}

        if umaban:
            cyokyo_dict[umaban] = payload
        else:
            if bamei_hint:
                cyokyo_dict[bamei_hint] = payload

    return cyokyo_dict


def parse_syutuba(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")

    table = soup.find("table", class_=lambda c: c and "syutuba_sp" in c.split())
    if not table:
        table = soup.find("table", class_=lambda c: c and "syutuba" in c)

    if not table or not table.tbody:
        return {}

    result = {}
    for tr in table.tbody.find_all("tr", recursive=False):
        tds = tr.find_all("td", recursive=False)
        if not tds:
            continue

        umaban_raw = tds[0].get_text(strip=True)
        umaban = re.sub(r"\D", "", umaban_raw)
        if not umaban:
            continue

        bamei = ""
        kbamei_p = tr.find("p", class_="kbamei")
        if kbamei_p:
            bamei = kbamei_p.get_text(" ", strip=True)

        kisyu = ""
        kisyu_change = False

        kisyu_p = tr.find("p", class_="kisyu")
        if kisyu_p:
            a = kisyu_p.find("a")
            if a:
                norika = a.find("span", class_="norikawari")
                if norika:
                    kisyu_change = True
                    kisyu = norika.get_text(strip=True)
                else:
                    kisyu = a.get_text(strip=True)
            else:
                norika = kisyu_p.find("span", class_="norikawari")
                if norika:
                    kisyu_change = True
                    kisyu = norika.get_text(strip=True)
                else:
                    kisyu = kisyu_p.get_text(" ", strip=True)

        result[umaban] = {
            "umaban": umaban,
            "bamei": bamei,
            "kisyu": kisyu,
            "kisyu_change": kisyu_change,
        }

    return result


# ==================================================
# â˜…ä¿®æ­£å¼·åŒ–:netkeiba ã‚¿ã‚¤ãƒ æŒ‡æ•° parser(1000â†’ç„¡ ã‚’é©ç”¨)
# ==================================================
def parse_netkeiba_speed_index(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_=lambda c: c and ("SpeedIndex_Table" in c))
    if not table or not table.tbody:
        return {}

    out = {}

    for tr in table.tbody.find_all("tr", class_=lambda c: c and ("HorseList" in c.split()), recursive=False):
        um_td = tr.find("td", class_=lambda c: c and "sk__umaban" in c)
        if not um_td:
            continue
        umaban = re.sub(r"\D", "", um_td.get_text(" ", strip=True))
        if not umaban:
            continue

        def cell_text(cell_class: str) -> str:
            td = tr.find("td", class_=lambda c: c and cell_class in c.split())
            if not td:
                return "ç„¡"

            hidden_span = td.find("span", class_="Sort_Function_Data_Hidden")
            if hidden_span:
                a_tag = td.find("a")
                if a_tag:
                    txt = a_tag.get_text(strip=True)
                else:
                    hidden_span.decompose()
                    txt = td.get_text(strip=True)
            else:
                txt = td.get_text(strip=True)

            return normalize_netkeiba_index_cell(txt)

        out[umaban] = {
            "index1": cell_text("sk__index1"),
            "index2": cell_text("sk__index2"),
            "index3": cell_text("sk__index3"),
            "course": cell_text("sk__max_course_index"),
            "avg5": cell_text("sk__average_index"),
        }

    return out


def safe_get_netkeiba(driver: webdriver.Chrome, url: str, retries: int = 2) -> str:
    """
    netkeibaã‚¢ã‚¯ã‚»ã‚¹å…±é€š:
    - UAã‚’æ¯å›æ›´æ–°
    - 2ã€œ3ç§’ä»¥ä¸Šã®é–“éš”
    - 400/ãƒ–ãƒ­ãƒƒã‚¯æ™‚ã¯ãƒãƒƒã‚¯ã‚ªãƒ•ã—ã¦å†è©¦è¡Œ
    """
    last_html = ""
    for i in range(retries + 1):
        apply_netkeiba_headers(driver, ua=nk_random_ua())
        driver.get(url)
        nk_sleep()

        html = driver.page_source
        last_html = html

        if not looks_like_netkeiba_block(html, driver.current_url):
            return html

        wait = 5.0 + i * 6.0 + random.random() * 3.0
        print(f"netkeiba blocked/400 detected. retry {i+1}/{retries}, sleep={wait:.1f}s")
        time.sleep(wait)

    return last_html


def fetch_netkeiba_speed_dict(driver: webdriver.Chrome, netkeiba_race_id: str) -> dict:
    url = f"https://race.netkeiba.com/race/speed.html?race_id={netkeiba_race_id}&type=shutuba&mode=default"

    html = safe_get_netkeiba(driver, url, retries=2)

    # ãƒ­ã‚°ã‚¤ãƒ³è¦æ±‚ãªã‚‰ä¸€åº¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦å–ã‚Šç›´ã—
    needs_login = False
    if "ç„¡æ–™ä¼šå“¡ç™»éŒ²" in html or "ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦" in html or "ä¼šå“¡ç™»éŒ²" in html:
        needs_login = True
    if "stage_login" in driver.current_url or "login" in driver.current_url.lower():
        needs_login = True

    if needs_login and NETKEIBA_ID and NETKEIBA_PASS:
        print(f"netkeiba speed: ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ (URL: {driver.current_url})")
        ok = login_netkeiba(driver)
        if ok:
            html = safe_get_netkeiba(driver, url, retries=2)

    # ãƒ†ãƒ¼ãƒ–ãƒ«å¾…æ©Ÿï¼ˆJSé…å»¶ã«å‚™ãˆã‚‹ï¼‰
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.SpeedIndex_Table"))
        )
        time.sleep(0.6)
        html = driver.page_source
    except Exception as e:
        print(f"netkeiba speed: ãƒ†ãƒ¼ãƒ–ãƒ«å¾…æ©Ÿã‚¨ãƒ©ãƒ¼: {e}")

    # ãƒ–ãƒ­ãƒƒã‚¯ã£ã½ã„ãªã‚‰ç©ºã§è¿”ã™ï¼ˆã“ã®å¾Œã®å‡¦ç†ã¯ç¶™ç¶šï¼‰
    if looks_like_netkeiba_block(html, driver.current_url):
        print("netkeiba speed: blocked/400 page. return empty.")
        return {}

    # ãƒ‘ãƒ¼ã‚¹å®Ÿè¡Œ
    result = parse_netkeiba_speed_index(html)
    print(f"netkeiba speed: parsed {len(result)} horses")
    return result


def keibabook_race_id_to_netkeiba_race_id(year: str, kai: str, place: str, day: str, race_num_2: str) -> str:
    nk_place = KEIBABOOK_TO_NETKEIBA_PLACE.get(place)
    if not nk_place:
        return ""
    return f"{str(year)}{nk_place}{str(kai).zfill(2)}{str(day).zfill(2)}{str(race_num_2).zfill(2)}"


# ==================================================
# â˜…ä¿®æ­£:netkeiba é¦¬æŸ±(5èµ°) ã‹ã‚‰ã€Œæˆ¦ç¸¾ã€ã‚’æŠ½å‡º
# ==================================================
def _extract_race_result_from_past_td(past_td) -> str:
    if past_td is None:
        return ""

    cell_text = past_td.get_text(" ", strip=True)
    if not cell_text or len(cell_text) < 5:
        return ""

    data_item = past_td.find("div", class_="Data_Item")
    container = data_item if data_item else past_td

    # Data01
    data01 = container.find("div", class_="Data01")
    date_str = ""
    place_str = ""
    rank_num = ""

    if data01:
        spans = data01.find_all("span")
        if spans:
            first_span_text = spans[0].get_text(" ", strip=True)
            date_match = re.search(r"(\d{4}[./]\d{1,2}[./]\d{1,2})", first_span_text)
            if date_match:
                raw_date = date_match.group(1)
                date_str = raw_date.replace("/", ".")

            place_match = re.search(r"\d{4}[./]\d{1,2}[./]\d{1,2}\s*(.+)", first_span_text)
            if place_match:
                place_str = place_match.group(1).strip()
                place_str = re.sub(r"\d+$", "", place_str).strip()

        num_span = data01.find("span", class_="Num")
        if num_span:
            rank_num = num_span.get_text(strip=True)
            if rank_num and rank_num not in ["ä¸­", "å–", "é™¤"]:
                rank_num = re.sub(r"\D", "", rank_num)

    # Data02
    data02 = container.find("div", class_="Data02")
    race_name = ""
    class_str = ""

    if data02:
        a_tag = data02.find("a")
        if a_tag:
            for span in a_tag.find_all("span"):
                span_text = span.get_text(strip=True)
                if span_text:
                    class_str = span_text
            race_name = a_tag.get_text(strip=True)
            if class_str and class_str in race_name:
                race_name = race_name.replace(class_str, "").strip()
        else:
            race_name = data02.get_text(strip=True)

        grade_span = data02.find("span", class_=lambda c: c and "Icon_GradeType" in str(c))
        if grade_span:
            class_str = grade_span.get_text(strip=True)

    # Data05
    data05 = container.find("div", class_="Data05")
    course_str = ""
    if data05:
        text = data05.get_text(" ", strip=True)
        course_match = re.match(r"(èŠ|ãƒ€)\d+(\([å†…å¤–]\))?", text)
        if course_match:
            course_str = course_match.group(0)

    # Data03
    data03 = container.find("div", class_="Data03")
    field_info = ""
    jockey = ""
    weight = ""
    if data03:
        text = data03.get_text(" ", strip=True)
        head_match = re.search(r"(\d+)é ­", text)
        num_match = re.search(r"(\d+)ç•ª", text)
        pop_match = re.search(r"(\d+)äºº", text)

        parts = []
        if head_match:
            parts.append(f"{head_match.group(1)}é ­")
        if num_match:
            parts.append(f"{num_match.group(1)}ç•ª")
        if pop_match:
            parts.append(f"{pop_match.group(1)}äºº")
        field_info = "".join(parts)

        weight_match = re.search(r"(\d+\.\d+|\d+\.0)", text)
        if weight_match:
            weight = weight_match.group(1)

        jockey_match = re.search(r"\d+äºº\s+([^\d\s]+(?:\s+[^\d\s]+)?)\s+\d", text)
        if jockey_match:
            jockey = jockey_match.group(1).strip()
        else:
            jockey_match2 = re.search(r"ç•ª\s+([^\d]+)\s+\d+\.", text)
            if jockey_match2:
                jockey = jockey_match2.group(1).strip()

    # Data06
    data06 = container.find("div", class_="Data06")
    passing = ""
    if data06:
        text = data06.get_text(" ", strip=True)
        passing_match = re.match(r"([\d]+-[\d]+(?:-[\d]+)*)", text)
        if passing_match:
            passing = passing_match.group(1)

    result_parts = []
    if date_str:
        result_parts.append(date_str)
    if place_str:
        result_parts.append(place_str)
    if race_name and class_str:
        result_parts.append(f"{race_name}({class_str})")
    elif race_name:
        result_parts.append(race_name)
    if course_str:
        result_parts.append(course_str)
    if field_info:
        result_parts.append(field_info)
    if jockey and weight:
        result_parts.append(f"{jockey} {weight}")
    elif jockey:
        result_parts.append(jockey)
    if passing:
        result_parts.append(f"â†’{passing}")
    if rank_num:
        result_parts.append(f"...æœ€çµ‚{rank_num}ç€")

    return " ".join(result_parts) if result_parts else ""


def parse_netkeiba_shutuba_past5(html: str, take_last_n: int = 3) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", id="sort_table")
    if not table:
        table = soup.find("table", class_=lambda c: c and "Shutuba_Past5_Table" in str(c))
    if not table:
        table = soup.find("table", class_=lambda c: c and "Shutuba_Table" in str(c))
    if not table:
        return {}

    tbody = table.find("tbody") or table
    out = {}

    rows = tbody.find_all("tr", class_=lambda c: c and "HorseList" in str(c))
    for tr in rows:
        umaban = ""
        waku_tds = tr.find_all("td", class_=lambda c: c and "Waku" in str(c))
        for td in waku_tds:
            td_class = td.get("class") or []
            if td_class == ["Waku"]:
                umaban = re.sub(r"\D", "", td.get_text(strip=True))
                break
        if not umaban and len(waku_tds) >= 2:
            umaban = re.sub(r"\D", "", waku_tds[1].get_text(strip=True))
        if not umaban:
            continue

        past_tds = tr.find_all("td", class_=lambda c: c and "Past" in str(c))
        past_tds = [td for td in past_tds if "Rest" not in str(td.get("class", []))]

        past_summaries = []
        for td in past_tds[:take_last_n]:
            past_summaries.append(_extract_race_result_from_past_td(td))

        while len(past_summaries) < take_last_n:
            past_summaries.append("")

        out[umaban] = {"past3": past_summaries[:take_last_n]}

    return out


def fetch_netkeiba_past5_dict(driver: webdriver.Chrome, netkeiba_race_id: str) -> dict:
    url = f"https://race.netkeiba.com/race/shutuba_past.html?race_id={netkeiba_race_id}&rf=shutuba_submenu"

    html = safe_get_netkeiba(driver, url, retries=2)

    # ãƒ­ã‚°ã‚¤ãƒ³è¦æ±‚ãªã‚‰ä¸€åº¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦å–ã‚Šç›´ã—
    if (("ç„¡æ–™ä¼šå“¡ç™»éŒ²" in html or "ãƒ­ã‚°ã‚¤ãƒ³" in html) or ("stage_login" in driver.current_url)) and NETKEIBA_ID and NETKEIBA_PASS:
        ok = login_netkeiba(driver)
        if ok:
            html = safe_get_netkeiba(driver, url, retries=2)

    # JSå¾…æ©Ÿ
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table#sort_table, table.Shutuba_Past5_Table"))
        )
        time.sleep(0.6)
        html = driver.page_source
    except Exception:
        pass

    if looks_like_netkeiba_block(html, driver.current_url):
        print("netkeiba past5: blocked/400 page. return empty.")
        return {}

    return parse_netkeiba_shutuba_past5(html, take_last_n=3)


# ==================================================
# fetch(Selenium)ç«¶é¦¬ãƒ–ãƒƒã‚¯
# ==================================================
def fetch_danwa_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/danwa/0/{race_id}"
    driver.get(url)
    time.sleep(0.8)
    html = driver.page_source
    return html, parse_race_info(html), parse_danwa_comments(html)

def fetch_zenkoso_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syoin/{race_id}"
    driver.get(url)
    time.sleep(0.8)
    return parse_zenkoso_interview(driver.page_source)

def fetch_cyokyo_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/cyokyo/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.cyokyo"))
        )
    except Exception:
        pass
    return parse_cyokyo(driver.page_source)

def fetch_syutuba_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syutuba/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.syutuba_sp, table.syutuba"))
        )
    except Exception:
        pass
    return parse_syutuba(driver.page_source)


# ==================================================
# ç›´è¿‘é–‹å‚¬:è¤‡æ•°å€™è£œæ¤œå‡º
# ==================================================
def detect_meet_candidates(driver, max_candidates: int = 12):
    driver.get(f"{BASE_URL}/cyuou/")
    time.sleep(1.0)
    html = driver.page_source

    keys12 = re.findall(r"/cyuou/syutuba/(\d{12})", html)
    if not keys12:
        keys12 = re.findall(r"/cyuou/thursday/(\d{12})", html)

    if not keys12:
        driver.get(f"{BASE_URL}/")
        time.sleep(1.0)
        html2 = driver.page_source
        keys12 = re.findall(r"/cyuou/syutuba/(\d{12})", html2)
        if not keys12:
            keys12 = re.findall(r"/cyuou/thursday/(\d{12})", html2)

    if not keys12:
        return []

    meet10_set = set(k[:10] for k in keys12 if len(k) >= 10)
    meet10_sorted = sorted(meet10_set, reverse=True)

    candidates = []
    for m10 in meet10_sorted[:max_candidates]:
        year = m10[0:4]
        kai = m10[4:6]
        place = m10[6:8]
        day = m10[8:10]
        candidates.append({
            "meet10": m10,
            "year": year,
            "kai": kai,
            "place": place,
            "day": day,
            "place_name": PLACE_NAMES.get(place, "ä¸æ˜"),
        })

    return candidates

def auto_detect_meet_candidates():
    driver = build_driver()
    try:
        login_keibabook(driver)
        return detect_meet_candidates(driver)
    finally:
        try:
            driver.quit()
        except Exception:
            pass


# ==================================================
# â˜…ä¿®æ­£:Dify(Streaming)
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ ã‚¨ãƒ©ãƒ¼: DIFY_API_KEY ãŒæœªè¨­å®š"
        return

    payload = {
        "inputs": {"text": full_text},
        "response_mode": "streaming",
        "user": "keiba-bot-user",
    }

    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}",
        "Content-Type": "application/json",
    }

    try:
        res = requests.post(
            "https://api.dify.ai/v1/workflows/run",
            headers=headers,
            json=payload,
            stream=True,
            timeout=600,
        )

        if res.status_code != 200:
            yield f"âš ï¸ ã‚¨ãƒ©ãƒ¼: Dify API Error {res.status_code}\n{res.text}"
            return

        for line in res.iter_lines():
            if not line:
                continue
            decoded = line.decode("utf-8", errors="ignore")
            if not decoded.startswith("data:"):
                continue

            json_str = decoded.replace("data: ", "")
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError:
                continue

            event = data.get("event")
            if event in ["workflow_started", "node_started", "node_finished"]:
                continue

            chunk = data.get("answer", "")
            if chunk:
                yield chunk

            if event == "workflow_finished":
                outputs = data.get("data", {}).get("outputs", {})
                if outputs:
                    found_text = ""
                    for _, value in outputs.items():
                        if isinstance(value, str):
                            found_text += value + "\n"
                    if found_text.strip():
                        yield found_text.strip()

    except requests.exceptions.Timeout:
        yield "\n\nâš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: Difyã®å‡¦ç†æ™‚é–“ãŒé•·ã™ãã¾ã™ã€‚"
    except Exception as e:
        yield f"\n\nâš ï¸ Request Error: {str(e)}"


# ==================================================
# çµåˆç”¨:é¦¬åã‚­ãƒ¼æ•‘æ¸ˆ
# ==================================================
def _find_by_name_key(d: dict, bamei: str):
    if not bamei:
        return None
    if bamei in d:
        return d[bamei]
    for k, v in d.items():
        if (not str(k).isdigit()) and (str(k).strip() == bamei.strip()):
            return v
    return None


# ==================================================
# â˜…ãƒ¡ã‚¤ãƒ³å‡¦ç†(è¤‡æ•°ãƒ¬ãƒ¼ã‚¹)
# ==================================================
def run_all_races(target_races=None):
    race_numbers = (
        list(range(1, 13))
        if target_races is None
        else sorted({int(r) for r in target_races})
    )

    base_id = f"{YEAR}{KAI}{PLACE}{DAY}"
    place_name = PLACE_NAMES.get(PLACE, "ä¸æ˜")

    combined_blocks: list[str] = []

    driver = build_driver()

    try:
        st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...(ç«¶é¦¬ãƒ–ãƒƒã‚¯)")
        login_keibabook(driver)
        st.success("âœ… ç«¶é¦¬ãƒ–ãƒƒã‚¯ ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†")

        # â˜…netkeiba: æœ€åˆã«ä¸€åº¦ã ã‘ãƒ­ã‚°ã‚¤ãƒ³ï¼ˆãŸã ã—å¤±æ•—ã—ã¦ã‚‚å‡¦ç†ç¶™ç¶šï¼‰
        netkeiba_logged_in = False
        if NETKEIBA_ID and NETKEIBA_PASS:
            st.info("ğŸ”‘ netkeiba ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
            netkeiba_logged_in = login_netkeiba(driver)
            if netkeiba_logged_in:
                st.success("âœ… netkeiba ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ç¢ºç«‹ï¼‰")
            else:
                st.warning("âš ï¸ netkeiba ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸã€‚UA/ã‚¢ã‚¯ã‚»ã‚¹é–“éš”/IPåˆ¶é™ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        else:
            st.info("â„¹ï¸ netkeibaèªè¨¼æƒ…å ±ãŒæœªè¨­å®šã®ãŸã‚ã€ç„¡æ–™ç¯„å›²ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿å–å¾—ã—ã¾ã™ã€‚")

        for r in race_numbers:
            race_num = f"{r:02}"
            race_id = base_id + race_num
            netkeiba_race_id = keibabook_race_id_to_netkeiba_race_id(YEAR, KAI, PLACE, DAY, race_num)

            st.markdown(f"### {place_name} {r}R")

            status_area = st.empty()
            result_container = st.container()
            full_answer = ""

            try:
                status_area.info(f"ğŸ“¡ {place_name}{r}R ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")

                _html_danwa, race_info, danwa_dict = fetch_danwa_dict(driver, race_id)
                zenkoso_dict = fetch_zenkoso_dict(driver, race_id)
                cyokyo_dict = fetch_cyokyo_dict(driver, race_id)
                syutuba_dict = fetch_syutuba_dict(driver, race_id)

                if not syutuba_dict:
                    status_area.warning("âš ï¸ å‡ºé¦¬è¡¨ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ(å…¨é ­ä¿è¨¼ã§ããªã„å¯èƒ½æ€§)ã€‚")

                # netkeiba æŒ‡æ•°
                speed_dict = {}
                if netkeiba_race_id:
                    try:
                        status_area.info(f"ğŸ“Š netkeiba æŒ‡æ•°ã‚’å–å¾—ä¸­... (race_id: {netkeiba_race_id})")
                        speed_dict = fetch_netkeiba_speed_dict(driver, netkeiba_race_id)
                        if speed_dict:
                            status_area.success(f"âœ… netkeiba æŒ‡æ•°å–å¾—å®Œäº† ({len(speed_dict)}é ­åˆ†)")
                        else:
                            status_area.warning("âš ï¸ netkeiba æŒ‡æ•°ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆ400/åˆ¶é™/è¦ãƒ­ã‚°ã‚¤ãƒ³ã®å¯èƒ½æ€§ï¼‰")
                    except Exception as e:
                        print("netkeiba speed fetch error:", e)
                        status_area.warning(f"âš ï¸ netkeiba æŒ‡æ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        speed_dict = {}

                # netkeiba æˆ¦ç¸¾ï¼ˆé¦¬æŸ±ï¼‰
                past5_dict = {}
                if netkeiba_race_id:
                    try:
                        status_area.info("ğŸ“ netkeiba æˆ¦ç¸¾ã‚’å–å¾—ä¸­...")
                        past5_dict = fetch_netkeiba_past5_dict(driver, netkeiba_race_id)
                        if past5_dict:
                            status_area.success(f"âœ… netkeiba æˆ¦ç¸¾å–å¾—å®Œäº† ({len(past5_dict)}é ­åˆ†)")
                        else:
                            status_area.warning("âš ï¸ netkeiba æˆ¦ç¸¾ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆ400/åˆ¶é™/è¦ãƒ­ã‚°ã‚¤ãƒ³ã®å¯èƒ½æ€§ï¼‰")
                    except Exception as e:
                        print("netkeiba past5 fetch error:", e)
                        status_area.warning(f"âš ï¸ netkeiba æˆ¦ç¸¾å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        past5_dict = {}

                # çµåˆ
                merged = []
                umaban_list = (
                    sorted(syutuba_dict.keys(), key=lambda x: int(x))
                    if syutuba_dict
                    else sorted(
                        list(set(danwa_dict.keys()) | set(zenkoso_dict.keys()) | set(cyokyo_dict.keys())),
                        key=lambda x: int(x) if str(x).isdigit() else 999
                    )
                )

                for umaban in umaban_list:
                    sb = syutuba_dict.get(umaban, {})
                    bamei = (sb.get("bamei") or "").strip() or "åç§°ä¸æ˜"

                    kisyu_raw = (sb.get("kisyu") or "").strip()
                    kisyu_change = bool(sb.get("kisyu_change"))
                    if kisyu_raw:
                        kisyu = f"æ›¿ãƒ»{kisyu_raw}" if kisyu_change else kisyu_raw
                    else:
                        kisyu = "(é¨æ‰‹ä¸æ˜)"

                    d_comment = danwa_dict.get(umaban)
                    if not d_comment:
                        alt = _find_by_name_key(danwa_dict, bamei)
                        d_comment = alt if isinstance(alt, str) else None
                    if not d_comment:
                        d_comment = "(æƒ…å ±ãªã—)"

                    z_data = zenkoso_dict.get(umaban)
                    if not z_data:
                        alt = _find_by_name_key(zenkoso_dict, bamei)
                        z_data = alt if isinstance(alt, dict) else None
                    z_data = z_data or {}

                    z_prev_info = ""
                    z_comment = ""
                    if z_data:
                        z_prev_info = f"{z_data.get('prev_date_course','')} {z_data.get('prev_class','')} {z_data.get('prev_finish','')}".strip()
                        z_comment = (z_data.get("prev_comment") or "").strip()

                    if z_prev_info or z_comment:
                        prev_block = (
                            f"  ã€å‰èµ°æƒ…å ±ã€‘ {z_prev_info or '(æƒ…å ±ãªã—)'}\n"
                            f"  ã€å‰èµ°è«‡è©±ã€‘ {z_comment or '(ç„¡ã—)'}\n"
                        )
                    else:
                        prev_block = "  ã€å‰èµ°ã€‘ æ–°é¦¬(å‰èµ°æƒ…å ±ãªã—)\n"

                    c = cyokyo_dict.get(umaban)
                    if not c:
                        c = _find_by_name_key(cyokyo_dict, bamei)
                    c = c or {}
                    c_tanpyo = (c.get("tanpyo") or "").strip()
                    c_detail = (c.get("detail") or "").strip()

                    if c_tanpyo or c_detail:
                        cyokyo_block = f"  ã€èª¿æ•™ã€‘ çŸ­è©•:{c_tanpyo or '(ãªã—)'} / è©³ç´°:{c_detail or '(ãªã—)'}\n"
                    else:
                        cyokyo_block = "  ã€èª¿æ•™ã€‘ (æƒ…å ±ãªã—)\n"

                    sp = speed_dict.get(umaban, {}) if isinstance(speed_dict, dict) else {}
                    idx1 = normalize_netkeiba_index_cell(sp.get("index1", "ç„¡"))
                    idx2 = normalize_netkeiba_index_cell(sp.get("index2", "ç„¡"))
                    idx3 = normalize_netkeiba_index_cell(sp.get("index3", "ç„¡"))
                    course = normalize_netkeiba_index_cell(sp.get("course", "ç„¡"))
                    avg5 = normalize_netkeiba_index_cell(sp.get("avg5", "ç„¡"))
                    speed_line = f"  ã€æŒ‡æ•°ã€‘ å‰èµ°:{idx1}ã€2èµ°å‰:{idx2}ã€3èµ°å‰:{idx3}ã€ã‚³ãƒ¼ã‚¹æœ€é«˜:{course}ã€5èµ°å¹³å‡:{avg5}\n"

                    past_info = past5_dict.get(umaban, {}) if isinstance(past5_dict, dict) else {}
                    past3 = past_info.get("past3") or ["", "", ""]
                    while len(past3) < 3:
                        past3.append("")
                    past3 = past3[:3]

                    senreki_lines = []
                    labels = ["å‰èµ°", "2èµ°å‰", "3èµ°å‰"]
                    for label, record in zip(labels, past3):
                        if record:
                            senreki_lines.append(f"{label}:{record}")

                    if senreki_lines:
                        senreki_block = "  ã€æˆ¦ç¸¾ã€‘" + " ".join(senreki_lines) + "\n"
                    else:
                        senreki_block = "  ã€æˆ¦ç¸¾ã€‘ æ–°é¦¬(éå»èµ°ãªã—)\n"

                    text = (
                        f"â–¼[é¦¬ç•ª{umaban}] {bamei} / é¨æ‰‹:{kisyu}\n"
                        f"  ã€å©èˆã®è©±ã€‘ {d_comment}\n"
                        f"{prev_block}"
                        f"{cyokyo_block}"
                        f"{speed_line}"
                        f"{senreki_block}"
                    )
                    merged.append(text)

                if not merged:
                    status_area.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    st.write("---")
                    continue

                race_header_lines = []
                if race_info.get("date_meet"):
                    race_header_lines.append(race_info["date_meet"])
                if race_info.get("race_name"):
                    race_header_lines.append(race_info["race_name"])
                if race_info.get("cond1"):
                    race_header_lines.append(race_info["cond1"])
                if race_info.get("course_line"):
                    race_header_lines.append(race_info["course_line"])
                race_header = "\n".join(race_header_lines)

                merged_text = "\n".join(merged)

                full_text = (
                    "â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n"
                    f"{race_header}\n\n"
                    f"ä»¥ä¸‹ã¯{place_name}{r}Rã®å…¨é ­ãƒ‡ãƒ¼ã‚¿ã€‚\n"
                    "â– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿\n"
                    + merged_text
                )

                status_area.info("ğŸ¤– AIãŒåˆ†æãƒ»åŸ·ç­†ä¸­ã§ã™...")

                with result_container:
                    result_area = st.empty()
                    for chunk in stream_dify_workflow(full_text):
                        if chunk:
                            full_answer += chunk
                            result_area.markdown(full_answer + "â–Œ")
                    result_area.markdown(full_answer)

                if full_answer.strip():
                    status_area.success("âœ… åˆ†æå®Œäº†")
                    save_history(YEAR, KAI, PLACE, place_name, DAY, race_num, race_id, full_answer)

                    with st.expander("ğŸ“‹ ã“ã®ãƒ¬ãƒ¼ã‚¹ã®å‡ºåŠ›ã‚’ã‚³ãƒ”ãƒ¼/ä¿å­˜", expanded=False):
                        dom_id = f"copy_race_{race_id}_{int(time.time()*1000)}"
                        render_copy_button(
                            text=full_answer.strip(),
                            label=f"ğŸ“‹ {place_name}{r}R ã‚’ã‚³ãƒ”ãƒ¼(ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯)",
                            dom_id=dom_id,
                        )
                        st.download_button(
                            label=f"â¬‡ï¸ {place_name}{r}R ã‚’txtä¿å­˜",
                            data=full_answer.strip(),
                            file_name=f"{YEAR}{KAI}{PLACE}{DAY}_{place_name}_{r}R.txt",
                            mime="text/plain",
                            key=f"dl_race_{race_id}",
                        )

                    combined_blocks.append(f"ã€{place_name} {r}Rã€‘\n{full_answer.strip()}\n")
                else:
                    status_area.error("âš ï¸ AIã‹ã‚‰ã®å›ç­”ãŒç©ºã§ã—ãŸã€‚")

            except Exception as e:
                err_msg = f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ ({place_name} {r}R): {str(e)}"
                print(err_msg)
                import traceback
                traceback.print_exc()
                status_area.error(err_msg)

            st.write("---")

        if combined_blocks:
            combined_text = "\n".join(combined_blocks).strip()
            st.session_state["combined_output"] = combined_text

            st.subheader("ğŸ“Œ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚(è¦æ±‚ã—ãŸãƒ¬ãƒ¼ã‚¹ã‚’å…¨éƒ¨ã¾ã¨ã‚ã¦ã‚³ãƒ”ãƒ¼)")

            dom_id_all = f"copy_all_{base_id}_{int(time.time()*1000)}"
            render_copy_button(
                text=combined_text,
                label="ğŸ“‹ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ã‚’ã‚³ãƒ”ãƒ¼(ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯)",
                dom_id=dom_id_all,
            )

            st.download_button(
                label="â¬‡ï¸ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ã‚’txtä¿å­˜",
                data=combined_text,
                file_name=f"{YEAR}{KAI}{PLACE}{DAY}_{place_name}_ALL.txt",
                mime="text/plain",
                key=f"dl_all_{base_id}",
            )

            with st.expander("ğŸ‘€ ã¾ã¨ã‚è¡¨ç¤º(é–²è¦§ç”¨)", expanded=False):
                st.text_area(
                    "å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ãƒ†ã‚­ã‚¹ãƒˆ",
                    value=combined_text,
                    height=420,
                    key=f"ta_all_{base_id}",
                )
        else:
            st.info("ã¾ã¨ã‚å¯¾è±¡ã®å‡ºåŠ›ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    finally:
        try:
            driver.quit()
        except Exception:
            pass
