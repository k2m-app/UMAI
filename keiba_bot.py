import time
import json
import re
import requests
import streamlit as st
import streamlit.components.v1 as components
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# â˜…è¿½åŠ :netkeiba
NETKEIBA_ID = st.secrets.get("NETKEIBA_ID", "")
NETKEIBA_PASS = st.secrets.get("NETKEIBA_PASS", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š(app.py å´ã§ set_race_params ãŒå‘¼ã°ã‚Œã‚‹ã¨æ›¸ãæ›ã‚ã‚‹)
YEAR = "2025"
KAI = "04"
PLACE = "02"
DAY = "02"

BASE_URL = "https://s.keibabook.co.jp"

PLACE_NAMES = {
    "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰", "04": "æ±äº¬",
    "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ", "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
}

# â˜…è¿½åŠ :ç«¶é¦¬ãƒ–ãƒƒã‚¯ PLACEã‚³ãƒ¼ãƒ‰ â†’ netkeiba ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
# 01æœ­å¹Œ 02å‡½é¤¨ 03ç¦å³¶ 04æ–°æ½Ÿ 05æ±äº¬ 06ä¸­å±± 07ä¸­äº¬ 08äº¬éƒ½ 09é˜ªç¥ 10å°å€‰
KEIBABOOK_TO_NETKEIBA_PLACE = {
    "08": "01",  # æœ­å¹Œ
    "09": "02",  # å‡½é¤¨
    "06": "03",  # ç¦å³¶
    "07": "04",  # æ–°æ½Ÿ
    "04": "05",  # æ±äº¬
    "05": "06",  # ä¸­å±±
    "02": "07",  # ä¸­äº¬
    "00": "08",  # äº¬éƒ½
    "01": "09",  # é˜ªç¥
    "03": "10",  # å°å€‰
}

def set_race_params(year, kai, place, day):
    """app.py ã‹ã‚‰é–‹å‚¬æƒ…å ±ã‚’å·®ã—æ›¿ãˆã‚‹ãŸã‚ã®é–¢æ•°"""
    global YEAR, KAI, PLACE, DAY
    YEAR = str(year)
    KAI = str(kai).zfill(2)
    PLACE = str(place).zfill(2)
    DAY = str(day).zfill(2)

def get_current_params():
    """ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿(UIè¡¨ç¤ºç”¨)"""
    return YEAR, KAI, PLACE, DAY


# ==================================================
# â˜…ä¿®æ­£:netkeiba æŒ‡æ•°ã‚»ãƒ«æ­£è¦åŒ–(1000ã¯å¿…ãšã€Œç„¡ã€)
# ==================================================
def normalize_netkeiba_index_cell(raw: str) -> str:
    """
    ç›®çš„:
    - netkeibaæŒ‡æ•°ãŒã€Œæœªã€ã€Œ-ã€ã®ã¨ãå†…éƒ¨çš„ã«1000ã«ãªã‚‹ã‚±ãƒ¼ã‚¹ã‚’ã€Œç„¡ã€ã«çµ±ä¸€
    - "1070 70" ç­‰ã®æ··åœ¨ã‹ã‚‰æœ¬å‘½å€¤ã ã‘ã‚’æŠ½å‡º
    ãƒ«ãƒ¼ãƒ«:
    - "1000" ã¯ç„¡æ¡ä»¶ã§ã€Œç„¡ã€
    - "æœª" / "-" / "ï¼" / ç©º ã¯ã€Œç„¡ã€
    - æ•°å­—ã¯ 3æ¡ä»¥ä¸‹ã‚’å„ªå…ˆã—ã¦æœ«å°¾ã‚’æ¡ç”¨(70, 54, 107ãªã©)
    - ãã‚Œã‚‚ç„¡ã‘ã‚Œã°ã€Œç„¡ã€
    """
    if raw is None:
        return "ç„¡"

    t = str(raw).replace("\xa0", " ").strip()
    if t == "":
        return "ç„¡"

    # æ˜ç¤ºçš„ãªæœª/æ¬ æ
    if "æœª" in t:
        return "ç„¡"
    if "ï¼" in t or "-" in t:
        return "ç„¡"

    # æ•°å­—æŠ½å‡º
    nums = re.findall(r"\d+", t)
    if not nums:
        return "ç„¡"

    # 1000ãŒå«ã¾ã‚Œã‚‹(ã¾ãŸã¯å˜ä½“)ãªã‚‰ã€Œç„¡ã€
    if any(n == "1000" for n in nums):
        # ãŸã ã— "1070 70" ã®ã‚ˆã†ã«1000ä»¥å¤–ã®æœ‰åŠ¹å€¤ãŒã‚ã‚‹å ´åˆã¯æœ‰åŠ¹å€¤ã‚’å„ªå…ˆ
        short = [n for n in nums if len(n) <= 3 and n != "1000"]
        return short[-1] if short else "ç„¡"

    # 3æ¡ä»¥ä¸‹å„ªå…ˆ(å¤šãã®å ´åˆã€ã“ã“ãŒæœ¬å‘½ã®æŒ‡æ•°)
    short = [n for n in nums if len(n) <= 3]
    if short:
        return short[-1]

    return "ç„¡"


# ==================================================
# ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚³ãƒ”ãƒ¼(components.html + clipboard)
# ==================================================
def render_copy_button(text: str, label: str, dom_id: str):
    safe_text = json.dumps(text)
    html = f"""
    <div style="display:flex; gap:8px; align-items:center; flex-wrap:wrap;">
      <button id="{dom_id}" style="
        padding:8px 12px;
        border-radius:10px;
        border:1px solid #ddd;
        background:#fff;
        cursor:pointer;
        font-size:14px;
      ">{label}</button>
      <span id="{dom_id}-msg" style="font-size:12px; color:#666;"></span>
    </div>
    <script>
      (function() {{
        const btn = document.getElementById("{dom_id}");
        const msg = document.getElementById("{dom_id}-msg");
        if (!btn) return;

        btn.addEventListener("click", async () => {{
          try {{
            await navigator.clipboard.writeText({safe_text});
            msg.textContent = "ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";
            setTimeout(() => msg.textContent = "", 1200);
          }} catch (e) {{
            msg.textContent = "ã‚³ãƒ”ãƒ¼ã«å¤±æ•—(ãƒ–ãƒ©ã‚¦ã‚¶åˆ¶é™ã®å¯èƒ½æ€§)";
            setTimeout(() => msg.textContent = "", 2200);
          }}
        }});
      }})();
    </script>
    """
    components.html(html, height=54)


# ==================================================
# Supabase
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client | None:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)

def save_history(
    year: str,
    kai: str,
    place_code: str,
    place_name: str,
    day: str,
    race_num_str: str,
    race_id: str,
    ai_answer: str,
) -> None:
    supabase = get_supabase_client()
    if supabase is None:
        return

    data = {
        "year": str(year),
        "kai": str(kai),
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }

    try:
        supabase.table("history").insert(data).execute()
    except Exception as e:
        print("Supabase insert error:", e)


# ==================================================
# Selenium
# ==================================================
def build_driver() -> webdriver.Chrome:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,2200")
    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(60)
    return driver


def login_keibabook(driver: webdriver.Chrome) -> None:
    if not KEIBA_ID or not KEIBA_PASS:
        raise RuntimeError("KEIBA_ID / KEIBA_PASS ãŒ secrets ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    driver.get(f"{BASE_URL}/login/login")

    WebDriverWait(driver, 15).until(
        EC.visibility_of_element_located((By.NAME, "login_id"))
    ).send_keys(KEIBA_ID)

    WebDriverWait(driver, 15).until(
        EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
    ).send_keys(KEIBA_PASS)

    WebDriverWait(driver, 15).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))
    ).click()

    time.sleep(1.2)


# â˜…è¿½åŠ :netkeibaãƒ­ã‚°ã‚¤ãƒ³(å¿…è¦ãªã¨ãã ã‘)
def login_netkeiba(driver: webdriver.Chrome) -> bool:
    """
    æˆåŠŸã—ãŸã‚‰ Trueã€å¤±æ•—/æœªè¨­å®šãªã‚‰ False
    """
    if not NETKEIBA_ID or not NETKEIBA_PASS:
        return False

    try:
        driver.get("https://regist.netkeiba.com/?pid=stage_login")
        time.sleep(0.8)

        id_candidates = [
            (By.NAME, "login_id"),
            (By.NAME, "userid"),
            (By.NAME, "id"),
            (By.CSS_SELECTOR, "input[type='text']"),
        ]
        pass_candidates = [
            (By.NAME, "pswd"),
            (By.NAME, "password"),
            (By.CSS_SELECTOR, "input[type='password']"),
        ]

        id_el = None
        for how, sel in id_candidates:
            try:
                id_el = WebDriverWait(driver, 5).until(
                    EC.visibility_of_element_located((how, sel))
                )
                if id_el:
                    break
            except Exception:
                continue

        pw_el = None
        for how, sel in pass_candidates:
            try:
                pw_el = WebDriverWait(driver, 5).until(
                    EC.visibility_of_element_located((how, sel))
                )
                if pw_el:
                    break
            except Exception:
                continue

        if not id_el or not pw_el:
            return False

        id_el.clear()
        id_el.send_keys(NETKEIBA_ID)
        pw_el.clear()
        pw_el.send_keys(NETKEIBA_PASS)

        btn_candidates = [
            (By.CSS_SELECTOR, "input[type='submit']"),
            (By.CSS_SELECTOR, "button[type='submit']"),
            (By.CSS_SELECTOR, ".Btn_Login, .btn_login, .btn"),
        ]
        clicked = False
        for how, sel in btn_candidates:
            try:
                btn = WebDriverWait(driver, 5).until(
                    EC.element_to_be_clickable((how, sel))
                )
                btn.click()
                clicked = True
                break
            except Exception:
                continue

        if not clicked:
            return False

        time.sleep(1.2)

        html = driver.page_source
        if "ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ" in html or "action=logout" in html:
            return True

        return False

    except Exception:
        return False


# ==================================================
# Parser:å…±é€š(ç«¶é¦¬ãƒ–ãƒƒã‚¯)
# ==================================================
def parse_race_info(html: str):
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"date_meet": "", "race_name": "", "cond1": "", "course_line": ""}

    racemei = racetitle.find("div", class_="racemei")
    date_meet = ""
    race_name = ""
    if racemei:
        ps = racemei.find_all("p")
        if len(ps) >= 1:
            date_meet = ps[0].get_text(strip=True)
        if len(ps) >= 2:
            race_name = ps[1].get_text(strip=True)

    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    cond1 = ""
    course_line = ""
    if racetitle_sub:
        sub_ps = racetitle_sub.find_all("p")
        if len(sub_ps) >= 1:
            cond1 = sub_ps[0].get_text(strip=True)
        if len(sub_ps) >= 2:
            course_line = sub_ps[1].get_text(" ", strip=True)

    return {
        "date_meet": date_meet,
        "race_name": race_name,
        "cond1": cond1,
        "course_line": course_line,
    }


def parse_danwa_comments(html: str):
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table or not table.tbody:
        return {}

    danwa_dict = {}
    current_key = None

    for row in table.tbody.find_all("tr"):
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")

        if uma_td:
            text = re.sub(r"\D", "", uma_td.get_text(strip=True))
            if text:
                current_key = text
                continue

        if bamei_td and not current_key:
            text = bamei_td.get_text(strip=True)
            if text:
                current_key = text
                continue

        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current_key:
            danwa_dict[current_key] = danwa_td.get_text(strip=True)
            current_key = None

    return danwa_dict


def parse_zenkoso_interview(html: str):
    soup = BeautifulSoup(html, "html.parser")
    h2 = soup.find("h2", string=lambda s: s and "å‰èµ°" in s)
    if not h2:
        return {}

    table = h2.find_next("table", class_="syoin")
    if not table or not table.tbody:
        return {}

    rows = table.tbody.find_all("tr")
    result_dict = {}

    i = 0
    while i < len(rows):
        row = rows[i]
        if "spacer" in (row.get("class") or []):
            i += 1
            continue

        waku_td = row.find("td", class_="waku")
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")

        if not (waku_td and uma_td and bamei_td):
            i += 1
            continue

        waku = waku_td.get_text(strip=True)
        umaban = re.sub(r"\D", "", uma_td.get_text(strip=True))
        name = bamei_td.get_text(strip=True)

        prev_date = ""
        prev_class = ""
        prev_finish = ""
        prev_comment = ""

        detail = rows[i + 1] if i + 1 < len(rows) else None
        if detail:
            syoin_td = detail.find("td", class_="syoin")
            if syoin_td:
                sdata = syoin_td.find("div", class_="syoindata")
                if sdata:
                    ps = sdata.find_all("p")
                    if ps:
                        prev_date = ps[0].get_text(strip=True)
                    if len(ps) >= 2:
                        spans = ps[1].find_all("span")
                        if len(spans) >= 1:
                            prev_class = spans[0].get_text(strip=True)
                        if len(spans) >= 2:
                            prev_finish = spans[1].get_text(strip=True)

                direct = syoin_td.find_all("p", recursive=False)
                if direct:
                    txt = direct[0].get_text(strip=True)
                    if txt != "ï¼":
                        prev_comment = txt

        if umaban:
            result_dict[umaban] = {
                "waku": waku,
                "umaban": umaban,
                "name": name,
                "prev_date_course": prev_date,
                "prev_class": prev_class,
                "prev_finish": prev_finish,
                "prev_comment": prev_comment,
            }

        i += 2

    return result_dict


def parse_cyokyo(html: str):
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}

    section = None
    h2 = soup.find("h2", string=lambda s: s and ("èª¿æ•™" in s or "ä¸­é–“" in s))
    if h2:
        midasi_div = h2.find_parent("div", class_="midasi")
        if midasi_div:
            section = midasi_div.find_next_sibling("div", class_="section")
    if section is None:
        section = soup

    tables = section.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody:
            continue
        rows = tbody.find_all("tr", recursive=False)
        if len(rows) < 1:
            continue

        header = rows[0]
        uma_td = header.find("td", class_="umaban")
        name_td = header.find("td", class_="kbamei")

        umaban_text = uma_td.get_text(strip=True) if uma_td else ""
        umaban = re.sub(r"\D", "", umaban_text)

        bamei_hint = ""
        if name_td:
            bamei_hint = name_td.get_text(" ", strip=True)

        tanpyo_td = header.find("td", class_="tanpyo")
        tanpyo = tanpyo_td.get_text(strip=True) if tanpyo_td else ""

        detail_row = rows[1] if len(rows) >= 2 else None
        detail_text = detail_row.get_text(" ", strip=True) if detail_row else ""

        payload = {"tanpyo": tanpyo, "detail": detail_text, "bamei_hint": bamei_hint}

        if umaban:
            cyokyo_dict[umaban] = payload
        else:
            if bamei_hint:
                cyokyo_dict[bamei_hint] = payload

    return cyokyo_dict


def parse_syutuba(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")

    table = soup.find("table", class_=lambda c: c and "syutuba_sp" in c.split())
    if not table:
        table = soup.find("table", class_=lambda c: c and "syutuba" in c)

    if not table or not table.tbody:
        return {}

    result = {}
    for tr in table.tbody.find_all("tr", recursive=False):
        tds = tr.find_all("td", recursive=False)
        if not tds:
            continue

        umaban_raw = tds[0].get_text(strip=True)
        umaban = re.sub(r"\D", "", umaban_raw)
        if not umaban:
            continue

        bamei = ""
        kbamei_p = tr.find("p", class_="kbamei")
        if kbamei_p:
            bamei = kbamei_p.get_text(" ", strip=True)

        kisyu = ""
        kisyu_change = False

        kisyu_p = tr.find("p", class_="kisyu")
        if kisyu_p:
            a = kisyu_p.find("a")
            if a:
                norika = a.find("span", class_="norikawari")
                if norika:
                    kisyu_change = True
                    kisyu = norika.get_text(strip=True)
                else:
                    kisyu = a.get_text(strip=True)
            else:
                norika = kisyu_p.find("span", class_="norikawari")
                if norika:
                    kisyu_change = True
                    kisyu = norika.get_text(strip=True)
                else:
                    kisyu = kisyu_p.get_text(" ", strip=True)

        result[umaban] = {
            "umaban": umaban,
            "bamei": bamei,
            "kisyu": kisyu,
            "kisyu_change": kisyu_change,
        }

    return result


# ==================================================
# â˜…ä¿®æ­£å¼·åŒ–:netkeiba ã‚¿ã‚¤ãƒ æŒ‡æ•° parser(1000â†’ç„¡ ã‚’é©ç”¨)
# ==================================================
def parse_netkeiba_speed_index(html: str) -> dict:
    """
    netkeiba speed.html ã®å‡ºé¦¬è¡¨ã‹ã‚‰æŒ‡æ•°ã‚’æŠœãã€‚
    HTMLæ§‹é€ :
    <td class="cellcolor_ sk__index1">
        <span class="Sort_Function_Data_Hidden">1089</span>
        <a href="...">89</a>  â† ã“ã®89ã‚’å–å¾—
    </td>
    ã¾ãŸã¯
    <td class="cellcolor_ sk__average_index">
        <span class="Sort_Function_Data_Hidden">1086</span>
        86*  â† ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    </td>
    
    æˆ»ã‚Šå€¤:{ "1": {"index1":"89","index2":"77","index3":"94","course":"ç„¡","avg5":"86"}, ... }
    """
    soup = BeautifulSoup(html, "html.parser")

    table = soup.find("table", class_=lambda c: c and ("SpeedIndex_Table" in c))
    if not table or not table.tbody:
        return {}

    out = {}

    for tr in table.tbody.find_all("tr", class_=lambda c: c and ("HorseList" in c.split()), recursive=False):
        um_td = tr.find("td", class_=lambda c: c and "sk__umaban" in c)
        if not um_td:
            continue
        umaban = re.sub(r"\D", "", um_td.get_text(" ", strip=True))
        if not umaban:
            continue

        def cell_text(cell_class: str) -> str:
            """
            æŒ‡å®šã•ã‚ŒãŸã‚¯ãƒ©ã‚¹åã®tdã‚»ãƒ«ã‚’æ¢ã—ã€Sort_Function_Data_Hiddenã®å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            """
            td = tr.find("td", class_=lambda c: c and cell_class in c.split())
            if not td:
                return "ç„¡"
            
            # Sort_Function_Data_Hiddenã‚¹ãƒ‘ãƒ³ã‚’è¦‹ã¤ã‘ã‚‹
            hidden_span = td.find("span", class_="Sort_Function_Data_Hidden")
            if hidden_span:
                # éš ã—ã‚¹ãƒ‘ãƒ³ã®å¾Œã«aã‚¿ã‚°ãŒã‚ã‚‹ã‹ç¢ºèª
                a_tag = td.find("a")
                if a_tag:
                    # aã‚¿ã‚°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                    txt = a_tag.get_text(strip=True)
                else:
                    # aã‚¿ã‚°ãŒãªã„å ´åˆã€éš ã—ã‚¹ãƒ‘ãƒ³ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
                    hidden_span.decompose()
                    txt = td.get_text(strip=True)
            else:
                # éš ã—ã‚¹ãƒ‘ãƒ³ãŒãªã„å ´åˆã¯æ™®é€šã«ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
                txt = td.get_text(strip=True)
            
            # æ­£è¦åŒ–ã—ã¦è¿”ã™
            return normalize_netkeiba_index_cell(txt)

        out[umaban] = {
            "index1": cell_text("sk__index1"),             # å‰èµ°
            "index2": cell_text("sk__index2"),             # 2èµ°å‰
            "index3": cell_text("sk__index3"),             # 3èµ°å‰
            "course": cell_text("sk__max_course_index"),   # ã‚³ãƒ¼ã‚¹æœ€é«˜
            "avg5":   cell_text("sk__average_index"),      # 5èµ°å¹³å‡
        }

    return out


def fetch_netkeiba_speed_dict(driver: webdriver.Chrome, netkeiba_race_id: str) -> dict:
    """
    netkeiba speed.html ã‚’é–‹ã„ã¦æŒ‡æ•°è¾æ›¸ã‚’è¿”ã™
    """
    url = f"https://race.netkeiba.com/race/speed.html?race_id={netkeiba_race_id}&type=shutuba&mode=default"
    driver.get(url)

    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.SpeedIndex_Table"))
        )
    except Exception:
        pass

    time.sleep(1.0)  # JavaScript ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾…ã¡

    html = driver.page_source

    # ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ãã†ãªã‚‰ 1å›ã ã‘ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦å†å–å¾—
    if ("ç„¡æ–™ä¼šå“¡ç™»éŒ²" in html or "ãƒ­ã‚°ã‚¤ãƒ³" in html) and NETKEIBA_ID and NETKEIBA_PASS:
        ok = login_netkeiba(driver)
        if ok:
            driver.get(url)
            time.sleep(1.0)
            html = driver.page_source

    return parse_netkeiba_speed_index(html)


def keibabook_race_id_to_netkeiba_race_id(year: str, kai: str, place: str, day: str, race_num_2: str) -> str:
    """
    netkeiba race_id = YYYY + (netkeibaå ´ã‚³ãƒ¼ãƒ‰2æ¡) + å›2æ¡ + æ—¥2æ¡ + R2æ¡
    """
    nk_place = KEIBABOOK_TO_NETKEIBA_PLACE.get(place)
    if not nk_place:
        return ""
    return f"{str(year)}{nk_place}{str(kai).zfill(2)}{str(day).zfill(2)}{str(race_num_2).zfill(2)}"


# ==================================================
# â˜…ä¿®æ­£:netkeiba é¦¬æŸ±(5èµ°) ã‹ã‚‰ã€Œæˆ¦ç¸¾ã€ã‚’æ­£ç¢ºã«æŠ½å‡º
# å®Ÿéš›ã®HTMLæ§‹é€ ã«åŸºã¥ã„ã¦å®Ÿè£…
# ==================================================
def _extract_race_result_from_past_td(past_td) -> str:
    """
    td.Past ã®ä¸­èº«ã‹ã‚‰æˆ¦ç¸¾ã‚’æŒ‡å®šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æŠ½å‡º:
    ä¾‹: 2025.11.02 äº¬éƒ½ å¤éƒ½S(3å‹) èŠ3000(å¤–) 13é ­4ç•ª2äºº å²©ç”°æœ›æ¥ 55.0â†’1-1-1-1 ...æœ€çµ‚9ç€
    
    å®Ÿéš›ã®HTMLæ§‹é€ :
    <td class="Past">
      <div class="Data_Item">
        <div class="Data01"><span>2025.11.02 äº¬éƒ½</span><span class="Num">9</span></div>
        <div class="Data02"><a href="...">å¤éƒ½S<span class="Icon_GradeType...">3å‹</span></a></div>
        <div class="Data05">èŠ3000(å¤–) 3:05.5 <strong>è‰¯</strong></div>
        <div class="Data03">13é ­ 4ç•ª 2äºº å²©ç”°æœ›æ¥ 55.0</div>
        <div class="Data06">1-1-1-1 (36.4) 514(-12)</div>
        <div class="Data07"><a href="...">ãƒ´ã‚©ãƒ©ãƒ³ãƒ†</a>(1.7)</div>
      </div>
    </td>
    """
    if past_td is None:
        return ""
    
    # ç©ºã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
    cell_text = past_td.get_text(" ", strip=True)
    if not cell_text or len(cell_text) < 5:
        return ""
    
    # Data_Item ã‚’æ¢ã™(å­˜åœ¨ã—ãªã„å ´åˆã¯ç›´æ¥æ¢ã™)
    data_item = past_td.find("div", class_="Data_Item")
    if data_item:
        container = data_item
    else:
        container = past_td
    
    # ===== Data01: æ—¥ä»˜ãƒ»ç«¶é¦¬å ´ãƒ»ç€é † =====
    data01 = container.find("div", class_="Data01")
    date_str = ""
    place_str = ""
    rank_num = ""
    
    if data01:
        spans = data01.find_all("span")
        if spans:
            # æœ€åˆã®span: "2025.11.02 äº¬éƒ½" ã¾ãŸã¯ "2025.11.02ã€€äº¬éƒ½"
            first_span_text = spans[0].get_text(" ", strip=True)
            # æ—¥ä»˜ã¨ç«¶é¦¬å ´ã‚’åˆ†é›¢
            date_match = re.search(r"(\d{4}[./]\d{1,2}[./]\d{1,2})", first_span_text)
            if date_match:
                raw_date = date_match.group(1)
                date_str = raw_date.replace("/", ".")
            
            # ç«¶é¦¬å ´å(æ—¥ä»˜ä»¥é™ã®æ–‡å­—åˆ—)
            place_match = re.search(r"\d{4}[./]\d{1,2}[./]\d{1,2}\s*(.+)", first_span_text)
            if place_match:
                place_str = place_match.group(1).strip()
                # "äº¬éƒ½10" ã®ã‚ˆã†ãªå ´åˆã€æ•°å­—ã‚’é™¤å»
                place_str = re.sub(r"\d+$", "", place_str).strip()
        
        # ç€é †(class="Num" ã®span)
        num_span = data01.find("span", class_="Num")
        if num_span:
            rank_num = num_span.get_text(strip=True)
            # "ä¸­" ã¯ç«¶èµ°ä¸­æ­¢
            if rank_num and rank_num not in ["ä¸­", "å–", "é™¤"]:
                rank_num = re.sub(r"\D", "", rank_num)
    
    # ===== Data02: ãƒ¬ãƒ¼ã‚¹åãƒ»ã‚¯ãƒ©ã‚¹ =====
    data02 = container.find("div", class_="Data02")
    race_name = ""
    class_str = ""
    
    if data02:
        # ãƒ¬ãƒ¼ã‚¹å(aã‚¿ã‚°ã®ãƒ†ã‚­ã‚¹ãƒˆã€ãŸã ã—spanã‚’é™¤ã)
        a_tag = data02.find("a")
        if a_tag:
            # spanã‚’ä¸€æ™‚çš„ã«é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
            for span in a_tag.find_all("span"):
                span_text = span.get_text(strip=True)
                if span_text:
                    class_str = span_text
            race_name = a_tag.get_text(strip=True)
            # ã‚¯ãƒ©ã‚¹æ–‡å­—åˆ—ã‚’é™¤å»
            if class_str and class_str in race_name:
                race_name = race_name.replace(class_str, "").strip()
        else:
            race_name = data02.get_text(strip=True)
        
        # Icon_GradeType ã‹ã‚‰ã‚¯ãƒ©ã‚¹ã‚’å–å¾—
        grade_span = data02.find("span", class_=lambda c: c and "Icon_GradeType" in str(c))
        if grade_span:
            class_str = grade_span.get_text(strip=True)
    
    # ===== Data05: ã‚³ãƒ¼ã‚¹æƒ…å ±(èŠ/ãƒ€ã€è·é›¢ã€ã‚¿ã‚¤ãƒ ) =====
    data05 = container.find("div", class_="Data05")
    course_str = ""
    
    if data05:
        text = data05.get_text(" ", strip=True)
        # "èŠ3000(å¤–) 3:05.5 è‰¯" ã®ã‚ˆã†ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        # ã‚³ãƒ¼ã‚¹éƒ¨åˆ†ã ã‘æŠ½å‡º(èŠ/ãƒ€ + è·é›¢ + (å†…/å¤–))
        course_match = re.match(r"(èŠ|ãƒ€)\d+(\([å†…å¤–]\))?", text)
        if course_match:
            course_str = course_match.group(0)
    
    # ===== Data03: é ­æ•°ãƒ»é¦¬ç•ªãƒ»äººæ°—ãƒ»é¨æ‰‹ãƒ»æ–¤é‡ =====
    data03 = container.find("div", class_="Data03")
    field_info = ""
    jockey = ""
    weight = ""
    
    if data03:
        text = data03.get_text(" ", strip=True)
        # "13é ­ 4ç•ª 2äºº å²©ç”°æœ›æ¥ 55.0" ã®ã‚ˆã†ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        
        # é ­æ•°ãƒ»é¦¬ç•ªãƒ»äººæ°—
        head_match = re.search(r"(\d+)é ­", text)
        num_match = re.search(r"(\d+)ç•ª", text)
        pop_match = re.search(r"(\d+)äºº", text)
        
        parts = []
        if head_match:
            parts.append(f"{head_match.group(1)}é ­")
        if num_match:
            parts.append(f"{num_match.group(1)}ç•ª")
        if pop_match:
            parts.append(f"{pop_match.group(1)}äºº")
        field_info = "".join(parts)
        
        # æ–¤é‡(æ•°å­—.æ•°å­— ã®ãƒ‘ã‚¿ãƒ¼ãƒ³)
        weight_match = re.search(r"(\d+\.\d+|\d+\.0)", text)
        if weight_match:
            weight = weight_match.group(1)
        
        # é¨æ‰‹å(äººæ°—ã®å¾Œã€æ–¤é‡ã®å‰)
        # "2äºº å²©ç”°æœ›æ¥ 55.0" â†’ "å²©ç”°æœ›æ¥"
        jockey_match = re.search(r"\d+äºº\s+([^\d\s]+(?:\s+[^\d\s]+)?)\s+\d", text)
        if jockey_match:
            jockey = jockey_match.group(1).strip()
        else:
            # åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³: äººæ°—ãŒãªã„å ´åˆ
            jockey_match2 = re.search(r"ç•ª\s+([^\d]+)\s+\d+\.", text)
            if jockey_match2:
                jockey = jockey_match2.group(1).strip()
    
    # ===== Data06: é€šéé †ä½ =====
    data06 = container.find("div", class_="Data06")
    passing = ""
    
    if data06:
        text = data06.get_text(" ", strip=True)
        # "1-1-1-1 (36.4) 514(-12)" ã®ã‚ˆã†ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        # é€šéé †ä½ãƒ‘ã‚¿ãƒ¼ãƒ³(ãƒã‚¤ãƒ•ãƒ³åŒºåˆ‡ã‚Šã®æ•°å­—åˆ—)
        passing_match = re.match(r"([\d]+-[\d]+(?:-[\d]+)*)", text)
        if passing_match:
            passing = passing_match.group(1)
    
    # ===== çµæœæ–‡å­—åˆ—ã‚’çµ„ã¿ç«‹ã¦ =====
    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: 2025.11.02 äº¬éƒ½ å¤éƒ½S(3å‹) èŠ3000(å¤–) 13é ­4ç•ª2äºº å²©ç”°æœ›æ¥ 55.0â†’1-1-1-1 ...æœ€çµ‚9ç€
    
    result_parts = []
    
    if date_str:
        result_parts.append(date_str)
    
    if place_str:
        result_parts.append(place_str)
    
    # ãƒ¬ãƒ¼ã‚¹å(ã‚¯ãƒ©ã‚¹)
    if race_name and class_str:
        result_parts.append(f"{race_name}({class_str})")
    elif race_name:
        result_parts.append(race_name)
    
    if course_str:
        result_parts.append(course_str)
    
    if field_info:
        result_parts.append(field_info)
    
    # é¨æ‰‹ æ–¤é‡
    if jockey and weight:
        result_parts.append(f"{jockey} {weight}")
    elif jockey:
        result_parts.append(jockey)
    
    # é€šéé †â†’ç€é †
    if passing:
        result_parts.append(f"â†’{passing}")
    
    if rank_num:
        result_parts.append(f"...æœ€çµ‚{rank_num}ç€")
    
    if not result_parts:
        return ""
    
    return " ".join(result_parts)


def parse_netkeiba_shutuba_past5(html: str, take_last_n: int = 3) -> dict:
    """
    shutuba_past.html(é¦¬æŸ±5èµ°è¡¨ç¤º)ã‹ã‚‰æˆ¦ç¸¾ã‚’æŠœãã€‚
    
    å®Ÿéš›ã®HTMLæ§‹é€ :
    <table class="Shutuba_Table Shutuba_Past5_Table" id="sort_table">
      <tbody>
        <tr class="HorseList" id="tr_8">
          <td class="Waku1">1</td>      <!-- æ ç•ª -->
          <td class="Waku">1</td>       <!-- é¦¬ç•ª -->
          <td class="Horse_Select">...</td>
          <td class="Horse_Info">...</td>
          <td class="Jockey">...</td>
          <td class="Past">...</td>     <!-- å‰èµ° -->
          <td class="Past">...</td>     <!-- 2èµ°å‰ -->
          <td class="Past">...</td>     <!-- 3èµ°å‰ -->
          ...
        </tr>
      </tbody>
    </table>

    æˆ»ã‚Šå€¤:
      {
        "1": {"past3": [str, str, str]},
        "2": {"past3": [str, str, str]},
        ...
      }
    
    past3[0] = å‰èµ°, past3[1] = 2èµ°å‰, past3[2] = 3èµ°å‰
    """
    soup = BeautifulSoup(html, "html.parser")

    # ãƒ†ãƒ¼ãƒ–ãƒ«æ¤œç´¢
    table = soup.find("table", id="sort_table")
    if not table:
        table = soup.find("table", class_=lambda c: c and "Shutuba_Past5_Table" in str(c))
    if not table:
        table = soup.find("table", class_=lambda c: c and "Shutuba_Table" in str(c))
    
    if not table:
        return {}
    
    tbody = table.find("tbody")
    if not tbody:
        tbody = table

    out = {}

    # å„é¦¬ã®è¡Œã‚’å–å¾—
    rows = tbody.find_all("tr", class_=lambda c: c and "HorseList" in str(c))
    
    for tr in rows:
        # ===== é¦¬ç•ªã‚’å–å¾— =====
        # æ ç•ªã¨é¦¬ç•ªã®2ã¤ã®td.WakuãŒã‚ã‚‹
        # æ ç•ª: class="Waku1" ãªã© (Waku + æ•°å­—)
        # é¦¬ç•ª: class="Waku" ã®ã¿
        umaban = ""
        
        waku_tds = tr.find_all("td", class_=lambda c: c and "Waku" in str(c))
        for td in waku_tds:
            td_class = td.get("class") or []
            # class ãŒ ["Waku"] ã®ã¿ã®ã‚‚ã®ãŒé¦¬ç•ª
            if td_class == ["Waku"]:
                umaban = re.sub(r"\D", "", td.get_text(strip=True))
                break
        
        # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯2ç•ªç›®ã®Wakuç³»tdã‚’ä½¿ç”¨
        if not umaban and len(waku_tds) >= 2:
            umaban = re.sub(r"\D", "", waku_tds[1].get_text(strip=True))
        
        if not umaban:
            continue

        # ===== éå»èµ°(Past)ã‚’å–å¾— =====
        past_tds = tr.find_all("td", class_=lambda c: c and "Past" in str(c))
        
        # Rest(ä¼‘ã¿æ˜ã‘æƒ…å ±)ã‚’é™¤å¤–
        past_tds = [td for td in past_tds if "Rest" not in str(td.get("class", []))]
        
        past_summaries = []
        for td in past_tds[:take_last_n]:
            summary = _extract_race_result_from_past_td(td)
            past_summaries.append(summary)
        
        # ä¸è¶³åˆ†ã¯ç©ºæ–‡å­—ã§åŸ‹ã‚ã‚‹
        while len(past_summaries) < take_last_n:
            past_summaries.append("")
        
        out[umaban] = {
            "past3": past_summaries[:take_last_n],
        }

    return out


def fetch_netkeiba_past5_dict(driver: webdriver.Chrome, netkeiba_race_id: str) -> dict:
    """
    shutuba_past.html ã‚’é–‹ã„ã¦ã€æˆ¦ç¸¾æƒ…å ±ã‚’è¾æ›¸ã§è¿”ã™
    """
    url = f"https://race.netkeiba.com/race/shutuba_past.html?race_id={netkeiba_race_id}&rf=shutuba_submenu"
    driver.get(url)

    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table#sort_table, table.Shutuba_Past5_Table"))
        )
    except Exception:
        pass
    
    time.sleep(0.5)  # JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾…ã¡

    html = driver.page_source

    # ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ãã†ãªã‚‰ 1å›ã ã‘ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦å†å–å¾—
    if ("ç„¡æ–™ä¼šå“¡ç™»éŒ²" in html or "ãƒ­ã‚°ã‚¤ãƒ³" in html) and NETKEIBA_ID and NETKEIBA_PASS:
        ok = login_netkeiba(driver)
        if ok:
            driver.get(url)
            time.sleep(0.8)
            html = driver.page_source

    return parse_netkeiba_shutuba_past5(html, take_last_n=3)


# ==================================================
# fetch(Selenium)ç«¶é¦¬ãƒ–ãƒƒã‚¯
# ==================================================
def fetch_danwa_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/danwa/0/{race_id}"
    driver.get(url)
    time.sleep(0.8)
    html = driver.page_source
    return html, parse_race_info(html), parse_danwa_comments(html)

def fetch_zenkoso_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syoin/{race_id}"
    driver.get(url)
    time.sleep(0.8)
    return parse_zenkoso_interview(driver.page_source)

def fetch_cyokyo_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/cyokyo/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.cyokyo"))
        )
    except Exception:
        pass
    return parse_cyokyo(driver.page_source)

def fetch_syutuba_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syutuba/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.syutuba_sp, table.syutuba"))
        )
    except Exception:
        pass
    return parse_syutuba(driver.page_source)


# ==================================================
# ç›´è¿‘é–‹å‚¬:è¤‡æ•°å€™è£œæ¤œå‡º
# ==================================================
def detect_meet_candidates(driver, max_candidates: int = 12):
    driver.get(f"{BASE_URL}/cyuou/")
    time.sleep(1.0)
    html = driver.page_source

    keys12 = re.findall(r"/cyuou/syutuba/(\d{12})", html)
    if not keys12:
        keys12 = re.findall(r"/cyuou/thursday/(\d{12})", html)

    if not keys12:
        driver.get(f"{BASE_URL}/")
        time.sleep(1.0)
        html2 = driver.page_source
        keys12 = re.findall(r"/cyuou/syutuba/(\d{12})", html2)
        if not keys12:
            keys12 = re.findall(r"/cyuou/thursday/(\d{12})", html2)

    if not keys12:
        return []

    meet10_set = set(k[:10] for k in keys12 if len(k) >= 10)
    meet10_sorted = sorted(meet10_set, reverse=True)

    candidates = []
    for m10 in meet10_sorted[:max_candidates]:
        year = m10[0:4]
        kai = m10[4:6]
        place = m10[6:8]
        day = m10[8:10]
        candidates.append({
            "meet10": m10,
            "year": year,
            "kai": kai,
            "place": place,
            "day": day,
            "place_name": PLACE_NAMES.get(place, "ä¸æ˜"),
        })

    return candidates

def auto_detect_meet_candidates():
    driver = build_driver()
    try:
        login_keibabook(driver)
        return detect_meet_candidates(driver)
    finally:
        try:
            driver.quit()
        except Exception:
            pass


# ==================================================
# â˜…ä¿®æ­£:Dify(Streaming) - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–å¼·åŒ–
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ ã‚¨ãƒ©ãƒ¼: DIFY_API_KEY ãŒæœªè¨­å®š"
        return

    payload = {
        "inputs": {"text": full_text},
        "response_mode": "streaming",
        "user": "keiba-bot-user",
    }

    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}",
        "Content-Type": "application/json",
    }

    try:
        res = requests.post(
            "https://api.dify.ai/v1/workflows/run",
            headers=headers,
            json=payload,
            stream=True,
            timeout=600,  # 10åˆ†ã«å»¶é•·
        )

        if res.status_code != 200:
            yield f"âš ï¸ ã‚¨ãƒ©ãƒ¼: Dify API Error {res.status_code}\n{res.text}"
            return

        for line in res.iter_lines():
            if not line:
                continue
            decoded = line.decode("utf-8", errors="ignore")
            if not decoded.startswith("data:"):
                continue

            json_str = decoded.replace("data: ", "")
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError:
                continue

            event = data.get("event")
            if event in ["workflow_started", "node_started", "node_finished"]:
                continue

            chunk = data.get("answer", "")
            if chunk:
                yield chunk

            if event == "workflow_finished":
                outputs = data.get("data", {}).get("outputs", {})
                if outputs:
                    found_text = ""
                    for _, value in outputs.items():
                        if isinstance(value, str):
                            found_text += value + "\n"
                    if found_text.strip():
                        yield found_text.strip()

    except requests.exceptions.Timeout:
        yield "\n\nâš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: Difyã®å‡¦ç†æ™‚é–“ãŒé•·ã™ãã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã¯ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ãŒã€å…¨ã¦ã®å‡ºåŠ›ã‚’å—ä¿¡ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    except Exception as e:
        yield f"\n\nâš ï¸ Request Error: {str(e)}"


# ==================================================
# çµåˆç”¨:é¦¬åã‚­ãƒ¼æ•‘æ¸ˆ
# ==================================================
def _find_by_name_key(d: dict, bamei: str):
    if not bamei:
        return None
    if bamei in d:
        return d[bamei]
    for k, v in d.items():
        if (not str(k).isdigit()) and (str(k).strip() == bamei.strip()):
            return v
    return None


# ==================================================
# â˜…ä¿®æ­£:ãƒ¡ã‚¤ãƒ³å‡¦ç†(è¤‡æ•°ãƒ¬ãƒ¼ã‚¹) - Streamlitã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–
# ==================================================
def run_all_races(target_races=None):
    race_numbers = (
        list(range(1, 13))
        if target_races is None
        else sorted({int(r) for r in target_races})
    )

    base_id = f"{YEAR}{KAI}{PLACE}{DAY}"
    place_name = PLACE_NAMES.get(PLACE, "ä¸æ˜")

    combined_blocks: list[str] = []

    driver = build_driver()

    try:
        st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...(ç«¶é¦¬ãƒ–ãƒƒã‚¯)")
        login_keibabook(driver)
        st.success("âœ… ç«¶é¦¬ãƒ–ãƒƒã‚¯ ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†")

        # netkeibaã¯ã€Œå¿…è¦ãªã‚‰ã€ãƒ­ã‚°ã‚¤ãƒ³(å¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œ)
        if NETKEIBA_ID and NETKEIBA_PASS:
            st.info("ğŸ”‘ netkeiba ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèªä¸­(å¿…è¦ãªã‚‰)...")
            netkeiba_logged_in = login_netkeiba(driver)
            if netkeiba_logged_in:
                st.success("âœ… netkeiba ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†")
            else:
                st.warning("âš ï¸ netkeiba ãƒ­ã‚°ã‚¤ãƒ³ã¯æœªç¢ºèª(é–²è¦§å¯èƒ½ãªã‚‰å–å¾—ã§ãã¾ã™)")

        for r in race_numbers:
            race_num = f"{r:02}"
            race_id = base_id + race_num

            netkeiba_race_id = keibabook_race_id_to_netkeiba_race_id(YEAR, KAI, PLACE, DAY, race_num)

            st.markdown(f"### {place_name} {r}R")
            
            # â˜…ä¿®æ­£:st.empty()ã‚’ä½¿ã£ã¦æ›´æ–°å¯èƒ½ãªã‚³ãƒ³ãƒ†ãƒŠã‚’ä½œæˆ
            status_area = st.empty()
            result_container = st.container()
            
            full_answer = ""

            try:
                status_area.info(f"ğŸ“¡ {place_name}{r}R ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")

                # A-1 danwa + race_info
                _html_danwa, race_info, danwa_dict = fetch_danwa_dict(driver, race_id)

                # A-2 syoin
                zenkoso_dict = fetch_zenkoso_dict(driver, race_id)

                # A-3 cyokyo
                cyokyo_dict = fetch_cyokyo_dict(driver, race_id)

                # A-3.5 syutuba(é¦¬ç•ªãƒ»é¦¬åãƒ»é¨æ‰‹)
                syutuba_dict = fetch_syutuba_dict(driver, race_id)

                if not syutuba_dict:
                    status_area.warning("âš ï¸ å‡ºé¦¬è¡¨ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ(å…¨é ­ä¿è¨¼ã§ããªã„å¯èƒ½æ€§)ã€‚")

                # A-4 netkeiba æŒ‡æ•°(å–ã‚Œãªãã¦ã‚‚ç¶šè¡Œ)
                speed_dict = {}
                if netkeiba_race_id:
                    try:
                        status_area.info(f"ğŸ“Š netkeiba æŒ‡æ•°ã‚’å–å¾—ä¸­... (race_id: {netkeiba_race_id})")
                        speed_dict = fetch_netkeiba_speed_dict(driver, netkeiba_race_id)
                        if speed_dict:
                            status_area.success(f"âœ… netkeiba æŒ‡æ•°å–å¾—å®Œäº† ({len(speed_dict)}é ­åˆ†)")
                        else:
                            status_area.warning("âš ï¸ netkeiba æŒ‡æ•°ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    except Exception as e:
                        print("netkeiba speed fetch error:", e)
                        status_area.warning(f"âš ï¸ netkeiba æŒ‡æ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        speed_dict = {}

                # â˜…A-4.5 netkeiba é¦¬æŸ±(5èµ°):æˆ¦ç¸¾(å–ã‚Œãªãã¦ã‚‚ç¶šè¡Œ)
                past5_dict = {}
                if netkeiba_race_id:
                    try:
                        status_area.info(f"ğŸ“ netkeiba æˆ¦ç¸¾ã‚’å–å¾—ä¸­...")
                        past5_dict = fetch_netkeiba_past5_dict(driver, netkeiba_race_id)
                        if past5_dict:
                            status_area.success(f"âœ… netkeiba æˆ¦ç¸¾å–å¾—å®Œäº† ({len(past5_dict)}é ­åˆ†)")
                    except Exception as e:
                        print("netkeiba past5 fetch error:", e)
                        status_area.warning(f"âš ï¸ netkeiba æˆ¦ç¸¾å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        past5_dict = {}

                # A-5 çµåˆ(å‡ºé¦¬è¡¨ãƒ™ãƒ¼ã‚¹)
                merged = []
                umaban_list = (
                    sorted(syutuba_dict.keys(), key=lambda x: int(x))
                    if syutuba_dict
                    else sorted(
                        list(set(danwa_dict.keys()) | set(zenkoso_dict.keys()) | set(cyokyo_dict.keys())),
                        key=lambda x: int(x) if str(x).isdigit() else 999
                    )
                )

                for umaban in umaban_list:
                    sb = syutuba_dict.get(umaban, {})
                    bamei = (sb.get("bamei") or "").strip() or "åç§°ä¸æ˜"

                    kisyu_raw = (sb.get("kisyu") or "").strip()
                    kisyu_change = bool(sb.get("kisyu_change"))
                    if kisyu_raw:
                        kisyu = f"æ›¿ãƒ»{kisyu_raw}" if kisyu_change else kisyu_raw
                    else:
                        kisyu = "(é¨æ‰‹ä¸æ˜)"

                    # å©èˆã®è©±
                    d_comment = danwa_dict.get(umaban)
                    if not d_comment:
                        alt = _find_by_name_key(danwa_dict, bamei)
                        d_comment = alt if isinstance(alt, str) else None
                    if not d_comment:
                        d_comment = "(æƒ…å ±ãªã—)"

                    # å‰èµ°(ç«¶é¦¬ãƒ–ãƒƒã‚¯:å‰èµ°è«‡è©±)
                    z_data = zenkoso_dict.get(umaban)
                    if not z_data:
                        alt = _find_by_name_key(zenkoso_dict, bamei)
                        z_data = alt if isinstance(alt, dict) else None
                    z_data = z_data or {}

                    z_prev_info = ""
                    z_comment = ""
                    if z_data:
                        z_prev_info = f"{z_data.get('prev_date_course','')} {z_data.get('prev_class','')} {z_data.get('prev_finish','')}".strip()
                        z_comment = (z_data.get("prev_comment") or "").strip()

                    if z_prev_info or z_comment:
                        prev_block = (
                            f"  ã€å‰èµ°æƒ…å ±ã€‘ {z_prev_info or '(æƒ…å ±ãªã—)'}\n"
                            f"  ã€å‰èµ°è«‡è©±ã€‘ {z_comment or '(ç„¡ã—)'}\n"
                        )
                    else:
                        prev_block = "  ã€å‰èµ°ã€‘ æ–°é¦¬(å‰èµ°æƒ…å ±ãªã—)\n"

                    # èª¿æ•™(ç«¶é¦¬ãƒ–ãƒƒã‚¯)
                    c = cyokyo_dict.get(umaban)
                    if not c:
                        c = _find_by_name_key(cyokyo_dict, bamei)
                    c = c or {}

                    c_tanpyo = (c.get("tanpyo") or "").strip()
                    c_detail = (c.get("detail") or "").strip()

                    if c_tanpyo or c_detail:
                        cyokyo_block = f"  ã€èª¿æ•™ã€‘ çŸ­è©•:{c_tanpyo or '(ãªã—)'} / è©³ç´°:{c_detail or '(ãªã—)'}\n"
                    else:
                        cyokyo_block = "  ã€èª¿æ•™ã€‘ (æƒ…å ±ãªã—)\n"

                    # â˜…ä¿®æ­£:æŒ‡æ•°(netkeiba)â€»ã™ã¹ã¦ normalize æ¸ˆã¿ã® dict ã«ãªã£ã¦ã‚‹ãŒå¿µã®ãŸã‚å†æ­£è¦åŒ–
                    sp = speed_dict.get(umaban, {}) if isinstance(speed_dict, dict) else {}
                    idx1 = normalize_netkeiba_index_cell(sp.get("index1", "ç„¡"))
                    idx2 = normalize_netkeiba_index_cell(sp.get("index2", "ç„¡"))
                    idx3 = normalize_netkeiba_index_cell(sp.get("index3", "ç„¡"))
                    course = normalize_netkeiba_index_cell(sp.get("course", "ç„¡"))
                    avg5 = normalize_netkeiba_index_cell(sp.get("avg5", "ç„¡"))
                    speed_line = f"  ã€æŒ‡æ•°ã€‘ å‰èµ°:{idx1}ã€2èµ°å‰:{idx2}ã€3èµ°å‰:{idx3}ã€ã‚³ãƒ¼ã‚¹æœ€é«˜:{course}ã€5èµ°å¹³å‡:{avg5}\n"

                    # â˜…æˆ¦ç¸¾(netkeiba é¦¬æŸ±5èµ°)
                    past_info = past5_dict.get(umaban, {}) if isinstance(past5_dict, dict) else {}
                    past3 = past_info.get("past3") or ["", "", ""]
                    
                    # ä¸è¶³åˆ†ã¯ç©ºæ–‡å­—ã§åŸ‹ã‚ã‚‹
                    while len(past3) < 3:
                        past3.append("")
                    past3 = past3[:3]
                    
                    # æˆ¦ç¸¾ãƒ–ãƒ­ãƒƒã‚¯ä½œæˆ(éå»èµ°ãŒã‚ã‚‹åˆ†ã ã‘å‡ºåŠ›)
                    senreki_lines = []
                    labels = ["å‰èµ°", "2èµ°å‰", "3èµ°å‰"]
                    for i, (label, record) in enumerate(zip(labels, past3)):
                        if record:  # ç©ºã§ãªã‘ã‚Œã°å‡ºåŠ›
                            senreki_lines.append(f"{label}:{record}")
                    
                    if senreki_lines:
                        senreki_block = "  ã€æˆ¦ç¸¾ã€‘" + " ".join(senreki_lines) + "\n"
                    else:
                        senreki_block = "  ã€æˆ¦ç¸¾ã€‘ æ–°é¦¬(éå»èµ°ãªã—)\n"

                    text = (
                        f"â–¼[é¦¬ç•ª{umaban}] {bamei} / é¨æ‰‹:{kisyu}\n"
                        f"  ã€å©èˆã®è©±ã€‘ {d_comment}\n"
                        f"{prev_block}"
                        f"{cyokyo_block}"
                        f"{speed_line}"
                        f"{senreki_block}"
                    )
                    merged.append(text)

                if not merged:
                    status_area.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    st.write("---")
                    continue

                # ãƒ¬ãƒ¼ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼
                race_header_lines = []
                if race_info.get("date_meet"):
                    race_header_lines.append(race_info["date_meet"])
                if race_info.get("race_name"):
                    race_header_lines.append(race_info["race_name"])
                if race_info.get("cond1"):
                    race_header_lines.append(race_info["cond1"])
                if race_info.get("course_line"):
                    race_header_lines.append(race_info["course_line"])
                race_header = "\n".join(race_header_lines)

                merged_text = "\n".join(merged)

                full_text = (
                    "â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n"
                    f"{race_header}\n\n"
                    f"ä»¥ä¸‹ã¯{place_name}{r}Rã®å…¨é ­ãƒ‡ãƒ¼ã‚¿ã€‚\n"
                    "â– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿\n"
                    + merged_text
                )

                status_area.info("ğŸ¤– AIãŒåˆ†æãƒ»åŸ·ç­†ä¸­ã§ã™...")

                # â˜…ä¿®æ­£:result_containerã®ä¸­ã«markdownã‚¨ãƒªã‚¢ã‚’ä½œæˆ
                with result_container:
                    result_area = st.empty()
                    
                    for chunk in stream_dify_workflow(full_text):
                        if chunk:
                            full_answer += chunk
                            # ã‚«ãƒ¼ã‚½ãƒ«è¡¨ç¤ºã§é€²è¡Œä¸­ã‚’ç¤ºã™
                            result_area.markdown(full_answer + "â–Œ")

                    # æœ€çµ‚å‡ºåŠ›(ã‚«ãƒ¼ã‚½ãƒ«å‰Šé™¤)
                    result_area.markdown(full_answer)

                if full_answer.strip():
                    status_area.success("âœ… åˆ†æå®Œäº†")
                    save_history(YEAR, KAI, PLACE, place_name, DAY, race_num, race_id, full_answer)

                    with st.expander("ğŸ“‹ ã“ã®ãƒ¬ãƒ¼ã‚¹ã®å‡ºåŠ›ã‚’ã‚³ãƒ”ãƒ¼/ä¿å­˜", expanded=False):
                        dom_id = f"copy_race_{race_id}_{int(time.time()*1000)}"
                        render_copy_button(
                            text=full_answer.strip(),
                            label=f"ğŸ“‹ {place_name}{r}R ã‚’ã‚³ãƒ”ãƒ¼(ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯)",
                            dom_id=dom_id,
                        )
                        st.download_button(
                            label=f"â¬‡ï¸ {place_name}{r}R ã‚’txtä¿å­˜",
                            data=full_answer.strip(),
                            file_name=f"{YEAR}{KAI}{PLACE}{DAY}_{place_name}_{r}R.txt",
                            mime="text/plain",
                            key=f"dl_race_{race_id}",
                        )

                    combined_blocks.append(f"ã€{place_name} {r}Rã€‘\n{full_answer.strip()}\n")

                else:
                    status_area.error("âš ï¸ AIã‹ã‚‰ã®å›ç­”ãŒç©ºã§ã—ãŸã€‚")

            except Exception as e:
                err_msg = f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ ({place_name} {r}R): {str(e)}"
                print(err_msg)
                import traceback
                traceback.print_exc()
                status_area.error(err_msg)

            st.write("---")

        if combined_blocks:
            combined_text = "\n".join(combined_blocks).strip()
            st.session_state["combined_output"] = combined_text

            st.subheader("ğŸ“Œ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚(è¦æ±‚ã—ãŸãƒ¬ãƒ¼ã‚¹ã‚’å…¨éƒ¨ã¾ã¨ã‚ã¦ã‚³ãƒ”ãƒ¼)")

            dom_id_all = f"copy_all_{base_id}_{int(time.time()*1000)}"
            render_copy_button(
                text=combined_text,
                label="ğŸ“‹ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ã‚’ã‚³ãƒ”ãƒ¼(ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯)",
                dom_id=dom_id_all,
            )

            st.download_button(
                label="â¬‡ï¸ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ã‚’txtä¿å­˜",
                data=combined_text,
                file_name=f"{YEAR}{KAI}{PLACE}{DAY}_{place_name}_ALL.txt",
                mime="text/plain",
                key=f"dl_all_{base_id}",
            )

            with st.expander("ğŸ‘€ ã¾ã¨ã‚è¡¨ç¤º(é–²è¦§ç”¨)", expanded=False):
                st.text_area(
                    "å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ãƒ†ã‚­ã‚¹ãƒˆ",
                    value=combined_text,
                    height=420,
                    key=f"ta_all_{base_id}",
                )
        else:
            st.info("ã¾ã¨ã‚å¯¾è±¡ã®å‡ºåŠ›ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    finally:
        try:
            driver.quit()
        except Exception:
            pass
