import time
import json
import re
import requests
import streamlit as st
import streamlit.components.v1 as components
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆapp.py å´ã§ set_race_params ãŒå‘¼ã°ã‚Œã‚‹ã¨æ›¸ãæ›ã‚ã‚‹ï¼‰
YEAR = "2025"
KAI = "04"
PLACE = "02"
DAY = "02"

BASE_URL = "https://s.keibabook.co.jp"

PLACE_NAMES = {
    "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰", "04": "æ±äº¬",
    "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ", "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
}


def set_race_params(year, kai, place, day):
    """app.py ã‹ã‚‰é–‹å‚¬æƒ…å ±ã‚’å·®ã—æ›¿ãˆã‚‹ãŸã‚ã®é–¢æ•°"""
    global YEAR, KAI, PLACE, DAY
    YEAR = str(year)
    KAI = str(kai).zfill(2)
    PLACE = str(place).zfill(2)
    DAY = str(day).zfill(2)


def get_current_params():
    """ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆUIè¡¨ç¤ºç”¨ï¼‰"""
    return YEAR, KAI, PLACE, DAY


# ==================================================
# ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚³ãƒ”ãƒ¼ï¼ˆcomponents.html + clipboardï¼‰
# â€» Streamlitç’°å¢ƒã«ã‚ˆã£ã¦ components.html ãŒ key å¼•æ•°ã‚’å—ã‘ä»˜ã‘ãªã„ãŸã‚
#   components.html(..., key=...) ã¯ä½¿ã‚ãªã„
# ==================================================
def render_copy_button(text: str, label: str, dom_id: str):
    """
    dom_id ã‚’HTMLå´ã® id ã¨ã—ã¦ä½¿ã„ã€ãƒšãƒ¼ã‚¸å†…ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«ã™ã‚‹ã€‚
    """
    safe_text = json.dumps(text)  # JSæ–‡å­—åˆ—ã¨ã—ã¦å®‰å…¨ã«åŸ‹ã‚è¾¼ã‚€
    html = f"""
    <div style="display:flex; gap:8px; align-items:center; flex-wrap:wrap;">
      <button id="{dom_id}" style="
        padding:8px 12px;
        border-radius:10px;
        border:1px solid #ddd;
        background:#fff;
        cursor:pointer;
        font-size:14px;
      ">{label}</button>
      <span id="{dom_id}-msg" style="font-size:12px; color:#666;"></span>
    </div>
    <script>
      (function() {{
        const btn = document.getElementById("{dom_id}");
        const msg = document.getElementById("{dom_id}-msg");
        if (!btn) return;

        btn.addEventListener("click", async () => {{
          try {{
            await navigator.clipboard.writeText({safe_text});
            msg.textContent = "ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";
            setTimeout(() => msg.textContent = "", 1200);
          }} catch (e) {{
            msg.textContent = "ã‚³ãƒ”ãƒ¼ã«å¤±æ•—ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶åˆ¶é™ã®å¯èƒ½æ€§ï¼‰";
            setTimeout(() => msg.textContent = "", 2200);
          }}
        }});
      }})();
    </script>
    """
    # â˜…keyå¼•æ•°ã¯æ¸¡ã•ãªã„ï¼ˆã‚ãªãŸã®ç’°å¢ƒã§ã¯ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ï¼‰
    components.html(html, height=54)


# ==================================================
# Supabase
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client | None:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)


def save_history(
    year: str,
    kai: str,
    place_code: str,
    place_name: str,
    day: str,
    race_num_str: str,
    race_id: str,
    ai_answer: str,
) -> None:
    """history ãƒ†ãƒ¼ãƒ–ãƒ«ã« 1 ãƒ¬ãƒ¼ã‚¹åˆ†ã®äºˆæƒ³ã‚’ä¿å­˜ã™ã‚‹ã€‚"""
    supabase = get_supabase_client()
    if supabase is None:
        return

    data = {
        "year": str(year),
        "kai": str(kai),
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }

    try:
        supabase.table("history").insert(data).execute()
    except Exception as e:
        print("Supabase insert error:", e)


# ==================================================
# Selenium
# ==================================================
def build_driver() -> webdriver.Chrome:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,2200")
    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(60)
    return driver


def login_keibabook(driver: webdriver.Chrome) -> None:
    if not KEIBA_ID or not KEIBA_PASS:
        raise RuntimeError("KEIBA_ID / KEIBA_PASS ãŒ secrets ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    driver.get(f"{BASE_URL}/login/login")

    WebDriverWait(driver, 15).until(
        EC.visibility_of_element_located((By.NAME, "login_id"))
    ).send_keys(KEIBA_ID)

    WebDriverWait(driver, 15).until(
        EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
    ).send_keys(KEIBA_PASS)

    WebDriverWait(driver, 15).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))
    ).click()

    time.sleep(1.2)


# ==================================================
# Parserï¼šå…±é€š
# ==================================================
def parse_race_info(html: str):
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"date_meet": "", "race_name": "", "cond1": "", "course_line": ""}

    racemei = racetitle.find("div", class_="racemei")
    date_meet = ""
    race_name = ""
    if racemei:
        ps = racemei.find_all("p")
        if len(ps) >= 1:
            date_meet = ps[0].get_text(strip=True)
        if len(ps) >= 2:
            race_name = ps[1].get_text(strip=True)

    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    cond1 = ""
    course_line = ""
    if racetitle_sub:
        sub_ps = racetitle_sub.find_all("p")
        if len(sub_ps) >= 1:
            cond1 = sub_ps[0].get_text(strip=True)
        if len(sub_ps) >= 2:
            course_line = sub_ps[1].get_text(" ", strip=True)

    return {
        "date_meet": date_meet,
        "race_name": race_name,
        "cond1": cond1,
        "course_line": course_line,
    }


def parse_danwa_comments(html: str):
    """
    å©èˆã®è©±ã‚’å–å¾—ã€‚
    æˆ»ã‚Šå€¤ï¼š{ "1": "ã‚³ãƒ¡ãƒ³ãƒˆ", ... }ï¼ˆé¦¬ç•ªå„ªå…ˆã€‚ç„¡ç†ãªã‚‰é¦¬åã‚­ãƒ¼æ··åœ¨ï¼‰
    """
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table or not table.tbody:
        return {}

    danwa_dict = {}
    current_key = None

    for row in table.tbody.find_all("tr"):
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")

        # é¦¬ç•ª
        if uma_td:
            text = re.sub(r"\D", "", uma_td.get_text(strip=True))
            if text:
                current_key = text
                continue

        # é¦¬åï¼ˆæ–°é¦¬ç­‰ã®æºã‚Œå¯¾ç­–ï¼‰
        if bamei_td and not current_key:
            text = bamei_td.get_text(strip=True)
            if text:
                current_key = text
                continue

        # ã‚³ãƒ¡ãƒ³ãƒˆæœ¬ä½“
        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current_key:
            danwa_dict[current_key] = danwa_td.get_text(strip=True)
            current_key = None

    return danwa_dict


def parse_zenkoso_interview(html: str):
    """
    å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ï¼ˆsyoinï¼‰ã‚’å–å¾—ã€‚ç„¡ã„å ´åˆã¯ {}ã€‚
    æˆ»ã‚Šå€¤ï¼š{ "1": {...}, ... }ï¼ˆé¦¬ç•ªã‚­ãƒ¼ï¼‰
    """
    soup = BeautifulSoup(html, "html.parser")
    h2 = soup.find("h2", string=lambda s: s and "å‰èµ°" in s)
    if not h2:
        return {}

    table = h2.find_next("table", class_="syoin")
    if not table or not table.tbody:
        return {}

    rows = table.tbody.find_all("tr")
    result_dict = {}

    i = 0
    while i < len(rows):
        row = rows[i]
        if "spacer" in (row.get("class") or []):
            i += 1
            continue

        waku_td = row.find("td", class_="waku")
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")

        if not (waku_td and uma_td and bamei_td):
            i += 1
            continue

        waku = waku_td.get_text(strip=True)
        umaban = re.sub(r"\D", "", uma_td.get_text(strip=True))
        name = bamei_td.get_text(strip=True)

        prev_date = ""
        prev_class = ""
        prev_finish = ""
        prev_comment = ""

        detail = rows[i + 1] if i + 1 < len(rows) else None
        if detail:
            syoin_td = detail.find("td", class_="syoin")
            if syoin_td:
                sdata = syoin_td.find("div", class_="syoindata")
                if sdata:
                    ps = sdata.find_all("p")
                    if ps:
                        prev_date = ps[0].get_text(strip=True)
                    if len(ps) >= 2:
                        spans = ps[1].find_all("span")
                        if len(spans) >= 1:
                            prev_class = spans[0].get_text(strip=True)
                        if len(spans) >= 2:
                            prev_finish = spans[1].get_text(strip=True)

                direct = syoin_td.find_all("p", recursive=False)
                if direct:
                    txt = direct[0].get_text(strip=True)
                    if txt != "ï¼":
                        prev_comment = txt

        if umaban:
            result_dict[umaban] = {
                "waku": waku,
                "umaban": umaban,
                "name": name,
                "prev_date_course": prev_date,
                "prev_class": prev_class,
                "prev_finish": prev_finish,
                "prev_comment": prev_comment,
            }

        i += 2

    return result_dict


def parse_cyokyo(html: str):
    """
    èª¿æ•™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã€‚
    æˆ»ã‚Šå€¤ï¼š
      - é¦¬ç•ªã‚­ãƒ¼: { "1": {"tanpyo":"", "detail":"", "bamei_hint":""}, ... }
      - é¦¬ç•ªãŒå–ã‚Œãªã„å ´åˆï¼šé¦¬åã‚­ãƒ¼ã§å…¥ã‚‹ã“ã¨ãŒã‚ã‚‹ï¼ˆæ•‘æ¸ˆç”¨ï¼‰
    """
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}

    section = None
    h2 = soup.find("h2", string=lambda s: s and ("èª¿æ•™" in s or "ä¸­é–“" in s))
    if h2:
        midasi_div = h2.find_parent("div", class_="midasi")
        if midasi_div:
            section = midasi_div.find_next_sibling("div", class_="section")
    if section is None:
        section = soup

    tables = section.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody:
            continue
        rows = tbody.find_all("tr", recursive=False)
        if len(rows) < 1:
            continue

        header = rows[0]
        uma_td = header.find("td", class_="umaban")
        name_td = header.find("td", class_="kbamei")

        umaban_text = uma_td.get_text(strip=True) if uma_td else ""
        umaban = re.sub(r"\D", "", umaban_text)

        bamei_hint = ""
        if name_td:
            bamei_hint = name_td.get_text(" ", strip=True)

        tanpyo_td = header.find("td", class_="tanpyo")
        tanpyo = tanpyo_td.get_text(strip=True) if tanpyo_td else ""

        detail_row = rows[1] if len(rows) >= 2 else None
        detail_text = detail_row.get_text(" ", strip=True) if detail_row else ""

        payload = {"tanpyo": tanpyo, "detail": detail_text, "bamei_hint": bamei_hint}

        if umaban:
            cyokyo_dict[umaban] = payload
        else:
            if bamei_hint:
                cyokyo_dict[bamei_hint] = payload

    return cyokyo_dict


def parse_syutuba(html: str) -> dict:
    """
    ç¢ºå®šå‡ºé¦¬(å‡ºé¦¬è¡¨)ãƒšãƒ¼ã‚¸ã‹ã‚‰
    { "1": {"umaban":"1","bamei":"ã‚±ã‚¤ãƒ™ã‚¨","kisyu":"æœ¨å¹¡å·§","kisyu_change":True}, ... }
    ã‚’è¿”ã™ã€‚é¦¬ç•ªã‚’ä¸»ã‚­ãƒ¼ã«ã™ã‚‹ã€‚
    """
    soup = BeautifulSoup(html, "html.parser")

    table = soup.find("table", class_=lambda c: c and "syutuba_sp" in c.split())
    if not table:
        table = soup.find("table", class_=lambda c: c and "syutuba" in c)

    if not table or not table.tbody:
        return {}

    result = {}
    for tr in table.tbody.find_all("tr", recursive=False):
        tds = tr.find_all("td", recursive=False)
        if not tds:
            continue

        umaban_raw = tds[0].get_text(strip=True)
        umaban = re.sub(r"\D", "", umaban_raw)
        if not umaban:
            continue

        bamei = ""
        kbamei_p = tr.find("p", class_="kbamei")
        if kbamei_p:
            bamei = kbamei_p.get_text(" ", strip=True)

        kisyu = ""
        kisyu_change = False

        kisyu_p = tr.find("p", class_="kisyu")
        if kisyu_p:
            a = kisyu_p.find("a")
            if a:
                norika = a.find("span", class_="norikawari")
                if norika:
                    kisyu_change = True
                    kisyu = norika.get_text(strip=True)
                else:
                    kisyu = a.get_text(strip=True)
            else:
                norika = kisyu_p.find("span", class_="norikawari")
                if norika:
                    kisyu_change = True
                    kisyu = norika.get_text(strip=True)
                else:
                    kisyu = kisyu_p.get_text(" ", strip=True)

        result[umaban] = {
            "umaban": umaban,
            "bamei": bamei,
            "kisyu": kisyu,
            "kisyu_change": kisyu_change,
        }

    return result


# ==================================================
# fetchï¼ˆSeleniumï¼‰
# ==================================================
def fetch_danwa_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/danwa/0/{race_id}"
    driver.get(url)
    time.sleep(0.8)
    html = driver.page_source
    return html, parse_race_info(html), parse_danwa_comments(html)


def fetch_zenkoso_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syoin/{race_id}"
    driver.get(url)
    time.sleep(0.8)
    return parse_zenkoso_interview(driver.page_source)


def fetch_cyokyo_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/cyokyo/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.cyokyo"))
        )
    except Exception:
        pass
    return parse_cyokyo(driver.page_source)


def fetch_syutuba_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syutuba/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.syutuba_sp, table.syutuba"))
        )
    except Exception:
        pass
    return parse_syutuba(driver.page_source)


# ==================================================
# ç›´è¿‘é–‹å‚¬ï¼šè¤‡æ•°å€™è£œæ¤œå‡º
# ==================================================
def detect_meet_candidates(driver, max_candidates: int = 12):
    """
    Keibabookå†…ãƒšãƒ¼ã‚¸ã‹ã‚‰ syutuba racekey ã‚’æ‹¾ã„ã€
    é–‹å‚¬å˜ä½ï¼ˆYYYYKAIPLACEDAY = 10æ¡ï¼‰ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ã—ã¦å€™è£œãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    """
    driver.get(f"{BASE_URL}/cyuou/")
    time.sleep(1.0)
    html = driver.page_source

    keys12 = re.findall(r"/cyuou/syutuba/(\d{12})", html)
    if not keys12:
        keys12 = re.findall(r"/cyuou/thursday/(\d{12})", html)

    if not keys12:
        driver.get(f"{BASE_URL}/")
        time.sleep(1.0)
        html2 = driver.page_source
        keys12 = re.findall(r"/cyuou/syutuba/(\d{12})", html2)
        if not keys12:
            keys12 = re.findall(r"/cyuou/thursday/(\d{12})", html2)

    if not keys12:
        return []

    meet10_set = set(k[:10] for k in keys12 if len(k) >= 10)
    meet10_sorted = sorted(meet10_set, reverse=True)

    candidates = []
    for m10 in meet10_sorted[:max_candidates]:
        year = m10[0:4]
        kai = m10[4:6]
        place = m10[6:8]
        day = m10[8:10]
        candidates.append({
            "meet10": m10,
            "year": year,
            "kai": kai,
            "place": place,
            "day": day,
            "place_name": PLACE_NAMES.get(place, "ä¸æ˜"),
        })

    return candidates


def auto_detect_meet_candidates():
    driver = build_driver()
    try:
        login_keibabook(driver)
        return detect_meet_candidates(driver)
    finally:
        try:
            driver.quit()
        except Exception:
            pass


# ==================================================
# Difyï¼ˆStreamingï¼‰
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ ã‚¨ãƒ©ãƒ¼: DIFY_API_KEY ãŒæœªè¨­å®š"
        return

    payload = {
        "inputs": {"text": full_text},
        "response_mode": "streaming",
        "user": "keiba-bot-user",
    }

    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}",
        "Content-Type": "application/json",
    }

    try:
        res = requests.post(
            "https://api.dify.ai/v1/workflows/run",
            headers=headers,
            json=payload,
            stream=True,
            timeout=300,
        )

        if res.status_code != 200:
            yield f"âš ï¸ ã‚¨ãƒ©ãƒ¼: Dify API Error {res.status_code}\n{res.text}"
            return

        for line in res.iter_lines():
            if not line:
                continue
            decoded = line.decode("utf-8", errors="ignore")
            if not decoded.startswith("data:"):
                continue

            json_str = decoded.replace("data: ", "")
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError:
                continue

            event = data.get("event")
            if event in ["workflow_started", "node_started", "node_finished"]:
                continue

            chunk = data.get("answer", "")
            if chunk:
                yield chunk

            if event == "workflow_finished":
                outputs = data.get("data", {}).get("outputs", {})
                if outputs:
                    found_text = ""
                    for _, value in outputs.items():
                        if isinstance(value, str):
                            found_text += value + "\n"
                    if found_text.strip():
                        yield found_text.strip()

    except Exception as e:
        yield f"âš ï¸ Request Error: {str(e)}"


# ==================================================
# çµåˆç”¨ï¼šé¦¬åã‚­ãƒ¼æ•‘æ¸ˆ
# ==================================================
def _find_by_name_key(d: dict, bamei: str):
    if not bamei:
        return None
    if bamei in d:
        return d[bamei]
    for k, v in d.items():
        if (not str(k).isdigit()) and (str(k).strip() == bamei.strip()):
            return v
    return None


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆè¤‡æ•°ãƒ¬ãƒ¼ã‚¹ï¼‰
# ==================================================
def run_all_races(target_races=None):
    """
    target_races: None -> 1~12
                 list/set -> æŒ‡å®šãƒ¬ãƒ¼ã‚¹ç•ªå·ã ã‘å®Ÿè¡Œ

    ä»•æ§˜ï¼š
      - ãƒ¬ãƒ¼ã‚¹å˜ä½ã§å‡ºåŠ›è¡¨ç¤º
      - å„ãƒ¬ãƒ¼ã‚¹ã€Œãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚³ãƒ”ãƒ¼ã€ï¼‹txtä¿å­˜
      - æœ€å¾Œã«ã€Œå…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ã€ã‚’ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚³ãƒ”ãƒ¼ï¼‹txtä¿å­˜ï¼‹é–²è¦§ç”¨text_area
    """
    race_numbers = (
        list(range(1, 13))
        if target_races is None
        else sorted({int(r) for r in target_races})
    )

    base_id = f"{YEAR}{KAI}{PLACE}{DAY}"
    place_name = PLACE_NAMES.get(PLACE, "ä¸æ˜")

    combined_blocks: list[str] = []

    driver = build_driver()

    try:
        st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        login_keibabook(driver)
        st.success("âœ… ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†")

        for r in race_numbers:
            race_num = f"{r:02}"
            race_id = base_id + race_num

            st.markdown(f"### {place_name} {r}R")
            status_area = st.empty()
            result_area = st.empty()
            full_answer = ""

            try:
                status_area.info(f"ğŸ“¡ {place_name}{r}R ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")

                # A-1 danwa + race_info
                _html_danwa, race_info, danwa_dict = fetch_danwa_dict(driver, race_id)

                # A-2 syoin
                zenkoso_dict = fetch_zenkoso_dict(driver, race_id)

                # A-3 cyokyo
                cyokyo_dict = fetch_cyokyo_dict(driver, race_id)

                # A-3.5 syutubaï¼ˆé¦¬ç•ªãƒ»é¦¬åãƒ»é¨æ‰‹ï¼‰
                syutuba_dict = fetch_syutuba_dict(driver, race_id)

                if not syutuba_dict:
                    status_area.warning("âš ï¸ å‡ºé¦¬è¡¨ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆå…¨é ­ä¿è¨¼ã§ããªã„å¯èƒ½æ€§ï¼‰ã€‚")

                # A-4 çµåˆï¼ˆå‡ºé¦¬è¡¨ãƒ™ãƒ¼ã‚¹ï¼‰
                merged = []
                umaban_list = (
                    sorted(syutuba_dict.keys(), key=lambda x: int(x))
                    if syutuba_dict
                    else sorted(
                        list(set(danwa_dict.keys()) | set(zenkoso_dict.keys()) | set(cyokyo_dict.keys())),
                        key=lambda x: int(x) if str(x).isdigit() else 999
                    )
                )

                for umaban in umaban_list:
                    sb = syutuba_dict.get(umaban, {})
                    bamei = (sb.get("bamei") or "").strip() or "åç§°ä¸æ˜"

                    kisyu_raw = (sb.get("kisyu") or "").strip()
                    kisyu_change = bool(sb.get("kisyu_change"))
                    if kisyu_raw:
                        kisyu = f"æ›¿ãƒ»{kisyu_raw}" if kisyu_change else kisyu_raw
                    else:
                        kisyu = "ï¼ˆé¨æ‰‹ä¸æ˜ï¼‰"

                    # å©èˆã®è©±
                    d_comment = danwa_dict.get(umaban)
                    if not d_comment:
                        alt = _find_by_name_key(danwa_dict, bamei)
                        d_comment = alt if isinstance(alt, str) else None
                    if not d_comment:
                        d_comment = "ï¼ˆæƒ…å ±ãªã—ï¼‰"

                    # å‰èµ°
                    z_data = zenkoso_dict.get(umaban)
                    if not z_data:
                        alt = _find_by_name_key(zenkoso_dict, bamei)
                        z_data = alt if isinstance(alt, dict) else None
                    z_data = z_data or {}

                    z_prev_info = ""
                    z_comment = ""
                    if z_data:
                        z_prev_info = f"{z_data.get('prev_date_course','')} {z_data.get('prev_class','')} {z_data.get('prev_finish','')}".strip()
                        z_comment = (z_data.get("prev_comment") or "").strip()

                    if z_prev_info or z_comment:
                        prev_block = (
                            f"  ã€å‰èµ°æƒ…å ±ã€‘ {z_prev_info or 'ï¼ˆæƒ…å ±ãªã—ï¼‰'}\n"
                            f"  ã€å‰èµ°è«‡è©±ã€‘ {z_comment or 'ï¼ˆç„¡ã—ï¼‰'}\n"
                        )
                    else:
                        prev_block = "  ã€å‰èµ°ã€‘ æ–°é¦¬ï¼ˆå‰èµ°æƒ…å ±ãªã—ï¼‰\n"

                    # èª¿æ•™
                    c = cyokyo_dict.get(umaban)
                    if not c:
                        c = _find_by_name_key(cyokyo_dict, bamei)
                    c = c or {}

                    c_tanpyo = (c.get("tanpyo") or "").strip()
                    c_detail = (c.get("detail") or "").strip()

                    if c_tanpyo or c_detail:
                        cyokyo_block = f"  ã€èª¿æ•™ã€‘ çŸ­è©•:{c_tanpyo or 'ï¼ˆãªã—ï¼‰'} / è©³ç´°:{c_detail or 'ï¼ˆãªã—ï¼‰'}\n"
                    else:
                        cyokyo_block = "  ã€èª¿æ•™ã€‘ ï¼ˆæƒ…å ±ãªã—ï¼‰\n"

                    text = (
                        f"â–¼[é¦¬ç•ª{umaban}] {bamei} / é¨æ‰‹:{kisyu}\n"
                        f"  ã€å©èˆã®è©±ã€‘ {d_comment}\n"
                        f"{prev_block}"
                        f"{cyokyo_block}"
                    )
                    merged.append(text)

                if not merged:
                    status_area.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    st.write("---")
                    continue

                # ãƒ¬ãƒ¼ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼
                race_header_lines = []
                if race_info.get("date_meet"):
                    race_header_lines.append(race_info["date_meet"])
                if race_info.get("race_name"):
                    race_header_lines.append(race_info["race_name"])
                if race_info.get("cond1"):
                    race_header_lines.append(race_info["cond1"])
                if race_info.get("course_line"):
                    race_header_lines.append(race_info["course_line"])
                race_header = "\n".join(race_header_lines)

                merged_text = "\n".join(merged)

                full_text = (
                    "â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n"
                    f"{race_header}\n\n"
                    f"ä»¥ä¸‹ã¯{place_name}{r}Rã®å…¨é ­ãƒ‡ãƒ¼ã‚¿ã€‚\n"
                    "â– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿\n"
                    + merged_text
                )

                status_area.info("ğŸ¤– AIãŒåˆ†æãƒ»åŸ·ç­†ä¸­ã§ã™...")

                for chunk in stream_dify_workflow(full_text):
                    if chunk:
                        full_answer += chunk
                        result_area.markdown(full_answer + "â–Œ")

                result_area.markdown(full_answer)

                if full_answer.strip():
                    status_area.success("âœ… åˆ†æå®Œäº†")
                    save_history(YEAR, KAI, PLACE, place_name, DAY, race_num, race_id, full_answer)

                    # ãƒ¬ãƒ¼ã‚¹å˜ä½ï¼šã‚³ãƒ”ãƒ¼ï¼†ä¿å­˜
                    with st.expander("ğŸ“‹ ã“ã®ãƒ¬ãƒ¼ã‚¹ã®å‡ºåŠ›ã‚’ã‚³ãƒ”ãƒ¼/ä¿å­˜", expanded=False):
                        # dom_idã‚’ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼ˆå†æç”»å¯¾ç­–ã§æ™‚åˆ»ã‚‚æ··ãœã‚‹ï¼‰
                        dom_id = f"copy_race_{race_id}_{int(time.time()*1000)}"
                        render_copy_button(
                            text=full_answer.strip(),
                            label=f"ğŸ“‹ {place_name}{r}R ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ï¼‰",
                            dom_id=dom_id,
                        )
                        st.download_button(
                            label=f"â¬‡ï¸ {place_name}{r}R ã‚’txtä¿å­˜",
                            data=full_answer.strip(),
                            file_name=f"{YEAR}{KAI}{PLACE}{DAY}_{place_name}_{r}R.txt",
                            mime="text/plain",
                            key=f"dl_race_{race_id}",
                        )

                    combined_blocks.append(f"ã€{place_name} {r}Rã€‘\n{full_answer.strip()}\n")

                else:
                    status_area.error("âš ï¸ AIã‹ã‚‰ã®å›ç­”ãŒç©ºã§ã—ãŸã€‚")

            except Exception as e:
                err_msg = f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ ({place_name} {r}R): {str(e)}"
                print(err_msg)
                status_area.error(err_msg)

            st.write("---")

        # å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ï¼šã‚³ãƒ”ãƒ¼ï¼†ä¿å­˜
        if combined_blocks:
            combined_text = "\n".join(combined_blocks).strip()
            st.session_state["combined_output"] = combined_text

            st.subheader("ğŸ“Œ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ï¼ˆè¦æ±‚ã—ãŸãƒ¬ãƒ¼ã‚¹ã‚’å…¨éƒ¨ã¾ã¨ã‚ã¦ã‚³ãƒ”ãƒ¼ï¼‰")

            dom_id_all = f"copy_all_{base_id}_{int(time.time()*1000)}"
            render_copy_button(
                text=combined_text,
                label="ğŸ“‹ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ï¼‰",
                dom_id=dom_id_all,
            )

            st.download_button(
                label="â¬‡ï¸ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ã‚’txtä¿å­˜",
                data=combined_text,
                file_name=f"{YEAR}{KAI}{PLACE}{DAY}_{place_name}_ALL.txt",
                mime="text/plain",
                key=f"dl_all_{base_id}",
            )

            with st.expander("ğŸ‘€ ã¾ã¨ã‚è¡¨ç¤ºï¼ˆé–²è¦§ç”¨ï¼‰", expanded=False):
                st.text_area(
                    "å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚ãƒ†ã‚­ã‚¹ãƒˆ",
                    value=combined_text,
                    height=420,
                    key=f"ta_all_{base_id}",
                )
        else:
            st.info("ã¾ã¨ã‚å¯¾è±¡ã®å‡ºåŠ›ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    finally:
        try:
            driver.quit()
        except Exception:
            pass
