import time
import json
import re
import math
import requests
import streamlit as st
import streamlit.components.v1 as components
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š (ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°)
YEAR = "2026"
KAI = "01"
PLACE = "05"  # ä¸­å±±
DAY = "03"

BASE_URL = "https://s.keibabook.co.jp"

PLACE_NAMES = {
    "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰", "04": "æ±äº¬",
    "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ", "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
}

# ç«¶é¦¬ãƒ–ãƒƒã‚¯ PLACEã‚³ãƒ¼ãƒ‰ â†’ netkeiba ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
KEIBABOOK_TO_NETKEIBA_PLACE = {
    "08": "01", "09": "02", "06": "03", "07": "04", "04": "05",
    "05": "06", "02": "07", "00": "08", "01": "09", "03": "10",
}

# ==================================================
# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šãƒ»å–å¾—é–¢æ•°
# ==================================================
def set_race_params(year, kai, place, day):
    """UIã‹ã‚‰é–‹å‚¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹ãŸã‚ã®é–¢æ•°"""
    global YEAR, KAI, PLACE, DAY
    YEAR = str(year)
    KAI = str(kai).zfill(2)
    PLACE = str(place).zfill(2)
    DAY = str(day).zfill(2)

def get_current_params():
    """ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿”ã™é–¢æ•°"""
    return YEAR, KAI, PLACE, DAY

# ==================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ==================================================
def _clean_text_ja(s: str) -> str:
    """å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹é™¤å»ãƒ»ç©ºç™½æ­£è¦åŒ–"""
    if not s:
        return ""
    s = s.replace("\u3000", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _is_missing_marker(s: str) -> bool:
    """æƒ…å ±ãªã—ãƒãƒ¼ã‚«ãƒ¼åˆ¤å®š"""
    t = _clean_text_ja(s)
    return t in {"ï¼", "-", "â€”", "â€•", "â€", ""}

def render_copy_button(text: str, label: str, dom_id: str):
    """ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³è¡¨ç¤º"""
    safe_text = json.dumps(text)
    html = f"""
    <div style="display:flex; gap:8px; align-items:center; flex-wrap:wrap;">
      <button id="{dom_id}" style="
        padding:8px 12px;
        border-radius:10px;
        border:1px solid #ddd;
        background:#fff;
        cursor:pointer;
        font-size:14px;
      ">{label}</button>
      <span id="{dom_id}-msg" style="font-size:12px; color:#666;"></span>
    </div>
    <script>
      (function() {{
        const btn = document.getElementById("{dom_id}");
        const msg = document.getElementById("{dom_id}-msg");
        if (!btn) return;
        btn.addEventListener("click", async () => {{
          try {{
            await navigator.clipboard.writeText({safe_text});
            msg.textContent = "ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";
            setTimeout(() => msg.textContent = "", 1200);
          }} catch (e) {{
            msg.textContent = "ã‚³ãƒ”ãƒ¼å¤±æ•—";
            setTimeout(() => msg.textContent = "", 2200);
          }}
        }});
      }})();
    </script>
    """
    components.html(html, height=54)

def _safe_int(s, default=0) -> int:
    """'-' ç­‰ã‚’å®‰å…¨ã« int åŒ–"""
    try:
        if s is None:
            return default
        if isinstance(s, (int, float)):
            return int(s)
        ss = str(s).strip()
        if ss in {"", "-", "ï¼"}:
            return default
        return int(ss)
    except:
        return default

# ==================================================
# ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°ï¼ˆç«¶é¦¬äºˆæƒ³ã¨ã—ã¦æœ€ã‚‚æ‰±ã„ã‚„ã™ã„å½¢ï¼‰
# ==================================================
def compute_speed_metrics(cpu_data: dict, T: float = 55.0, k: float = 4.0) -> dict:
    """
    ã€ç‹™ã„ã€‘
    - ã‚¯ãƒ©ã‚¹å·®ï¼ˆæœªå‹åˆ©/é‡è³ï¼‰ã§ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°ã®å½±éŸ¿ãŒæ­ªã‚€å•é¡Œã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€åŒãƒ¬ãƒ¼ã‚¹å†…ã®ç›¸å¯¾è©•ä¾¡ã«ã™ã‚‹
    - ãŸã ã—ã€Œä¸Šä½ãŒè¤‡æ•°ã„ã‚‹ã¨åå·®å€¤ãŒå›ºã¾ã‚‹ã€å•é¡Œã‚’é¿ã‘ã‚‹ãŸã‚ã€ä¸Šä½å´ã ã‘ãƒ¡ãƒªãƒãƒªã‚’ä»˜ã‘ã‚‹

    ã€æ‰‹é †ã€‘
    1) ã‚¹ãƒ”ãƒ¼ãƒ‰åŸºç¤å€¤ raw =ï¼ˆæœ€é«˜å€¤Ã—3 ï¼‹ å‰èµ° ï¼‹ å¹³å‡ï¼‰Ã·5
    2) raw ã‚’åŒãƒ¬ãƒ¼ã‚¹å†…ã§åå·®å€¤ devï¼ˆå¹³å‡50, SD10ï¼‰ã«ã™ã‚‹
    3) dev ã‚’ â€œä¸Šä½å¼·èª¿ã‚¹ã‚³ã‚¢â€ scoreï¼ˆ0ã€œ100ï¼‰ã«å¤‰æ›
       score = 100 / (1 + exp(-(dev - T)/k))
       - T: ä¸Šä½ã®å…¥å£ï¼ˆ55æ¨å¥¨ï¼‰
       - k: ãƒ¡ãƒªãƒãƒªï¼ˆ4æ¨å¥¨ã€å°ã•ã„ã»ã©å¼·èª¿ï¼‰

    return:
      {
        umaban: {
          "raw": float,
          "dev": float,    # åå·®å€¤ï¼ˆ0ã€œ100ã‚¯ãƒªãƒƒãƒ—ï¼‰
          "score": float   # 0ã€œ100ï¼ˆä¸Šä½å¼·èª¿ï¼‰
        }
      }
    """
    raw_scores = {}

    for umaban, d in cpu_data.items():
        last = _safe_int(d.get("sp_last"), 0)
        two  = _safe_int(d.get("sp_2"), 0)
        thr  = _safe_int(d.get("sp_3"), 0)

        vals = [v for v in [last, two, thr] if v > 0]
        if not vals:
            continue

        avg = sum(vals) / len(vals)
        max_v = max(vals)

        raw = (max_v * 3 + last + avg) / 5.0
        raw_scores[umaban] = raw

    if not raw_scores:
        return {}

    values = list(raw_scores.values())
    mean = sum(values) / len(values)
    std = math.sqrt(sum((v - mean) ** 2 for v in values) / len(values))

    out = {}
    for umaban, raw in raw_scores.items():
        if std == 0:
            dev = 50.0
        else:
            dev = 50.0 + 10.0 * (raw - mean) / std

        # åå·®å€¤ã¯è¡¨ç¤ºãƒ»ç›£æŸ»ç”¨ã¨ã—ã¦0ã€œ100ã«ã‚¯ãƒªãƒƒãƒ—
        dev_clip = max(0.0, min(100.0, round(dev, 1)))

        # ä¸Šä½å¼·èª¿ã‚¹ã‚³ã‚¢ï¼ˆ0ã€œ100ï¼‰
        x = (dev - float(T)) / float(k)
        score = 100.0 / (1.0 + math.exp(-x))
        score = max(0.0, min(100.0, round(score, 1)))

        out[umaban] = {"raw": round(raw, 2), "dev": dev_clip, "score": score}

    return out

# ==================================================
# Selenium Setup
# ==================================================
def build_driver() -> webdriver.Chrome:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,2200")
    options.add_argument("--lang=ja-JP")
    options.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(60)
    return driver

def login_keibabook(driver: webdriver.Chrome) -> None:
    if not KEIBA_ID or not KEIBA_PASS:
        return
    driver.get(f"{BASE_URL}/login/login")
    try:
        WebDriverWait(driver, 5).until(
            EC.visibility_of_element_located((By.NAME, "login_id"))
        ).send_keys(KEIBA_ID)
        WebDriverWait(driver, 5).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
        ).send_keys(KEIBA_PASS)
        WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))
        ).click()
        time.sleep(1.0)
    except:
        pass

# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼šå©èˆã®è©± (Danwa)
# ==================================================
def parse_race_info_from_danwa(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"header_text": ""}

    racemei = racetitle.find("div", class_="racemei")
    header_parts = []
    if racemei:
        for p in racemei.find_all("p"):
            header_parts.append(p.get_text(strip=True))

    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    if racetitle_sub:
        for p in racetitle_sub.find_all("p"):
            header_parts.append(p.get_text(strip=True))

    return {"header_text": "\n".join(header_parts)}

def parse_danwa_horses(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_=lambda c: c and "danwa" in str(c))
    if not table or not table.tbody:
        return {}

    horses = {}
    current_umaban = None

    rows = table.tbody.find_all("tr", recursive=False)
    for tr in rows:
        classes = tr.get("class", [])
        if "spacer" in classes:
            continue

        umaban_td = tr.find("td", class_="umaban")
        bamei_td = tr.find("td", class_="left")

        if umaban_td and bamei_td:
            raw_umaban = umaban_td.get_text(strip=True)
            current_umaban = re.sub(r"\D", "", raw_umaban)

            anchor = bamei_td.find("a")
            if anchor:
                raw_name = anchor.get_text(strip=True)
            else:
                raw_name = bamei_td.get_text(strip=True)

            clean_name = _clean_text_ja(raw_name)
            if current_umaban:
                horses[current_umaban] = {"name": clean_name, "danwa": ""}
            continue

        danwa_td = tr.find("td", class_="danwa")
        if danwa_td and current_umaban:
            comment_text = danwa_td.get_text("\n", strip=True)
            comment_text = _clean_text_ja(comment_text)

            if horses[current_umaban]["danwa"]:
                horses[current_umaban]["danwa"] += (" " + comment_text)
            else:
                horses[current_umaban]["danwa"] = comment_text

    return horses

def fetch_keibabook_danwa(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/danwa/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.default.danwa"))
        )
    except:
        pass
    html = driver.page_source
    return parse_race_info_from_danwa(html), parse_danwa_horses(html)

# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼šèª¿æ•™ (Chokyo)
# ==================================================
def parse_keibabook_chokyo(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    data = {}

    tables = soup.find_all("table", class_="cyokyo")

    for tbl in tables:
        umaban_td = tbl.find("td", class_="umaban")
        if not umaban_td:
            continue
        umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))

        tanpyo_td = tbl.find("td", class_="tanpyo")
        tanpyo = _clean_text_ja(tanpyo_td.get_text(strip=True)) if tanpyo_td else "ãªã—"

        details_text_parts = []
        detail_cell = tbl.find("td", colspan="5")
        if detail_cell:
            for child in detail_cell.children:
                if child.name == "dl" and "dl-table" in child.get("class", []):
                    dt_texts = [c.get_text(strip=True) for c in child.find_all(["dt", "dd"])]
                    line = " ".join(dt_texts)
                    details_text_parts.append(line)

                elif child.name == "table" and "cyokyodata" in child.get("class", []):
                    time_tr = child.find("tr", class_="time")
                    if time_tr:
                        times = [td.get_text(strip=True) for td in time_tr.find_all("td")]
                        details_text_parts.append(" ".join(times))

                    awase_tr = child.find("tr", class_="awase")
                    if awase_tr:
                        awase_txt = _clean_text_ja(awase_tr.get_text(strip=True))
                        details_text_parts.append(awase_txt)

            semekaisetu_div = detail_cell.find("div", class_="semekaisetu")
            if semekaisetu_div:
                kaisetu_p = semekaisetu_div.find("p")
                if kaisetu_p:
                    k_text = _clean_text_ja(kaisetu_p.get_text(strip=True))
                    details_text_parts.append(f"[æ”»ã‚è§£èª¬] {k_text}")

        full_detail = " ".join(details_text_parts)
        full_detail = re.sub(r"\s+", " ", full_detail).strip()

        data[umaban] = {"tanpyo": tanpyo, "details": full_detail}

    return data

def fetch_keibabook_chokyo(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/cyokyo/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "cyokyo"))
        )
    except:
        pass
    html = driver.page_source
    return parse_keibabook_chokyo(html)

# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼šå‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ (Syoin)
# ==================================================
def parse_zenkoso_interview(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_=lambda c: c and "syoin" in str(c))
    if not table or not table.tbody:
        return {}

    interview_data = {}
    current_umaban = None
    rows = table.tbody.find_all("tr", recursive=False)

    for tr in rows:
        classes = tr.get("class", [])
        if "spacer" in classes:
            continue

        umaban_td = tr.find("td", class_="umaban")
        if umaban_td:
            raw_u = umaban_td.get_text(strip=True)
            current_umaban = re.sub(r"\D", "", raw_u)
            continue

        syoin_td = tr.find("td", class_="syoin")
        if syoin_td and current_umaban:
            meta_div = syoin_td.find("div", class_="syoindata")
            if meta_div:
                meta_div.decompose()

            raw_text = syoin_td.get_text(" ", strip=True)
            clean_text = _clean_text_ja(raw_text)

            if not _is_missing_marker(clean_text) and len(clean_text) > 1:
                interview_data[current_umaban] = clean_text

            current_umaban = None

    return interview_data

def fetch_zenkoso_interview(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syoin/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.default.syoin"))
        )
    except:
        pass
    return parse_zenkoso_interview(driver.page_source)

# ==================================================
# ç«¶é¦¬ãƒ–ãƒƒã‚¯ï¼šCPUäºˆæƒ³ (æ–°é¦¬å¯¾å¿œç‰ˆ)
# ==================================================
def parse_keibabook_cpu(html: str, is_shinba: bool = False) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    data = {}

    # --- ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ« ---
    speed_tbl = soup.find("table", id="cpu_speed_sort_table")
    if speed_tbl and speed_tbl.tbody:
        for tr in speed_tbl.tbody.find_all("tr"):
            umaban_td = tr.find("td", class_="umaban")
            if not umaban_td:
                continue
            umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))
            if not umaban:
                continue

            tds = tr.find_all("td")
            if len(tds) < 8:
                continue

            def get_v(idx):
                p = tds[idx].find("p")
                txt = re.sub(r"\D", "", p.get_text(strip=True)) if p else ""
                val = int(txt) if txt else 0
                # 1000ç­‰ã®ç•°å¸¸å€¤ã¯æ¬ ææ‰±ã„
                return val if val < 900 else 0

            last = get_v(-1)
            two = get_v(-2)
            thr = get_v(-3)
            vals = [x for x in [last, two, thr] if x > 0]
            avg = round(sum(vals) / len(vals)) if vals else 0

            data[umaban] = {
                "sp_last": str(last) if last else "-",
                "sp_2": str(two) if two else "-",
                "sp_3": str(thr) if thr else "-",
                "sp_avg": str(avg) if avg else "-",
            }

    # --- ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ« ---
    factor_tbl = None
    for t in soup.find_all("table"):
        c = t.find("caption")
        if c and "ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼" in c.get_text():
            factor_tbl = t
            break

    if factor_tbl and factor_tbl.tbody:
        for tr in factor_tbl.tbody.find_all("tr"):
            umaban_td = tr.find("td", class_="umaban")
            if not umaban_td:
                continue
            umaban = re.sub(r"\D", "", umaban_td.get_text(strip=True))
            if not umaban:
                continue

            tds = tr.find_all("td")
            if len(tds) < 6:
                continue

            def get_m(idx):
                if idx >= len(tds):
                    return "-"
                p = tds[idx].find("p")
                t = p.get_text(strip=True) if p else ""
                return t if t else "-"

            if umaban not in data:
                data[umaban] = {}

            if is_shinba:
                data[umaban].update(
                    {
                        "fac_deashi": get_m(5),
                        "fac_kettou": get_m(6),
                        "fac_ugoki": get_m(8),
                    }
                )
            else:
                data[umaban].update(
                    {
                        "fac_crs": get_m(5),
                        "fac_dis": get_m(6),
                        "fac_zen": get_m(7),
                    }
                )

    return data

def fetch_keibabook_cpu_data(driver, race_id: str, is_shinba: bool = False):
    url = f"{BASE_URL}/cyuou/cpu/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, "main")))
    except:
        pass
    return parse_keibabook_cpu(driver.page_source, is_shinba)

# ==================================================
# Netkeiba (é¨æ‰‹ãƒ»æˆ¦ç¸¾è©³ç´°å–å¾—)
# ==================================================
def _parse_netkeiba_past_td(td) -> str:
    """netkeibaã®éå»èµ°ã‚»ãƒ«ï¼ˆtd.Pastï¼‰ã‚’è§£æã—ã¦æ–‡å­—åˆ—åŒ–"""
    if not td:
        return "-"

    data01 = td.find("div", class_="Data01")
    date_place = _clean_text_ja(data01.get_text(strip=True)) if data01 else ""

    data02 = td.find("div", class_="Data02")
    race_name = _clean_text_ja(data02.get_text(strip=True)) if data02 else ""

    data03 = td.find("div", class_="Data03")
    jockey_weight = _clean_text_ja(data03.get_text(" ", strip=True)) if data03 else ""

    rank = "?"
    rank_tag = td.find("span", class_="Num")
    if not rank_tag:
        rank_tag = td.find("div", class_="Rank") or td.find("span", class_="Rank") or td.find("span", class_="Order")
    if rank_tag:
        rank = _clean_text_ja(rank_tag.get_text(strip=True))

    data05 = td.find("div", class_="Data05")
    time_dist = _clean_text_ja(data05.get_text(" ", strip=True)) if data05 else ""

    passing = ""
    data06 = td.find("div", class_="Data06")
    if data06:
        raw_d6 = _clean_text_ja(data06.get_text(strip=True))
        match = re.search(r"(\d{1,2}(?:-\d{1,2})+)", raw_d6)
        if match:
            passing = match.group(1)
        if not passing:
            match_single = re.match(r"^(\d{1,2})\s", raw_d6)
            if match_single:
                passing = match_single.group(1)

    if len(date_place) < 2:
        return "-"

    rank_display = f"{passing}â†’{rank}ç€" if passing else f"{rank}ç€"
    return f"[{date_place} {race_name} {jockey_weight} {time_dist} ({rank_display})]"

def fetch_netkeiba_data(driver, year, kai, place, day, race_num):
    nk_place = KEIBABOOK_TO_NETKEIBA_PLACE.get(place, "")
    if not nk_place:
        return {}

    nk_race_id = f"{year}{nk_place}{kai.zfill(2)}{day.zfill(2)}{race_num.zfill(2)}"
    url = f"https://race.netkeiba.com/race/shutuba_past.html?race_id={nk_race_id}&rf=shutuba_submenu"

    driver.get(url)
    try:
        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, "Shutuba_Past5_Table")))
    except:
        return {}

    soup = BeautifulSoup(driver.page_source, "html.parser")
    data = {}

    rows = soup.find_all("tr", class_="HorseList")
    for tr in rows:
        waku_tds = tr.find_all("td", class_="Waku")
        umaban = ""
        for td in waku_tds:
            txt = re.sub(r"\D", "", td.get_text(strip=True))
            if txt:
                umaban = txt
                break
        if not umaban:
            continue

        jockey_td = tr.find("td", class_="Jockey")
        jockey = "ä¸æ˜"
        if jockey_td:
            a_tag = jockey_td.find("a")
            if a_tag:
                jockey = a_tag.get_text(strip=True)
            else:
                barei = jockey_td.find("span", class_="Barei")
                if barei:
                    barei.decompose()
                jockey = jockey_td.get_text(strip=True)
            jockey = _clean_text_ja(jockey)

        past_tds = tr.find_all("td", class_="Past")
        past_list = []
        for td in past_tds[:3]:
            if "Rest" in td.get("class", []):
                past_list.append("(æ”¾ç‰§/ä¼‘é¤Š)")
            else:
                past_list.append(_parse_netkeiba_past_td(td))

        data[umaban] = {"jockey": jockey, "past": past_list}

    return data

# ==================================================
# Dify Streaming
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ DIFY_API_KEY æœªè¨­å®š"
        return

    payload = {
        "inputs": {"text": full_text},
        "response_mode": "streaming",
        "user": "keiba-bot",
    }
    headers = {"Authorization": f"Bearer {DIFY_API_KEY}", "Content-Type": "application/json"}

    try:
        res = requests.post(
            "https://api.dify.ai/v1/workflows/run",
            headers=headers,
            json=payload,
            stream=True,
            timeout=90
        )
        for line in res.iter_lines():
            if not line:
                continue
            decoded = line.decode("utf-8").replace("data: ", "")
            try:
                data = json.loads(decoded)
                event = data.get("event")
                if event == "workflow_finished":
                    outputs = data.get("data", {}).get("outputs", {})
                    for val in outputs.values():
                        if isinstance(val, str):
                            yield val
                if event == "message" or "answer" in data:
                    yield data.get("answer", "")
            except:
                pass
    except Exception as e:
        yield f"Error: {e}"

# ==================================================
# Main Execution
# ==================================================
def run_all_races(target_races=None):
    race_nums = target_races if target_races else list(range(1, 13))
    race_nums = [int(r) for r in race_nums]

    base_id = f"{YEAR}{KAI}{PLACE}{DAY}"

    driver = build_driver()
    try:
        st.info(f"ãƒ­ã‚°ã‚¤ãƒ³ä¸­... (ID: {KEIBA_ID[:2]}**)")
        login_keibabook(driver)
        st.success("ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ")

        combined_text = ""

        for r in race_nums:
            race_num_str = f"{r:02}"
            race_id = base_id + race_num_str

            st.markdown(f"### {PLACE_NAMES.get(PLACE, 'å ´')} {r}R")
            status = st.empty()
            status.text("ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")

            # 1. å©èˆã®è©±
            header_info, danwa_data = fetch_keibabook_danwa(driver, race_id)
            if not danwa_data:
                st.error("é¦¬ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ (å©èˆã®è©±ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—)")
                continue

            # --- æ–°é¦¬æˆ¦åˆ¤å®š ---
            race_title = header_info.get("header_text", "")
            is_shinba = ("æ–°é¦¬" in race_title) or ("ãƒ¡ã‚¤ã‚¯ãƒ‡ãƒ“ãƒ¥ãƒ¼" in race_title)
            if is_shinba:
                st.caption("ğŸŒ± æ–°é¦¬æˆ¦(ãƒ¡ã‚¤ã‚¯ãƒ‡ãƒ“ãƒ¥ãƒ¼)ãƒ¢ãƒ¼ãƒ‰ã§è§£æã—ã¾ã™")

            # 2. CPUäºˆæƒ³
            cpu_data = fetch_keibabook_cpu_data(driver, race_id, is_shinba=is_shinba)

            # â˜…ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ¨™ã‚’ç®—å‡ºï¼ˆåå·®å€¤dev + ä¸Šä½å¼·èª¿scoreï¼‰
            # ç«¶é¦¬äºˆæƒ³ã®è¦‹ãŸç›®ã¨ã—ã¦è‡ªç„¶ãªã€Œãƒ¡ãƒªãƒãƒªã€ã¯ score ã‚’ä½¿ã†ã®ãŒãŠã™ã™ã‚
            speed_metrics = compute_speed_metrics(cpu_data, T=55.0, k=4.0)

            # 3. å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼
            interview_data = fetch_zenkoso_interview(driver, race_id)

            # 4. èª¿æ•™ãƒ‡ãƒ¼ã‚¿
            chokyo_data = fetch_keibabook_chokyo(driver, race_id)

            # 5. Netkeiba (é¨æ‰‹ãƒ»æˆ¦ç¸¾)
            nk_data = fetch_netkeiba_data(driver, YEAR, KAI, PLACE, DAY, race_num_str)

            # --- ãƒ‡ãƒ¼ã‚¿çµ±åˆ ---
            lines = []
            for umaban in sorted(danwa_data.keys(), key=int):
                d_info = danwa_data[umaban]
                c_info = cpu_data.get(umaban, {})
                i_text = interview_data.get(umaban, "ãªã—")
                k_info = chokyo_data.get(umaban, {"tanpyo": "-", "details": "-"})
                n_info = nk_data.get(umaban, {})

                # æˆ¦ç¸¾ãƒ†ã‚­ã‚¹ãƒˆ
                past_list = n_info.get("past", [])
                past_str = " / ".join(past_list) if past_list else "æƒ…å ±ãªã—"

                # ã‚¹ãƒ”ãƒ¼ãƒ‰ï¼ˆè¡¨ç¤ºç”¨score + ç›£æŸ»ç”¨dev + rawï¼‰
                sm = speed_metrics.get(umaban, {})
                sp_score = sm.get("score", "-")  # 0ã€œ100ï¼ˆä¸Šä½å¼·èª¿ï¼‰
                sp_dev = sm.get("dev", "-")      # åå·®å€¤ï¼ˆ0ã€œ100ã‚¯ãƒªãƒƒãƒ—ï¼‰
                sp_raw = sm.get("raw", "-")      # åŸºç¤å€¤

                # æŒ‡æ•°ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå…ƒã®å‰/2/3/å¹³ã¯ä¿æŒ + ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ¨™ã‚’ä»˜ä¸ï¼‰
                # LLMã«ã¯ã€Œscoreã€ã‚’ä¸»ã«ä½¿ã‚ã›ã€dev/rawã¯è£œåŠ©æƒ…å ±ã¨ã—ã¦æ®‹ã™
                sp_str = (
                    f"æŒ‡æ•°(å‰/2/3/å¹³):{c_info.get('sp_last','-')}/{c_info.get('sp_2','-')}/{c_info.get('sp_3','-')}/{c_info.get('sp_avg','-')} "
                    f"ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°:{sp_score} åå·®å€¤:{sp_dev} åŸºç¤å€¤:{sp_raw}"
                )

                # ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆåˆ†å²
                if is_shinba:
                    fac_str = f"F(å‡ºè„š/è¡€çµ±/å‹•ã):{c_info.get('fac_deashi','-')}/{c_info.get('fac_kettou','-')}/{c_info.get('fac_ugoki','-')}"
                else:
                    fac_str = f"F(ã‚³/è·/å‰):{c_info.get('fac_crs','-')}/{c_info.get('fac_dis','-')}/{c_info.get('fac_zen','-')}"

                cpu_str = f"{sp_str} {fac_str}"

                # èª¿æ•™ãƒ†ã‚­ã‚¹ãƒˆ
                chokyo_str = f"çŸ­è©•:{k_info['tanpyo']} / è©³ç´°:{k_info['details']}"

                # å…¥åŠ›ï¼ˆLLMå´ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¦ä»¶ã«åˆã‚ã›ã¦ã€HTMLã‚¿ã‚°ã¯ä½¿ã‚ãªã„ï¼‰
                line = (
                    f"â–¼é¦¬ç•ª{umaban} {d_info['name']} (é¨æ‰‹:{n_info.get('jockey','-')})\n"
                    f"ã€å©èˆã®è©±ã€‘{d_info['danwa']}\n"
                    f"ã€å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã€‘{i_text}\n"
                    f"ã€è¿‘èµ°ã€‘{past_str}\n"
                    f"ã€ãƒ‡ãƒ¼ã‚¿ã€‘{cpu_str}\n"
                    f"ã€èª¿æ•™ã€‘{chokyo_str}\n"
                )
                lines.append(line)

            full_prompt = (
                f"â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n{header_info.get('header_text','')}\n\n"
                f"â– å„é¦¬è©³ç´°\n" + "\n".join(lines)
            )

            # AIç”Ÿæˆ
            status.text("AIåˆ†æä¸­...")
            result_area = st.empty()
            ai_output = ""
            for chunk in stream_dify_workflow(full_prompt):
                ai_output += chunk
                result_area.markdown(ai_output + "â–Œ")
            result_area.markdown(ai_output)

            combined_text += f"\n\n--- {r}R ---\n{ai_output}"

            render_copy_button(ai_output, f"{r}R ã‚³ãƒ”ãƒ¼", f"copy_btn_{r}")
            status.success("å®Œäº†")

        st.subheader("å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚")
        render_copy_button(combined_text, "å…¨ãƒ¬ãƒ¼ã‚¹ã‚³ãƒ”ãƒ¼", "copy_btn_all")
        st.text_area("å‡ºåŠ›çµæœ", combined_text, height=300)

    finally:
        driver.quit()

if __name__ == "__main__":
    st.title("ğŸ‡ ç«¶é¦¬AIäºˆæƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ")
    run_all_races()
